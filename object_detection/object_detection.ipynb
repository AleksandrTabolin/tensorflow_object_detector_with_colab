{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4UBWIjoUgA",
        "colab_type": "text"
      },
      "source": [
        "1. **Подготовка**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbox8emXldKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "36af75e1-9b9f-4e5b-84a8-f3d9173984a4"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "!rm -r sample_data/                                                                                  \n",
        "!git clone https://github.com/tensorflow/models.git                                                 \n",
        "\n",
        "# устанавливаем Protobuf\n",
        "!apt-get -qq install libprotobuf-java protobuf-compiler                                                \n",
        "%cd ./models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%cd ../.. "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 30641, done.\u001b[K\n",
            "remote: Total 30641 (delta 0), reused 0 (delta 0), pack-reused 30641\u001b[K\n",
            "Receiving objects: 100% (30641/30641), 510.52 MiB | 35.13 MiB/s, done.\n",
            "Resolving deltas: 100% (19253/19253), done.\n",
            "Checking out files: 100% (3079/3079), done.\n",
            "/content/models/research\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-tSg17YoPNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/models/research/\"\n",
        "os.environ['PYTHONPATH'] += \":/content/models/research/slim\"\n",
        "os.environ['PYTHONPATH'] += \":/content/models/research/object_detection\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m8_5K0PjV8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM5QXdHA0RV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4ff6de9-d855-498d-940a-7179d03504a6"
      },
      "source": [
        "drive_file_id=\"1v2bzkVOt9007Js3GuIViEmr4y5ilUxSY\"\n",
        "\n",
        "training_demo_zip = drive.CreateFile({'id': drive_file_id})\n",
        "training_demo_zip.GetContentFile('training_demo.zip')\n",
        "\n",
        "!unzip training_demo.zip\n",
        "!rm training_demo.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  training_demo.zip\n",
            "   creating: training_demo/\n",
            "   creating: training_demo/annotations/\n",
            "  inflating: training_demo/annotations/label_map.pbtxt  \n",
            "  inflating: training_demo/annotations/test.record  \n",
            "  inflating: training_demo/annotations/test_labels.csv  \n",
            "  inflating: training_demo/annotations/train.record  \n",
            "  inflating: training_demo/annotations/train_labels.csv  \n",
            "   creating: training_demo/images/\n",
            "   creating: training_demo/images/test/\n",
            "  inflating: training_demo/images/test/45.jpg  \n",
            "  inflating: training_demo/images/test/45.xml  \n",
            "  inflating: training_demo/images/test/5.jpg  \n",
            "  inflating: training_demo/images/test/5.xml  \n",
            "  inflating: training_demo/images/test/6.jpg  \n",
            "  inflating: training_demo/images/test/6.xml  \n",
            "  inflating: training_demo/images/test/67.jpg  \n",
            "  inflating: training_demo/images/test/67.xml  \n",
            "  inflating: training_demo/images/test/68.jpg  \n",
            "  inflating: training_demo/images/test/68.xml  \n",
            "  inflating: training_demo/images/test/70.jpg  \n",
            "  inflating: training_demo/images/test/70.xml  \n",
            "  inflating: training_demo/images/test/71.jpg  \n",
            "  inflating: training_demo/images/test/71.xml  \n",
            "  inflating: training_demo/images/test/72.jpg  \n",
            "  inflating: training_demo/images/test/72.xml  \n",
            "  inflating: training_demo/images/test/8.jpg  \n",
            "  inflating: training_demo/images/test/8.xml  \n",
            "   creating: training_demo/images/train/\n",
            "  inflating: training_demo/images/train/1.jpg  \n",
            "  inflating: training_demo/images/train/1.xml  \n",
            "  inflating: training_demo/images/train/10.jpg  \n",
            "  inflating: training_demo/images/train/10.xml  \n",
            "  inflating: training_demo/images/train/11.jpg  \n",
            "  inflating: training_demo/images/train/11.xml  \n",
            "  inflating: training_demo/images/train/12.jpg  \n",
            "  inflating: training_demo/images/train/12.xml  \n",
            "  inflating: training_demo/images/train/13.jpg  \n",
            "  inflating: training_demo/images/train/13.xml  \n",
            "  inflating: training_demo/images/train/14.jpg  \n",
            "  inflating: training_demo/images/train/14.xml  \n",
            "  inflating: training_demo/images/train/15.jpg  \n",
            "  inflating: training_demo/images/train/15.xml  \n",
            "  inflating: training_demo/images/train/16.jpg  \n",
            "  inflating: training_demo/images/train/16.xml  \n",
            "  inflating: training_demo/images/train/17.jpg  \n",
            "  inflating: training_demo/images/train/17.xml  \n",
            "  inflating: training_demo/images/train/18.jpg  \n",
            "  inflating: training_demo/images/train/18.xml  \n",
            "  inflating: training_demo/images/train/19.jpg  \n",
            "  inflating: training_demo/images/train/19.xml  \n",
            "  inflating: training_demo/images/train/2.jpg  \n",
            "  inflating: training_demo/images/train/2.xml  \n",
            "  inflating: training_demo/images/train/20.jpg  \n",
            "  inflating: training_demo/images/train/20.xml  \n",
            "  inflating: training_demo/images/train/21.jpg  \n",
            "  inflating: training_demo/images/train/21.xml  \n",
            "  inflating: training_demo/images/train/22.jpg  \n",
            "  inflating: training_demo/images/train/22.xml  \n",
            "  inflating: training_demo/images/train/23.jpg  \n",
            "  inflating: training_demo/images/train/23.xml  \n",
            "  inflating: training_demo/images/train/24.jpg  \n",
            "  inflating: training_demo/images/train/24.xml  \n",
            "  inflating: training_demo/images/train/25.jpg  \n",
            "  inflating: training_demo/images/train/25.xml  \n",
            "  inflating: training_demo/images/train/26.jpg  \n",
            "  inflating: training_demo/images/train/26.xml  \n",
            "  inflating: training_demo/images/train/27.jpg  \n",
            "  inflating: training_demo/images/train/27.xml  \n",
            "  inflating: training_demo/images/train/28.jpg  \n",
            "  inflating: training_demo/images/train/28.xml  \n",
            "  inflating: training_demo/images/train/29.jpg  \n",
            "  inflating: training_demo/images/train/29.xml  \n",
            "  inflating: training_demo/images/train/3.jpg  \n",
            "  inflating: training_demo/images/train/3.xml  \n",
            "  inflating: training_demo/images/train/30.jpg  \n",
            "  inflating: training_demo/images/train/30.xml  \n",
            "  inflating: training_demo/images/train/31.jpg  \n",
            "  inflating: training_demo/images/train/31.xml  \n",
            "  inflating: training_demo/images/train/32.jpg  \n",
            "  inflating: training_demo/images/train/32.xml  \n",
            "  inflating: training_demo/images/train/33.jpg  \n",
            "  inflating: training_demo/images/train/33.xml  \n",
            "  inflating: training_demo/images/train/34.jpg  \n",
            "  inflating: training_demo/images/train/34.xml  \n",
            "  inflating: training_demo/images/train/35.jpg  \n",
            "  inflating: training_demo/images/train/35.xml  \n",
            "  inflating: training_demo/images/train/36.jpg  \n",
            "  inflating: training_demo/images/train/36.xml  \n",
            "  inflating: training_demo/images/train/37.jpg  \n",
            "  inflating: training_demo/images/train/37.xml  \n",
            "  inflating: training_demo/images/train/38.jpg  \n",
            "  inflating: training_demo/images/train/38.xml  \n",
            "  inflating: training_demo/images/train/39.jpg  \n",
            "  inflating: training_demo/images/train/39.xml  \n",
            "  inflating: training_demo/images/train/4.jpg  \n",
            "  inflating: training_demo/images/train/4.xml  \n",
            "  inflating: training_demo/images/train/40.jpg  \n",
            "  inflating: training_demo/images/train/40.xml  \n",
            "  inflating: training_demo/images/train/41.jpg  \n",
            "  inflating: training_demo/images/train/41.xml  \n",
            "  inflating: training_demo/images/train/42.jpg  \n",
            "  inflating: training_demo/images/train/42.xml  \n",
            "  inflating: training_demo/images/train/43.jpg  \n",
            "  inflating: training_demo/images/train/43.xml  \n",
            "  inflating: training_demo/images/train/44.jpg  \n",
            "  inflating: training_demo/images/train/44.xml  \n",
            "  inflating: training_demo/images/train/46.jpg  \n",
            "  inflating: training_demo/images/train/46.xml  \n",
            "  inflating: training_demo/images/train/47.jpg  \n",
            "  inflating: training_demo/images/train/47.xml  \n",
            "  inflating: training_demo/images/train/48.jpg  \n",
            "  inflating: training_demo/images/train/48.xml  \n",
            "  inflating: training_demo/images/train/49.jpg  \n",
            "  inflating: training_demo/images/train/49.xml  \n",
            "  inflating: training_demo/images/train/50.jpg  \n",
            "  inflating: training_demo/images/train/50.xml  \n",
            "  inflating: training_demo/images/train/51.jpg  \n",
            "  inflating: training_demo/images/train/51.xml  \n",
            "  inflating: training_demo/images/train/52.jpg  \n",
            "  inflating: training_demo/images/train/52.xml  \n",
            "  inflating: training_demo/images/train/53.jpg  \n",
            "  inflating: training_demo/images/train/53.xml  \n",
            "  inflating: training_demo/images/train/54.jpg  \n",
            "  inflating: training_demo/images/train/54.xml  \n",
            "  inflating: training_demo/images/train/55.jpg  \n",
            "  inflating: training_demo/images/train/55.xml  \n",
            "  inflating: training_demo/images/train/56.jpg  \n",
            "  inflating: training_demo/images/train/56.xml  \n",
            "  inflating: training_demo/images/train/57.jpg  \n",
            "  inflating: training_demo/images/train/57.xml  \n",
            "  inflating: training_demo/images/train/58.jpg  \n",
            "  inflating: training_demo/images/train/58.xml  \n",
            "  inflating: training_demo/images/train/59.jpg  \n",
            "  inflating: training_demo/images/train/59.xml  \n",
            "  inflating: training_demo/images/train/60.jpg  \n",
            "  inflating: training_demo/images/train/60.xml  \n",
            "  inflating: training_demo/images/train/61.jpg  \n",
            "  inflating: training_demo/images/train/61.xml  \n",
            "  inflating: training_demo/images/train/62.jpg  \n",
            "  inflating: training_demo/images/train/62.xml  \n",
            "  inflating: training_demo/images/train/63.jpg  \n",
            "  inflating: training_demo/images/train/63.xml  \n",
            "  inflating: training_demo/images/train/64.jpg  \n",
            "  inflating: training_demo/images/train/64.xml  \n",
            "  inflating: training_demo/images/train/65.jpg  \n",
            "  inflating: training_demo/images/train/65.xml  \n",
            "  inflating: training_demo/images/train/66.jpg  \n",
            "  inflating: training_demo/images/train/66.xml  \n",
            "  inflating: training_demo/images/train/69.jpg  \n",
            "  inflating: training_demo/images/train/69.xml  \n",
            "  inflating: training_demo/images/train/7.jpg  \n",
            "  inflating: training_demo/images/train/7.xml  \n",
            "  inflating: training_demo/images/train/73.jpg  \n",
            "  inflating: training_demo/images/train/73.xml  \n",
            "  inflating: training_demo/images/train/9.jpg  \n",
            "  inflating: training_demo/images/train/9.xml  \n",
            "   creating: training_demo/pre-trained-model/\n",
            "   creating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/\n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/checkpoint  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.data-00000-of-00001  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.index  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.meta  \n",
            "   creating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/\n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/checkpoint  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/frozen_inference_graph.pb  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/model.ckpt.data-00000-of-00001  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/model.ckpt.index  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/model.ckpt.meta  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/pipeline.config  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/PaxHeader/saved_model  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config  \n",
            "   creating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/\n",
            "   creating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/PaxHeader/\n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/PaxHeader/saved_model.pb  \n",
            " extracting: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/PaxHeader/variables  \n",
            "  inflating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/saved_model.pb  \n",
            "   creating: training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/variables/\n",
            "   creating: training_demo/training/\n",
            "  inflating: training_demo/training/ssdlite_mobilenet_v2_coco.config  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4dD1WAl0wiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1673772d-e40c-4bc6-dde2-2576ab9834d5"
      },
      "source": [
        "!python ./models/research/object_detection/legacy/train.py --logtostderr --train_dir=./training_demo/training --pipeline_config_path=./training_demo/training/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/legacy/train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/legacy/train.py:55: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/legacy/train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0915 13:44:37.800947 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "WARNING:tensorflow:From ./models/research/object_detection/legacy/train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0915 13:44:37.801164 139848901543808 deprecation_wrapper.py:119] From ./models/research/object_detection/legacy/train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0915 13:44:37.801526 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/legacy/train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0915 13:44:37.807080 139848901543808 deprecation_wrapper.py:119] From ./models/research/object_detection/legacy/train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0915 13:44:37.819564 139848901543808 deprecation.py:323] From /content/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0915 13:44:37.824787 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0915 13:44:37.825101 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0915 13:44:37.841688 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0915 13:44:37.847214 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0915 13:44:37.847341 139848901543808 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0915 13:44:37.854723 139848901543808 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0915 13:44:37.854901 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0915 13:44:37.884616 139848901543808 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0915 13:44:38.112300 139848901543808 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0915 13:44:38.119649 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0915 13:44:38.124597 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:196: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0915 13:44:38.187739 139848901543808 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:196: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/box_list_ops.py:206: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0915 13:44:38.200678 139848901543808 deprecation.py:323] From /content/models/research/object_detection/core/box_list_ops.py:206: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0915 13:44:39.177765 139848901543808 deprecation.py:323] From /content/models/research/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0915 13:44:39.182102 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0915 13:44:39.183254 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0915 13:44:39.193868 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0915 13:44:39.601771 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0915 13:44:44.073992 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.074265 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.206151 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.352780 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.699129 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.831659 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 13:44:44.964814 139848901543808 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0915 13:44:47.250461 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0915 13:44:47.252158 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W0915 13:44:47.845047 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0915 13:44:47.846033 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0915 13:44:47.846286 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0915 13:44:47.858265 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0915 13:44:50.279829 139848901543808 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0915 13:44:53.336011 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0915 13:44:59.755273 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0915 13:45:00.248266 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0915 13:45:00.250916 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0915 13:45:00.256155 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0915 13:45:00.267297 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0915 13:45:01.361839 139848901543808 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0915 13:45:01.365082 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.365217 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.365293 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.365398 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.365478 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.365539 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.365612 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.365672 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.365731 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.365793 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.365851 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.365907 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.365979 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.366039 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.366095 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.366173 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.366230 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.366286 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.366373 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.366451 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.366508 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.366570 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.366626 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.366681 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.366749 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.366808 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.366863 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.366924 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.367027 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.367086 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.367146 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.367203 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.367258 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.367331 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.367444 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.367504 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.367578 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.367634 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.367688 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.367749 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.367806 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.367861 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.367936 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.368013 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.368070 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.368129 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.368185 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.368239 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.368304 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.368410 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.368479 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.368561 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.368620 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.368677 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.368749 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.368806 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.368873 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.368937 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369003 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.369060 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.369127 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369184 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.369239 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.369300 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369383 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.369462 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.369527 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369593 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.369652 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.369719 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369776 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.369832 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.369892 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.369973 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.370034 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.370095 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.370151 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.370205 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.370273 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.370332 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.370407 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.370473 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.370530 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.370585 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.453220 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.453417 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.453532 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.453660 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.453776 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.453878 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.454005 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.454127 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.454242 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.454365 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.454464 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.454560 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.454694 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.454817 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.454920 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.455036 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.455132 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.455234 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.455379 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.455489 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.455581 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.455701 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.455807 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.455912 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.456049 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.456145 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.456233 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.456342 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.456473 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.456581 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.456698 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.456790 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.456887 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.457010 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.457118 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.457215 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.457314 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.457433 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.457542 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.457673 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.457788 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.457880 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.457990 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.458096 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.458199 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.458311 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.458429 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.458521 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.458642 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.458749 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.458863 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.458977 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.459070 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.459167 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.459295 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.459437 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.459535 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.459670 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.459774 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.459909 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.460101 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.460208 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.460312 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.460451 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.460552 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.460644 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.460757 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.460866 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.460983 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.461104 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.461205 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.461295 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.461431 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.461550 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.461686 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.461813 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.461918 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.462023 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.462125 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.462231 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.462367 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.462515 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.462621 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.462714 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.462824 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.462931 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.463051 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.463167 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.463273 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.463415 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.463545 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.463674 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.463773 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.463911 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.464067 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.464162 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.464261 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.464417 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.464528 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.464643 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.464751 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.464854 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.464976 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.465087 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.465193 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.465319 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.465461 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.465558 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.465655 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.465751 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.465856 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.465998 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.466110 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.466235 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.466346 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.466478 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.466579 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.466691 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.466813 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.466911 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.467082 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.467192 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.467307 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.467493 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.467602 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.467703 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.467803 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.467894 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.468010 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.468141 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.468251 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.468375 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.468485 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.468590 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.468695 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.468812 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.468932 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.469048 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.469160 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.469254 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.469376 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.469530 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.469638 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.469744 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.469856 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.469947 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.470075 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.470204 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.470316 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.470442 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.470550 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.470645 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.470746 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.470860 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.470979 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.471087 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.471200 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.471294 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.471422 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.471540 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.471646 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.471753 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.471855 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.471946 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.472064 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.472192 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.472301 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.472437 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.472544 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.472634 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.472732 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.472847 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.472952 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.473082 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.473199 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.473290 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.473418 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.473535 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.473655 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.473762 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.473867 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.473990 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.474090 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.474225 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.474333 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.474464 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.474575 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.474668 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.474758 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.474870 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.474989 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.475096 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.475221 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.475315 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.475429 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.475555 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.475663 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.475767 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.475882 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.475989 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.476081 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.476218 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.476341 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.476472 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.476588 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.476684 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.476773 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.476880 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.476999 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.477107 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.477233 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.477329 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.477445 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.477555 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.477675 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.477792 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.477918 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.478030 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.478120 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.478235 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.478366 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.478477 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.478598 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.478698 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.478789 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.478910 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.479041 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.479148 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.479277 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.479409 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.479506 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.479607 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.479713 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.479819 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.479947 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.480069 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.480159 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.480270 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.480397 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.480508 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.480633 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.480740 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.480831 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.480944 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.481090 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.481210 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.481379 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.481492 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.481587 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.481687 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.481792 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.481899 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.482028 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.482136 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.482228 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.482339 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.482479 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.482616 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.482745 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.482853 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.482987 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.483089 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.483189 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.483323 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.483476 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.483587 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.483682 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.483778 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.483875 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.483994 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.484111 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.484218 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.484313 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.484451 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.484563 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.484668 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.484783 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.484891 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.485003 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.485105 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.485200 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.485304 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.485455 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.485567 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.485668 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.485768 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.485860 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.485974 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.486093 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.486202 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.486320 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.486457 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.486553 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.486660 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.486778 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.486886 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.487001 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.487104 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.487196 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.487298 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.487449 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.487560 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.487666 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.487769 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.487860 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.487968 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.488088 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.488195 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.488301 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.488441 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.488537 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.488636 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.488750 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.488873 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.488991 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.489100 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.489192 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.489286 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.489432 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.489545 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.489654 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.489763 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.489856 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.489948 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.490088 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.490190 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.490293 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.490438 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.490558 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.490651 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.490748 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.490848 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.490966 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.491086 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.491193 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.491287 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.491422 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.491528 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.491636 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.491753 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.491863 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.491971 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.492074 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.492174 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.492280 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.492432 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.492544 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.492641 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.492739 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.492835 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.492937 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.493068 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.493178 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.493275 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.493405 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.493509 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.493614 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.493729 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.493863 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.493974 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.494076 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.494171 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.494274 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.494430 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.494555 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.494660 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.494762 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.494853 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.494952 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.495082 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.495187 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.495294 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.495433 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.495528 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.495627 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.495742 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.495849 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.495989 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.496094 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.496187 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.496285 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.496434 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.496550 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.496666 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.496773 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.496865 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.496969 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.497086 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.497200 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.497308 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.497458 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.497555 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.497651 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.497766 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.497874 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.497993 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.498104 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.498197 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.498290 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.498453 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.498564 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.498672 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.498784 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.498886 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.498989 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.499105 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.499213 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.499320 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.499469 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.499566 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.499656 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.499770 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.499880 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.500016 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.500132 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.500227 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.500316 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.500461 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.500573 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.500677 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.500794 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.500889 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.501000 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.501111 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.501228 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.501332 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.501495 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.501595 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.501686 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.501790 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.501900 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.502017 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.502134 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.502231 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.502321 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.502471 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.502583 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.502690 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.502805 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.502904 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.503009 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.503113 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.503220 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.503324 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.503478 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.503582 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.503674 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.503776 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.503881 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.504001 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.504117 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.504220 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.504310 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.504444 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.504556 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.504668 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.504784 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.504888 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.504992 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.505093 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.505204 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.505311 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.505461 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.505569 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.505662 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.505760 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.505865 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.505984 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.506100 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.506205 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.506297 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.506431 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.506537 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.506643 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.506772 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.506884 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.506989 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.507091 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.507191 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.507297 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.507450 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.507561 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.507655 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.507752 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.507849 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.507954 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.508082 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.508190 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.508286 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0915 13:45:01.508419 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0915 13:45:01.508521 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0915 13:45:01.508627 139848901543808 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "W0915 13:45:03.112886 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-09-15 13:45:04.709974: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-15 13:45:04.711952: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15743c00 executing computations on platform Host. Devices:\n",
            "2019-09-15 13:45:04.711987: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-15 13:45:04.719297: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-15 13:45:04.873572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:04.874451: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15743dc0 executing computations on platform CUDA. Devices:\n",
            "2019-09-15 13:45:04.874486: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-15 13:45:04.875506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:04.876198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 13:45:04.891886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 13:45:05.098871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 13:45:05.190412: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 13:45:05.220101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 13:45:05.444992: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 13:45:05.574663: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 13:45:05.974873: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 13:45:05.975181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:05.976045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:05.976721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 13:45:05.981378: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 13:45:05.983043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 13:45:05.983080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 13:45:05.983102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 13:45:05.985534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:05.986412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 13:45:05.987096: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-15 13:45:05.987159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-09-15 13:45:09.935258: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0915 13:45:10.378750 139848901543808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "I0915 13:45:10.380287 139848901543808 saver.py:1280] Restoring parameters from ./training_demo/pre-trained-model/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0915 13:45:11.028231 139848901543808 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0915 13:45:11.578729 139848901543808 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "I0915 13:45:22.460139 139848901543808 learning.py:754] Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "I0915 13:45:22.836888 139845851117312 supervisor.py:1117] Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "I0915 13:45:22.841516 139848901543808 learning.py:768] Starting Queues.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "I0915 13:45:33.785532 139845842724608 supervisor.py:1099] global_step/sec: 0\n",
            "2019-09-15 13:45:38.121578: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "I0915 13:45:41.995030 139845834331904 supervisor.py:1050] Recording summary at step 0.\n",
            "INFO:tensorflow:global step 1: loss = 27.7654 (20.121 sec/step)\n",
            "I0915 13:45:43.388021 139848901543808 learning.py:507] global step 1: loss = 27.7654 (20.121 sec/step)\n",
            "INFO:tensorflow:global step 2: loss = 26.1429 (0.631 sec/step)\n",
            "I0915 13:45:44.457100 139848901543808 learning.py:507] global step 2: loss = 26.1429 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 3: loss = 24.4705 (0.631 sec/step)\n",
            "I0915 13:45:45.089994 139848901543808 learning.py:507] global step 3: loss = 24.4705 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 4: loss = 22.8704 (0.649 sec/step)\n",
            "I0915 13:45:45.740749 139848901543808 learning.py:507] global step 4: loss = 22.8704 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 5: loss = 22.9726 (0.650 sec/step)\n",
            "I0915 13:45:46.392479 139848901543808 learning.py:507] global step 5: loss = 22.9726 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 6: loss = 21.9560 (0.615 sec/step)\n",
            "I0915 13:45:47.009027 139848901543808 learning.py:507] global step 6: loss = 21.9560 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 7: loss = 21.4113 (0.634 sec/step)\n",
            "I0915 13:45:47.644880 139848901543808 learning.py:507] global step 7: loss = 21.4113 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 8: loss = 21.7002 (0.649 sec/step)\n",
            "I0915 13:45:48.295881 139848901543808 learning.py:507] global step 8: loss = 21.7002 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 9: loss = 21.2744 (0.647 sec/step)\n",
            "I0915 13:45:48.946085 139848901543808 learning.py:507] global step 9: loss = 21.2744 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 10: loss = 20.9128 (0.645 sec/step)\n",
            "I0915 13:45:49.595465 139848901543808 learning.py:507] global step 10: loss = 20.9128 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 11: loss = 20.2202 (0.630 sec/step)\n",
            "I0915 13:45:50.227462 139848901543808 learning.py:507] global step 11: loss = 20.2202 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 12: loss = 19.8930 (0.650 sec/step)\n",
            "I0915 13:45:50.879313 139848901543808 learning.py:507] global step 12: loss = 19.8930 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 13: loss = 19.4659 (0.630 sec/step)\n",
            "I0915 13:45:51.510653 139848901543808 learning.py:507] global step 13: loss = 19.4659 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 14: loss = 19.1686 (0.642 sec/step)\n",
            "I0915 13:45:52.154561 139848901543808 learning.py:507] global step 14: loss = 19.1686 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 15: loss = 18.8694 (0.631 sec/step)\n",
            "I0915 13:45:52.787402 139848901543808 learning.py:507] global step 15: loss = 18.8694 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 16: loss = 18.3070 (0.630 sec/step)\n",
            "I0915 13:45:53.419845 139848901543808 learning.py:507] global step 16: loss = 18.3070 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 17: loss = 18.0723 (0.634 sec/step)\n",
            "I0915 13:45:54.055561 139848901543808 learning.py:507] global step 17: loss = 18.0723 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 18: loss = 18.2213 (0.624 sec/step)\n",
            "I0915 13:45:54.681736 139848901543808 learning.py:507] global step 18: loss = 18.2213 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 19: loss = 17.7502 (0.642 sec/step)\n",
            "I0915 13:45:55.325647 139848901543808 learning.py:507] global step 19: loss = 17.7502 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 20: loss = 17.2096 (0.646 sec/step)\n",
            "I0915 13:45:55.973667 139848901543808 learning.py:507] global step 20: loss = 17.2096 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 21: loss = 16.9250 (0.635 sec/step)\n",
            "I0915 13:45:56.610852 139848901543808 learning.py:507] global step 21: loss = 16.9250 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 22: loss = 16.8937 (0.641 sec/step)\n",
            "I0915 13:45:57.253859 139848901543808 learning.py:507] global step 22: loss = 16.8937 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 23: loss = 16.7333 (0.612 sec/step)\n",
            "I0915 13:45:57.868577 139848901543808 learning.py:507] global step 23: loss = 16.7333 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 24: loss = 16.5474 (0.652 sec/step)\n",
            "I0915 13:45:58.523028 139848901543808 learning.py:507] global step 24: loss = 16.5474 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 25: loss = 16.7921 (0.631 sec/step)\n",
            "I0915 13:45:59.156481 139848901543808 learning.py:507] global step 25: loss = 16.7921 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 26: loss = 16.1269 (0.631 sec/step)\n",
            "I0915 13:45:59.789310 139848901543808 learning.py:507] global step 26: loss = 16.1269 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 27: loss = 15.3217 (0.637 sec/step)\n",
            "I0915 13:46:00.428416 139848901543808 learning.py:507] global step 27: loss = 15.3217 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 28: loss = 15.4871 (0.640 sec/step)\n",
            "I0915 13:46:01.070248 139848901543808 learning.py:507] global step 28: loss = 15.4871 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 29: loss = 14.7452 (0.664 sec/step)\n",
            "I0915 13:46:01.736374 139848901543808 learning.py:507] global step 29: loss = 14.7452 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 30: loss = 14.9922 (0.641 sec/step)\n",
            "I0915 13:46:02.379337 139848901543808 learning.py:507] global step 30: loss = 14.9922 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 31: loss = 14.6631 (0.631 sec/step)\n",
            "I0915 13:46:03.012047 139848901543808 learning.py:507] global step 31: loss = 14.6631 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 32: loss = 14.0594 (0.636 sec/step)\n",
            "I0915 13:46:03.649968 139848901543808 learning.py:507] global step 32: loss = 14.0594 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 33: loss = 13.9830 (0.615 sec/step)\n",
            "I0915 13:46:04.266978 139848901543808 learning.py:507] global step 33: loss = 13.9830 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 34: loss = 13.9505 (0.635 sec/step)\n",
            "I0915 13:46:04.904303 139848901543808 learning.py:507] global step 34: loss = 13.9505 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 35: loss = 13.2286 (0.628 sec/step)\n",
            "I0915 13:46:05.534791 139848901543808 learning.py:507] global step 35: loss = 13.2286 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 36: loss = 13.8321 (0.649 sec/step)\n",
            "I0915 13:46:06.185659 139848901543808 learning.py:507] global step 36: loss = 13.8321 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 37: loss = 13.1895 (0.625 sec/step)\n",
            "I0915 13:46:06.812430 139848901543808 learning.py:507] global step 37: loss = 13.1895 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 38: loss = 12.8293 (0.641 sec/step)\n",
            "I0915 13:46:07.455549 139848901543808 learning.py:507] global step 38: loss = 12.8293 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 39: loss = 12.4874 (0.628 sec/step)\n",
            "I0915 13:46:08.086190 139848901543808 learning.py:507] global step 39: loss = 12.4874 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 40: loss = 12.1381 (0.639 sec/step)\n",
            "I0915 13:46:08.727526 139848901543808 learning.py:507] global step 40: loss = 12.1381 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 41: loss = 12.1781 (0.636 sec/step)\n",
            "I0915 13:46:09.365159 139848901543808 learning.py:507] global step 41: loss = 12.1781 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 42: loss = 12.2237 (0.644 sec/step)\n",
            "I0915 13:46:10.011604 139848901543808 learning.py:507] global step 42: loss = 12.2237 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 43: loss = 11.8709 (0.625 sec/step)\n",
            "I0915 13:46:10.639017 139848901543808 learning.py:507] global step 43: loss = 11.8709 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 44: loss = 11.5547 (0.634 sec/step)\n",
            "I0915 13:46:11.275610 139848901543808 learning.py:507] global step 44: loss = 11.5547 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 45: loss = 11.4497 (0.638 sec/step)\n",
            "I0915 13:46:11.916244 139848901543808 learning.py:507] global step 45: loss = 11.4497 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 46: loss = 11.7625 (0.639 sec/step)\n",
            "I0915 13:46:12.558129 139848901543808 learning.py:507] global step 46: loss = 11.7625 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 47: loss = 10.6960 (0.632 sec/step)\n",
            "I0915 13:46:13.192198 139848901543808 learning.py:507] global step 47: loss = 10.6960 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 48: loss = 11.2265 (0.628 sec/step)\n",
            "I0915 13:46:13.822586 139848901543808 learning.py:507] global step 48: loss = 11.2265 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 49: loss = 11.0599 (0.626 sec/step)\n",
            "I0915 13:46:14.450841 139848901543808 learning.py:507] global step 49: loss = 11.0599 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 50: loss = 11.2652 (0.647 sec/step)\n",
            "I0915 13:46:15.100095 139848901543808 learning.py:507] global step 50: loss = 11.2652 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 51: loss = 10.8684 (0.636 sec/step)\n",
            "I0915 13:46:15.738330 139848901543808 learning.py:507] global step 51: loss = 10.8684 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 52: loss = 10.2385 (0.660 sec/step)\n",
            "I0915 13:46:16.399955 139848901543808 learning.py:507] global step 52: loss = 10.2385 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 53: loss = 10.0039 (0.644 sec/step)\n",
            "I0915 13:46:17.045625 139848901543808 learning.py:507] global step 53: loss = 10.0039 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 54: loss = 10.2023 (0.624 sec/step)\n",
            "I0915 13:46:17.671446 139848901543808 learning.py:507] global step 54: loss = 10.2023 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 55: loss = 9.9369 (0.638 sec/step)\n",
            "I0915 13:46:18.311674 139848901543808 learning.py:507] global step 55: loss = 9.9369 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 56: loss = 9.8710 (0.641 sec/step)\n",
            "I0915 13:46:18.955113 139848901543808 learning.py:507] global step 56: loss = 9.8710 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 57: loss = 9.6369 (0.640 sec/step)\n",
            "I0915 13:46:19.597832 139848901543808 learning.py:507] global step 57: loss = 9.6369 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 58: loss = 10.0483 (0.630 sec/step)\n",
            "I0915 13:46:20.229606 139848901543808 learning.py:507] global step 58: loss = 10.0483 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 59: loss = 9.6049 (0.645 sec/step)\n",
            "I0915 13:46:20.876917 139848901543808 learning.py:507] global step 59: loss = 9.6049 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 60: loss = 9.5620 (0.614 sec/step)\n",
            "I0915 13:46:21.492712 139848901543808 learning.py:507] global step 60: loss = 9.5620 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 61: loss = 8.8756 (0.634 sec/step)\n",
            "I0915 13:46:22.128085 139848901543808 learning.py:507] global step 61: loss = 8.8756 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 62: loss = 8.8186 (0.641 sec/step)\n",
            "I0915 13:46:22.771300 139848901543808 learning.py:507] global step 62: loss = 8.8186 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 63: loss = 8.9996 (0.638 sec/step)\n",
            "I0915 13:46:23.411396 139848901543808 learning.py:507] global step 63: loss = 8.9996 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 64: loss = 9.5277 (0.650 sec/step)\n",
            "I0915 13:46:24.063946 139848901543808 learning.py:507] global step 64: loss = 9.5277 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 65: loss = 9.0394 (0.653 sec/step)\n",
            "I0915 13:46:24.719574 139848901543808 learning.py:507] global step 65: loss = 9.0394 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 66: loss = 8.3193 (0.634 sec/step)\n",
            "I0915 13:46:25.355832 139848901543808 learning.py:507] global step 66: loss = 8.3193 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 67: loss = 8.4871 (0.621 sec/step)\n",
            "I0915 13:46:25.979973 139848901543808 learning.py:507] global step 67: loss = 8.4871 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 68: loss = 8.8185 (0.643 sec/step)\n",
            "I0915 13:46:26.625395 139848901543808 learning.py:507] global step 68: loss = 8.8185 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 69: loss = 8.0432 (0.627 sec/step)\n",
            "I0915 13:46:27.255038 139848901543808 learning.py:507] global step 69: loss = 8.0432 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 70: loss = 8.5872 (0.623 sec/step)\n",
            "I0915 13:46:27.879436 139848901543808 learning.py:507] global step 70: loss = 8.5872 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 71: loss = 8.5939 (0.622 sec/step)\n",
            "I0915 13:46:28.503500 139848901543808 learning.py:507] global step 71: loss = 8.5939 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 72: loss = 8.0051 (0.605 sec/step)\n",
            "I0915 13:46:29.109997 139848901543808 learning.py:507] global step 72: loss = 8.0051 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 73: loss = 7.9921 (0.622 sec/step)\n",
            "I0915 13:46:29.734278 139848901543808 learning.py:507] global step 73: loss = 7.9921 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 74: loss = 8.3124 (0.614 sec/step)\n",
            "I0915 13:46:30.350678 139848901543808 learning.py:507] global step 74: loss = 8.3124 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 75: loss = 7.7479 (0.618 sec/step)\n",
            "I0915 13:46:30.970636 139848901543808 learning.py:507] global step 75: loss = 7.7479 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 76: loss = 7.9843 (0.612 sec/step)\n",
            "I0915 13:46:31.584457 139848901543808 learning.py:507] global step 76: loss = 7.9843 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 77: loss = 8.1072 (0.638 sec/step)\n",
            "I0915 13:46:32.223952 139848901543808 learning.py:507] global step 77: loss = 8.1072 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 78: loss = 7.4676 (0.625 sec/step)\n",
            "I0915 13:46:32.850777 139848901543808 learning.py:507] global step 78: loss = 7.4676 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 79: loss = 7.2648 (0.612 sec/step)\n",
            "I0915 13:46:33.464140 139848901543808 learning.py:507] global step 79: loss = 7.2648 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 80: loss = 7.3114 (0.623 sec/step)\n",
            "I0915 13:46:34.089021 139848901543808 learning.py:507] global step 80: loss = 7.3114 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 81: loss = 7.4115 (0.630 sec/step)\n",
            "I0915 13:46:34.720566 139848901543808 learning.py:507] global step 81: loss = 7.4115 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 82: loss = 8.0002 (0.621 sec/step)\n",
            "I0915 13:46:35.343155 139848901543808 learning.py:507] global step 82: loss = 8.0002 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 83: loss = 6.8795 (0.623 sec/step)\n",
            "I0915 13:46:35.967570 139848901543808 learning.py:507] global step 83: loss = 6.8795 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 84: loss = 7.5345 (0.631 sec/step)\n",
            "I0915 13:46:36.600425 139848901543808 learning.py:507] global step 84: loss = 7.5345 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 85: loss = 7.1860 (0.644 sec/step)\n",
            "I0915 13:46:37.247522 139848901543808 learning.py:507] global step 85: loss = 7.1860 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 86: loss = 7.3212 (0.626 sec/step)\n",
            "I0915 13:46:37.876036 139848901543808 learning.py:507] global step 86: loss = 7.3212 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 87: loss = 7.6226 (0.614 sec/step)\n",
            "I0915 13:46:38.492120 139848901543808 learning.py:507] global step 87: loss = 7.6226 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 88: loss = 6.9359 (0.604 sec/step)\n",
            "I0915 13:46:39.097893 139848901543808 learning.py:507] global step 88: loss = 6.9359 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 89: loss = 6.6739 (0.612 sec/step)\n",
            "I0915 13:46:39.711858 139848901543808 learning.py:507] global step 89: loss = 6.6739 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 90: loss = 6.7383 (0.615 sec/step)\n",
            "I0915 13:46:40.328750 139848901543808 learning.py:507] global step 90: loss = 6.7383 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 91: loss = 6.8910 (0.653 sec/step)\n",
            "I0915 13:46:40.983970 139848901543808 learning.py:507] global step 91: loss = 6.8910 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 92: loss = 6.7672 (0.643 sec/step)\n",
            "I0915 13:46:41.629585 139848901543808 learning.py:507] global step 92: loss = 6.7672 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 93: loss = 6.9214 (0.631 sec/step)\n",
            "I0915 13:46:42.262960 139848901543808 learning.py:507] global step 93: loss = 6.9214 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 94: loss = 6.7451 (0.629 sec/step)\n",
            "I0915 13:46:42.893789 139848901543808 learning.py:507] global step 94: loss = 6.7451 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 95: loss = 6.1560 (0.640 sec/step)\n",
            "I0915 13:46:43.535997 139848901543808 learning.py:507] global step 95: loss = 6.1560 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 96: loss = 7.2617 (0.688 sec/step)\n",
            "I0915 13:46:44.225906 139848901543808 learning.py:507] global step 96: loss = 7.2617 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 97: loss = 6.8080 (0.625 sec/step)\n",
            "I0915 13:46:44.853601 139848901543808 learning.py:507] global step 97: loss = 6.8080 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 98: loss = 6.7079 (0.634 sec/step)\n",
            "I0915 13:46:45.490039 139848901543808 learning.py:507] global step 98: loss = 6.7079 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 99: loss = 6.2349 (0.624 sec/step)\n",
            "I0915 13:46:46.116895 139848901543808 learning.py:507] global step 99: loss = 6.2349 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 100: loss = 6.1732 (0.641 sec/step)\n",
            "I0915 13:46:46.759634 139848901543808 learning.py:507] global step 100: loss = 6.1732 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 101: loss = 6.4943 (0.648 sec/step)\n",
            "I0915 13:46:47.409163 139848901543808 learning.py:507] global step 101: loss = 6.4943 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 102: loss = 6.4170 (0.623 sec/step)\n",
            "I0915 13:46:48.033926 139848901543808 learning.py:507] global step 102: loss = 6.4170 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 103: loss = 6.5867 (0.636 sec/step)\n",
            "I0915 13:46:48.673188 139848901543808 learning.py:507] global step 103: loss = 6.5867 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 104: loss = 6.4194 (0.633 sec/step)\n",
            "I0915 13:46:49.308828 139848901543808 learning.py:507] global step 104: loss = 6.4194 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 105: loss = 6.5417 (0.631 sec/step)\n",
            "I0915 13:46:49.941943 139848901543808 learning.py:507] global step 105: loss = 6.5417 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 106: loss = 6.2921 (0.655 sec/step)\n",
            "I0915 13:46:50.598648 139848901543808 learning.py:507] global step 106: loss = 6.2921 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 107: loss = 6.0179 (0.629 sec/step)\n",
            "I0915 13:46:51.229896 139848901543808 learning.py:507] global step 107: loss = 6.0179 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 108: loss = 6.0039 (0.646 sec/step)\n",
            "I0915 13:46:51.877641 139848901543808 learning.py:507] global step 108: loss = 6.0039 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 109: loss = 6.1366 (0.631 sec/step)\n",
            "I0915 13:46:52.511001 139848901543808 learning.py:507] global step 109: loss = 6.1366 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 110: loss = 5.9716 (0.660 sec/step)\n",
            "I0915 13:46:53.173666 139848901543808 learning.py:507] global step 110: loss = 5.9716 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 111: loss = 6.2759 (0.633 sec/step)\n",
            "I0915 13:46:53.809259 139848901543808 learning.py:507] global step 111: loss = 6.2759 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 112: loss = 4.9771 (0.656 sec/step)\n",
            "I0915 13:46:54.467749 139848901543808 learning.py:507] global step 112: loss = 4.9771 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 113: loss = 5.8713 (0.661 sec/step)\n",
            "I0915 13:46:55.130932 139848901543808 learning.py:507] global step 113: loss = 5.8713 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 114: loss = 5.5550 (0.638 sec/step)\n",
            "I0915 13:46:55.770831 139848901543808 learning.py:507] global step 114: loss = 5.5550 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 115: loss = 5.3964 (0.643 sec/step)\n",
            "I0915 13:46:56.415437 139848901543808 learning.py:507] global step 115: loss = 5.3964 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 116: loss = 6.4022 (0.647 sec/step)\n",
            "I0915 13:46:57.064210 139848901543808 learning.py:507] global step 116: loss = 6.4022 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 117: loss = 7.1005 (0.656 sec/step)\n",
            "I0915 13:46:57.722274 139848901543808 learning.py:507] global step 117: loss = 7.1005 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 118: loss = 5.5506 (0.631 sec/step)\n",
            "I0915 13:46:58.354804 139848901543808 learning.py:507] global step 118: loss = 5.5506 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 119: loss = 5.4940 (0.624 sec/step)\n",
            "I0915 13:46:58.980699 139848901543808 learning.py:507] global step 119: loss = 5.4940 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 120: loss = 5.9888 (0.633 sec/step)\n",
            "I0915 13:46:59.615378 139848901543808 learning.py:507] global step 120: loss = 5.9888 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 121: loss = 5.6542 (0.659 sec/step)\n",
            "I0915 13:47:00.276407 139848901543808 learning.py:507] global step 121: loss = 5.6542 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 122: loss = 6.6523 (0.639 sec/step)\n",
            "I0915 13:47:00.917514 139848901543808 learning.py:507] global step 122: loss = 6.6523 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 123: loss = 5.4102 (0.635 sec/step)\n",
            "I0915 13:47:01.554605 139848901543808 learning.py:507] global step 123: loss = 5.4102 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 124: loss = 6.5270 (0.640 sec/step)\n",
            "I0915 13:47:02.196169 139848901543808 learning.py:507] global step 124: loss = 6.5270 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 125: loss = 5.4331 (0.629 sec/step)\n",
            "I0915 13:47:02.827117 139848901543808 learning.py:507] global step 125: loss = 5.4331 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 126: loss = 6.0257 (0.629 sec/step)\n",
            "I0915 13:47:03.457967 139848901543808 learning.py:507] global step 126: loss = 6.0257 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 127: loss = 5.5736 (0.620 sec/step)\n",
            "I0915 13:47:04.080758 139848901543808 learning.py:507] global step 127: loss = 5.5736 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 128: loss = 5.9285 (0.659 sec/step)\n",
            "I0915 13:47:04.741210 139848901543808 learning.py:507] global step 128: loss = 5.9285 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 129: loss = 5.8853 (0.663 sec/step)\n",
            "I0915 13:47:05.406141 139848901543808 learning.py:507] global step 129: loss = 5.8853 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 130: loss = 5.9156 (0.633 sec/step)\n",
            "I0915 13:47:06.041594 139848901543808 learning.py:507] global step 130: loss = 5.9156 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 131: loss = 5.8097 (0.665 sec/step)\n",
            "I0915 13:47:06.708409 139848901543808 learning.py:507] global step 131: loss = 5.8097 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 132: loss = 4.9272 (0.644 sec/step)\n",
            "I0915 13:47:07.354734 139848901543808 learning.py:507] global step 132: loss = 4.9272 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 133: loss = 5.8528 (0.664 sec/step)\n",
            "I0915 13:47:08.020174 139848901543808 learning.py:507] global step 133: loss = 5.8528 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 134: loss = 5.1078 (0.646 sec/step)\n",
            "I0915 13:47:08.668402 139848901543808 learning.py:507] global step 134: loss = 5.1078 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 135: loss = 6.4505 (0.631 sec/step)\n",
            "I0915 13:47:09.301828 139848901543808 learning.py:507] global step 135: loss = 6.4505 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 136: loss = 5.2851 (0.645 sec/step)\n",
            "I0915 13:47:09.948613 139848901543808 learning.py:507] global step 136: loss = 5.2851 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 137: loss = 5.3639 (0.657 sec/step)\n",
            "I0915 13:47:10.607559 139848901543808 learning.py:507] global step 137: loss = 5.3639 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 138: loss = 4.8668 (0.638 sec/step)\n",
            "I0915 13:47:11.247766 139848901543808 learning.py:507] global step 138: loss = 4.8668 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 139: loss = 5.4429 (0.634 sec/step)\n",
            "I0915 13:47:11.883743 139848901543808 learning.py:507] global step 139: loss = 5.4429 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 140: loss = 5.3603 (0.645 sec/step)\n",
            "I0915 13:47:12.530467 139848901543808 learning.py:507] global step 140: loss = 5.3603 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 141: loss = 5.8191 (0.636 sec/step)\n",
            "I0915 13:47:13.168752 139848901543808 learning.py:507] global step 141: loss = 5.8191 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 142: loss = 5.6444 (0.644 sec/step)\n",
            "I0915 13:47:13.814162 139848901543808 learning.py:507] global step 142: loss = 5.6444 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 143: loss = 5.9247 (0.618 sec/step)\n",
            "I0915 13:47:14.434120 139848901543808 learning.py:507] global step 143: loss = 5.9247 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 144: loss = 5.2155 (0.635 sec/step)\n",
            "I0915 13:47:15.070997 139848901543808 learning.py:507] global step 144: loss = 5.2155 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 145: loss = 5.7088 (0.649 sec/step)\n",
            "I0915 13:47:15.721743 139848901543808 learning.py:507] global step 145: loss = 5.7088 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 146: loss = 5.5840 (0.629 sec/step)\n",
            "I0915 13:47:16.353593 139848901543808 learning.py:507] global step 146: loss = 5.5840 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 147: loss = 5.3402 (0.651 sec/step)\n",
            "I0915 13:47:17.006148 139848901543808 learning.py:507] global step 147: loss = 5.3402 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 148: loss = 4.6950 (0.653 sec/step)\n",
            "I0915 13:47:17.661488 139848901543808 learning.py:507] global step 148: loss = 4.6950 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 149: loss = 5.5169 (0.642 sec/step)\n",
            "I0915 13:47:18.305466 139848901543808 learning.py:507] global step 149: loss = 5.5169 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 150: loss = 5.0207 (0.672 sec/step)\n",
            "I0915 13:47:18.979388 139848901543808 learning.py:507] global step 150: loss = 5.0207 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 151: loss = 4.6113 (0.611 sec/step)\n",
            "I0915 13:47:19.592113 139848901543808 learning.py:507] global step 151: loss = 4.6113 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 152: loss = 4.9797 (0.631 sec/step)\n",
            "I0915 13:47:20.224694 139848901543808 learning.py:507] global step 152: loss = 4.9797 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 153: loss = 4.4791 (0.664 sec/step)\n",
            "I0915 13:47:20.890943 139848901543808 learning.py:507] global step 153: loss = 4.4791 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 154: loss = 5.2964 (0.626 sec/step)\n",
            "I0915 13:47:21.518938 139848901543808 learning.py:507] global step 154: loss = 5.2964 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 155: loss = 5.0031 (0.634 sec/step)\n",
            "I0915 13:47:22.155460 139848901543808 learning.py:507] global step 155: loss = 5.0031 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 156: loss = 4.8824 (0.643 sec/step)\n",
            "I0915 13:47:22.801341 139848901543808 learning.py:507] global step 156: loss = 4.8824 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 157: loss = 5.1635 (1.094 sec/step)\n",
            "I0915 13:47:23.897861 139848901543808 learning.py:507] global step 157: loss = 5.1635 (1.094 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 157.\n",
            "I0915 13:47:23.900186 139845834331904 supervisor.py:1050] Recording summary at step 157.\n",
            "INFO:tensorflow:global step 158: loss = 4.8674 (0.651 sec/step)\n",
            "I0915 13:47:24.550854 139848901543808 learning.py:507] global step 158: loss = 4.8674 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 159: loss = 5.2877 (0.639 sec/step)\n",
            "I0915 13:47:25.192442 139848901543808 learning.py:507] global step 159: loss = 5.2877 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 160: loss = 4.5392 (0.625 sec/step)\n",
            "I0915 13:47:25.818935 139848901543808 learning.py:507] global step 160: loss = 4.5392 (0.625 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.42591\n",
            "I0915 13:47:25.994503 139845842724608 supervisor.py:1099] global_step/sec: 1.42591\n",
            "INFO:tensorflow:global step 161: loss = 5.2605 (0.648 sec/step)\n",
            "I0915 13:47:26.468638 139848901543808 learning.py:507] global step 161: loss = 5.2605 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 162: loss = 4.1062 (0.652 sec/step)\n",
            "I0915 13:47:27.122566 139848901543808 learning.py:507] global step 162: loss = 4.1062 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 163: loss = 5.6483 (0.633 sec/step)\n",
            "I0915 13:47:27.757403 139848901543808 learning.py:507] global step 163: loss = 5.6483 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 164: loss = 4.5516 (0.626 sec/step)\n",
            "I0915 13:47:28.385483 139848901543808 learning.py:507] global step 164: loss = 4.5516 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 165: loss = 4.7597 (0.667 sec/step)\n",
            "I0915 13:47:29.054695 139848901543808 learning.py:507] global step 165: loss = 4.7597 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 166: loss = 5.1366 (0.630 sec/step)\n",
            "I0915 13:47:29.686819 139848901543808 learning.py:507] global step 166: loss = 5.1366 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 167: loss = 5.0346 (0.656 sec/step)\n",
            "I0915 13:47:30.344193 139848901543808 learning.py:507] global step 167: loss = 5.0346 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 168: loss = 4.9907 (0.647 sec/step)\n",
            "I0915 13:47:30.992856 139848901543808 learning.py:507] global step 168: loss = 4.9907 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 169: loss = 4.1084 (0.640 sec/step)\n",
            "I0915 13:47:31.634424 139848901543808 learning.py:507] global step 169: loss = 4.1084 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 170: loss = 4.9778 (0.645 sec/step)\n",
            "I0915 13:47:32.280652 139848901543808 learning.py:507] global step 170: loss = 4.9778 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 171: loss = 5.0334 (0.625 sec/step)\n",
            "I0915 13:47:32.907251 139848901543808 learning.py:507] global step 171: loss = 5.0334 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 172: loss = 5.8331 (0.649 sec/step)\n",
            "I0915 13:47:33.558413 139848901543808 learning.py:507] global step 172: loss = 5.8331 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 173: loss = 5.3580 (0.664 sec/step)\n",
            "I0915 13:47:34.224195 139848901543808 learning.py:507] global step 173: loss = 5.3580 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 174: loss = 4.5880 (0.613 sec/step)\n",
            "I0915 13:47:34.838818 139848901543808 learning.py:507] global step 174: loss = 4.5880 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 175: loss = 4.5730 (0.667 sec/step)\n",
            "I0915 13:47:35.508886 139848901543808 learning.py:507] global step 175: loss = 4.5730 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 176: loss = 4.9482 (0.637 sec/step)\n",
            "I0915 13:47:36.147439 139848901543808 learning.py:507] global step 176: loss = 4.9482 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 177: loss = 5.0675 (0.631 sec/step)\n",
            "I0915 13:47:36.780581 139848901543808 learning.py:507] global step 177: loss = 5.0675 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 178: loss = 4.5877 (0.637 sec/step)\n",
            "I0915 13:47:37.419253 139848901543808 learning.py:507] global step 178: loss = 4.5877 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 179: loss = 5.1408 (0.631 sec/step)\n",
            "I0915 13:47:38.052253 139848901543808 learning.py:507] global step 179: loss = 5.1408 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 180: loss = 4.2302 (0.654 sec/step)\n",
            "I0915 13:47:38.709248 139848901543808 learning.py:507] global step 180: loss = 4.2302 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 181: loss = 4.8307 (0.669 sec/step)\n",
            "I0915 13:47:39.380258 139848901543808 learning.py:507] global step 181: loss = 4.8307 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 182: loss = 4.8116 (0.633 sec/step)\n",
            "I0915 13:47:40.014826 139848901543808 learning.py:507] global step 182: loss = 4.8116 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 183: loss = 4.1886 (0.644 sec/step)\n",
            "I0915 13:47:40.660690 139848901543808 learning.py:507] global step 183: loss = 4.1886 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 184: loss = 3.6885 (0.628 sec/step)\n",
            "I0915 13:47:41.290552 139848901543808 learning.py:507] global step 184: loss = 3.6885 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 185: loss = 4.8572 (0.629 sec/step)\n",
            "I0915 13:47:41.921511 139848901543808 learning.py:507] global step 185: loss = 4.8572 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 186: loss = 4.5243 (0.627 sec/step)\n",
            "I0915 13:47:42.549945 139848901543808 learning.py:507] global step 186: loss = 4.5243 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 187: loss = 5.1922 (0.654 sec/step)\n",
            "I0915 13:47:43.206447 139848901543808 learning.py:507] global step 187: loss = 5.1922 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 188: loss = 4.7705 (0.633 sec/step)\n",
            "I0915 13:47:43.841210 139848901543808 learning.py:507] global step 188: loss = 4.7705 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 189: loss = 5.0446 (0.662 sec/step)\n",
            "I0915 13:47:44.505154 139848901543808 learning.py:507] global step 189: loss = 5.0446 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 190: loss = 4.6073 (0.642 sec/step)\n",
            "I0915 13:47:45.149057 139848901543808 learning.py:507] global step 190: loss = 4.6073 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 191: loss = 5.0340 (0.652 sec/step)\n",
            "I0915 13:47:45.803059 139848901543808 learning.py:507] global step 191: loss = 5.0340 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 192: loss = 4.7629 (0.644 sec/step)\n",
            "I0915 13:47:46.449615 139848901543808 learning.py:507] global step 192: loss = 4.7629 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 193: loss = 3.8517 (0.632 sec/step)\n",
            "I0915 13:47:47.083384 139848901543808 learning.py:507] global step 193: loss = 3.8517 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 194: loss = 4.3563 (0.656 sec/step)\n",
            "I0915 13:47:47.741283 139848901543808 learning.py:507] global step 194: loss = 4.3563 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 195: loss = 4.1203 (0.626 sec/step)\n",
            "I0915 13:47:48.368690 139848901543808 learning.py:507] global step 195: loss = 4.1203 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 196: loss = 4.4152 (0.629 sec/step)\n",
            "I0915 13:47:48.999143 139848901543808 learning.py:507] global step 196: loss = 4.4152 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 197: loss = 3.9983 (0.666 sec/step)\n",
            "I0915 13:47:49.666976 139848901543808 learning.py:507] global step 197: loss = 3.9983 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 198: loss = 3.4559 (0.626 sec/step)\n",
            "I0915 13:47:50.294646 139848901543808 learning.py:507] global step 198: loss = 3.4559 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 199: loss = 4.4605 (0.640 sec/step)\n",
            "I0915 13:47:50.936842 139848901543808 learning.py:507] global step 199: loss = 4.4605 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 4.0439 (0.654 sec/step)\n",
            "I0915 13:47:51.592709 139848901543808 learning.py:507] global step 200: loss = 4.0439 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 201: loss = 4.0547 (0.640 sec/step)\n",
            "I0915 13:47:52.235162 139848901543808 learning.py:507] global step 201: loss = 4.0547 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 202: loss = 4.2347 (0.633 sec/step)\n",
            "I0915 13:47:52.870337 139848901543808 learning.py:507] global step 202: loss = 4.2347 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 203: loss = 5.4352 (0.632 sec/step)\n",
            "I0915 13:47:53.504474 139848901543808 learning.py:507] global step 203: loss = 5.4352 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 204: loss = 4.4376 (0.640 sec/step)\n",
            "I0915 13:47:54.146326 139848901543808 learning.py:507] global step 204: loss = 4.4376 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 205: loss = 4.3623 (0.644 sec/step)\n",
            "I0915 13:47:54.792416 139848901543808 learning.py:507] global step 205: loss = 4.3623 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 206: loss = 4.3484 (0.636 sec/step)\n",
            "I0915 13:47:55.430619 139848901543808 learning.py:507] global step 206: loss = 4.3484 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 207: loss = 4.7812 (0.665 sec/step)\n",
            "I0915 13:47:56.097830 139848901543808 learning.py:507] global step 207: loss = 4.7812 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 208: loss = 4.1606 (0.632 sec/step)\n",
            "I0915 13:47:56.731558 139848901543808 learning.py:507] global step 208: loss = 4.1606 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 209: loss = 3.4236 (0.634 sec/step)\n",
            "I0915 13:47:57.367638 139848901543808 learning.py:507] global step 209: loss = 3.4236 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 210: loss = 4.2008 (0.624 sec/step)\n",
            "I0915 13:47:57.994130 139848901543808 learning.py:507] global step 210: loss = 4.2008 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 211: loss = 4.6165 (0.631 sec/step)\n",
            "I0915 13:47:58.626563 139848901543808 learning.py:507] global step 211: loss = 4.6165 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 212: loss = 3.5458 (0.639 sec/step)\n",
            "I0915 13:47:59.267557 139848901543808 learning.py:507] global step 212: loss = 3.5458 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 213: loss = 4.9330 (0.667 sec/step)\n",
            "I0915 13:47:59.936420 139848901543808 learning.py:507] global step 213: loss = 4.9330 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 214: loss = 5.4353 (0.651 sec/step)\n",
            "I0915 13:48:00.589673 139848901543808 learning.py:507] global step 214: loss = 5.4353 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 215: loss = 4.0561 (0.639 sec/step)\n",
            "I0915 13:48:01.230838 139848901543808 learning.py:507] global step 215: loss = 4.0561 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 216: loss = 4.6675 (0.621 sec/step)\n",
            "I0915 13:48:01.853843 139848901543808 learning.py:507] global step 216: loss = 4.6675 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 217: loss = 3.5063 (0.637 sec/step)\n",
            "I0915 13:48:02.493010 139848901543808 learning.py:507] global step 217: loss = 3.5063 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 218: loss = 4.6991 (0.643 sec/step)\n",
            "I0915 13:48:03.138382 139848901543808 learning.py:507] global step 218: loss = 4.6991 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 219: loss = 3.7380 (0.627 sec/step)\n",
            "I0915 13:48:03.767326 139848901543808 learning.py:507] global step 219: loss = 3.7380 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 220: loss = 3.9707 (0.628 sec/step)\n",
            "I0915 13:48:04.397155 139848901543808 learning.py:507] global step 220: loss = 3.9707 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 221: loss = 4.3924 (0.648 sec/step)\n",
            "I0915 13:48:05.047528 139848901543808 learning.py:507] global step 221: loss = 4.3924 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 222: loss = 5.3450 (0.617 sec/step)\n",
            "I0915 13:48:05.667482 139848901543808 learning.py:507] global step 222: loss = 5.3450 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 223: loss = 4.0401 (0.659 sec/step)\n",
            "I0915 13:48:06.328264 139848901543808 learning.py:507] global step 223: loss = 4.0401 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 224: loss = 4.4296 (0.676 sec/step)\n",
            "I0915 13:48:07.005821 139848901543808 learning.py:507] global step 224: loss = 4.4296 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 225: loss = 3.9681 (0.644 sec/step)\n",
            "I0915 13:48:07.651642 139848901543808 learning.py:507] global step 225: loss = 3.9681 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 226: loss = 4.1353 (0.667 sec/step)\n",
            "I0915 13:48:08.320047 139848901543808 learning.py:507] global step 226: loss = 4.1353 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 227: loss = 3.3582 (0.639 sec/step)\n",
            "I0915 13:48:08.960569 139848901543808 learning.py:507] global step 227: loss = 3.3582 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 228: loss = 4.0908 (0.624 sec/step)\n",
            "I0915 13:48:09.586166 139848901543808 learning.py:507] global step 228: loss = 4.0908 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 229: loss = 4.9566 (0.633 sec/step)\n",
            "I0915 13:48:10.221187 139848901543808 learning.py:507] global step 229: loss = 4.9566 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 230: loss = 4.3176 (0.639 sec/step)\n",
            "I0915 13:48:10.862936 139848901543808 learning.py:507] global step 230: loss = 4.3176 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 231: loss = 3.8754 (0.656 sec/step)\n",
            "I0915 13:48:11.520906 139848901543808 learning.py:507] global step 231: loss = 3.8754 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 232: loss = 4.7696 (0.617 sec/step)\n",
            "I0915 13:48:12.140196 139848901543808 learning.py:507] global step 232: loss = 4.7696 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 233: loss = 4.0190 (0.639 sec/step)\n",
            "I0915 13:48:12.781635 139848901543808 learning.py:507] global step 233: loss = 4.0190 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 234: loss = 4.0296 (0.650 sec/step)\n",
            "I0915 13:48:13.433854 139848901543808 learning.py:507] global step 234: loss = 4.0296 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 235: loss = 4.0853 (0.630 sec/step)\n",
            "I0915 13:48:14.066274 139848901543808 learning.py:507] global step 235: loss = 4.0853 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 236: loss = 3.7484 (0.644 sec/step)\n",
            "I0915 13:48:14.712112 139848901543808 learning.py:507] global step 236: loss = 3.7484 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 237: loss = 3.8314 (0.646 sec/step)\n",
            "I0915 13:48:15.359754 139848901543808 learning.py:507] global step 237: loss = 3.8314 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 238: loss = 4.8420 (0.630 sec/step)\n",
            "I0915 13:48:15.992309 139848901543808 learning.py:507] global step 238: loss = 4.8420 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 239: loss = 4.1151 (0.656 sec/step)\n",
            "I0915 13:48:16.650054 139848901543808 learning.py:507] global step 239: loss = 4.1151 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 240: loss = 3.9054 (0.649 sec/step)\n",
            "I0915 13:48:17.300625 139848901543808 learning.py:507] global step 240: loss = 3.9054 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 241: loss = 3.9925 (0.621 sec/step)\n",
            "I0915 13:48:17.923410 139848901543808 learning.py:507] global step 241: loss = 3.9925 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 242: loss = 4.7819 (0.631 sec/step)\n",
            "I0915 13:48:18.556476 139848901543808 learning.py:507] global step 242: loss = 4.7819 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 243: loss = 3.2366 (0.637 sec/step)\n",
            "I0915 13:48:19.195325 139848901543808 learning.py:507] global step 243: loss = 3.2366 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 244: loss = 4.3588 (0.628 sec/step)\n",
            "I0915 13:48:19.825636 139848901543808 learning.py:507] global step 244: loss = 4.3588 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 245: loss = 3.9639 (0.664 sec/step)\n",
            "I0915 13:48:20.491394 139848901543808 learning.py:507] global step 245: loss = 3.9639 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 246: loss = 4.0749 (0.640 sec/step)\n",
            "I0915 13:48:21.133890 139848901543808 learning.py:507] global step 246: loss = 4.0749 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 247: loss = 4.2993 (0.635 sec/step)\n",
            "I0915 13:48:21.771430 139848901543808 learning.py:507] global step 247: loss = 4.2993 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 248: loss = 3.6947 (0.665 sec/step)\n",
            "I0915 13:48:22.438267 139848901543808 learning.py:507] global step 248: loss = 3.6947 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 249: loss = 5.1459 (0.651 sec/step)\n",
            "I0915 13:48:23.091095 139848901543808 learning.py:507] global step 249: loss = 5.1459 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 250: loss = 4.7188 (0.628 sec/step)\n",
            "I0915 13:48:23.721718 139848901543808 learning.py:507] global step 250: loss = 4.7188 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 251: loss = 3.9004 (0.632 sec/step)\n",
            "I0915 13:48:24.355919 139848901543808 learning.py:507] global step 251: loss = 3.9004 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 252: loss = 3.7442 (0.632 sec/step)\n",
            "I0915 13:48:24.990147 139848901543808 learning.py:507] global step 252: loss = 3.7442 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 253: loss = 3.7950 (0.656 sec/step)\n",
            "I0915 13:48:25.648269 139848901543808 learning.py:507] global step 253: loss = 3.7950 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 254: loss = 4.2122 (0.646 sec/step)\n",
            "I0915 13:48:26.296225 139848901543808 learning.py:507] global step 254: loss = 4.2122 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 255: loss = 3.3087 (0.643 sec/step)\n",
            "I0915 13:48:26.941453 139848901543808 learning.py:507] global step 255: loss = 3.3087 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 256: loss = 4.5774 (0.630 sec/step)\n",
            "I0915 13:48:27.573935 139848901543808 learning.py:507] global step 256: loss = 4.5774 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 257: loss = 3.5533 (0.645 sec/step)\n",
            "I0915 13:48:28.220232 139848901543808 learning.py:507] global step 257: loss = 3.5533 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 258: loss = 3.5874 (0.652 sec/step)\n",
            "I0915 13:48:28.873878 139848901543808 learning.py:507] global step 258: loss = 3.5874 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 259: loss = 4.7830 (0.647 sec/step)\n",
            "I0915 13:48:29.522545 139848901543808 learning.py:507] global step 259: loss = 4.7830 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 260: loss = 3.7925 (0.637 sec/step)\n",
            "I0915 13:48:30.161173 139848901543808 learning.py:507] global step 260: loss = 3.7925 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 261: loss = 4.0501 (0.679 sec/step)\n",
            "I0915 13:48:30.842365 139848901543808 learning.py:507] global step 261: loss = 4.0501 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 262: loss = 4.1892 (0.621 sec/step)\n",
            "I0915 13:48:31.465446 139848901543808 learning.py:507] global step 262: loss = 4.1892 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 263: loss = 2.7747 (0.646 sec/step)\n",
            "I0915 13:48:32.113318 139848901543808 learning.py:507] global step 263: loss = 2.7747 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 264: loss = 3.4931 (0.649 sec/step)\n",
            "I0915 13:48:32.763980 139848901543808 learning.py:507] global step 264: loss = 3.4931 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 265: loss = 3.9950 (0.638 sec/step)\n",
            "I0915 13:48:33.404029 139848901543808 learning.py:507] global step 265: loss = 3.9950 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 266: loss = 4.4340 (0.658 sec/step)\n",
            "I0915 13:48:34.064551 139848901543808 learning.py:507] global step 266: loss = 4.4340 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 267: loss = 2.9994 (0.636 sec/step)\n",
            "I0915 13:48:34.703032 139848901543808 learning.py:507] global step 267: loss = 2.9994 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 268: loss = 4.2030 (0.662 sec/step)\n",
            "I0915 13:48:35.367029 139848901543808 learning.py:507] global step 268: loss = 4.2030 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 269: loss = 4.6809 (0.641 sec/step)\n",
            "I0915 13:48:36.010265 139848901543808 learning.py:507] global step 269: loss = 4.6809 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 270: loss = 3.8588 (0.645 sec/step)\n",
            "I0915 13:48:36.656521 139848901543808 learning.py:507] global step 270: loss = 3.8588 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 271: loss = 3.8884 (0.631 sec/step)\n",
            "I0915 13:48:37.289668 139848901543808 learning.py:507] global step 271: loss = 3.8884 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 272: loss = 2.9096 (0.636 sec/step)\n",
            "I0915 13:48:37.927627 139848901543808 learning.py:507] global step 272: loss = 2.9096 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 273: loss = 3.2937 (0.641 sec/step)\n",
            "I0915 13:48:38.570304 139848901543808 learning.py:507] global step 273: loss = 3.2937 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 274: loss = 6.1883 (0.630 sec/step)\n",
            "I0915 13:48:39.202559 139848901543808 learning.py:507] global step 274: loss = 6.1883 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 275: loss = 4.2105 (0.623 sec/step)\n",
            "I0915 13:48:39.827507 139848901543808 learning.py:507] global step 275: loss = 4.2105 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 276: loss = 3.5917 (0.639 sec/step)\n",
            "I0915 13:48:40.468818 139848901543808 learning.py:507] global step 276: loss = 3.5917 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 277: loss = 3.7160 (0.673 sec/step)\n",
            "I0915 13:48:41.143508 139848901543808 learning.py:507] global step 277: loss = 3.7160 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 278: loss = 3.5084 (0.646 sec/step)\n",
            "I0915 13:48:41.791158 139848901543808 learning.py:507] global step 278: loss = 3.5084 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 279: loss = 3.6314 (0.632 sec/step)\n",
            "I0915 13:48:42.426081 139848901543808 learning.py:507] global step 279: loss = 3.6314 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 280: loss = 3.0755 (0.641 sec/step)\n",
            "I0915 13:48:43.069187 139848901543808 learning.py:507] global step 280: loss = 3.0755 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 281: loss = 3.6744 (0.637 sec/step)\n",
            "I0915 13:48:43.707770 139848901543808 learning.py:507] global step 281: loss = 3.6744 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 282: loss = 3.2250 (0.642 sec/step)\n",
            "I0915 13:48:44.352053 139848901543808 learning.py:507] global step 282: loss = 3.2250 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 283: loss = 3.2527 (0.660 sec/step)\n",
            "I0915 13:48:45.013669 139848901543808 learning.py:507] global step 283: loss = 3.2527 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 284: loss = 2.8886 (0.622 sec/step)\n",
            "I0915 13:48:45.637710 139848901543808 learning.py:507] global step 284: loss = 2.8886 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 285: loss = 3.2916 (0.633 sec/step)\n",
            "I0915 13:48:46.272826 139848901543808 learning.py:507] global step 285: loss = 3.2916 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 286: loss = 3.4620 (0.619 sec/step)\n",
            "I0915 13:48:46.893492 139848901543808 learning.py:507] global step 286: loss = 3.4620 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 287: loss = 2.9453 (0.657 sec/step)\n",
            "I0915 13:48:47.552336 139848901543808 learning.py:507] global step 287: loss = 2.9453 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 288: loss = 3.0793 (0.628 sec/step)\n",
            "I0915 13:48:48.181728 139848901543808 learning.py:507] global step 288: loss = 3.0793 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 289: loss = 3.4634 (0.630 sec/step)\n",
            "I0915 13:48:48.815247 139848901543808 learning.py:507] global step 289: loss = 3.4634 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 290: loss = 3.0154 (0.641 sec/step)\n",
            "I0915 13:48:49.458653 139848901543808 learning.py:507] global step 290: loss = 3.0154 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 291: loss = 4.5298 (0.624 sec/step)\n",
            "I0915 13:48:50.084465 139848901543808 learning.py:507] global step 291: loss = 4.5298 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 292: loss = 3.6167 (0.618 sec/step)\n",
            "I0915 13:48:50.704622 139848901543808 learning.py:507] global step 292: loss = 3.6167 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 293: loss = 3.0946 (0.677 sec/step)\n",
            "I0915 13:48:51.383817 139848901543808 learning.py:507] global step 293: loss = 3.0946 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 294: loss = 3.4567 (0.624 sec/step)\n",
            "I0915 13:48:52.010449 139848901543808 learning.py:507] global step 294: loss = 3.4567 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 295: loss = 3.4159 (0.633 sec/step)\n",
            "I0915 13:48:52.645704 139848901543808 learning.py:507] global step 295: loss = 3.4159 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 296: loss = 3.3892 (0.625 sec/step)\n",
            "I0915 13:48:53.272891 139848901543808 learning.py:507] global step 296: loss = 3.3892 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 297: loss = 3.7951 (0.647 sec/step)\n",
            "I0915 13:48:53.921865 139848901543808 learning.py:507] global step 297: loss = 3.7951 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 298: loss = 3.8713 (0.629 sec/step)\n",
            "I0915 13:48:54.552707 139848901543808 learning.py:507] global step 298: loss = 3.8713 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 299: loss = 3.5589 (0.621 sec/step)\n",
            "I0915 13:48:55.175285 139848901543808 learning.py:507] global step 299: loss = 3.5589 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 4.2377 (0.617 sec/step)\n",
            "I0915 13:48:55.794001 139848901543808 learning.py:507] global step 300: loss = 4.2377 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 301: loss = 4.5948 (0.617 sec/step)\n",
            "I0915 13:48:56.412260 139848901543808 learning.py:507] global step 301: loss = 4.5948 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 302: loss = 3.8606 (0.634 sec/step)\n",
            "I0915 13:48:57.048443 139848901543808 learning.py:507] global step 302: loss = 3.8606 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 303: loss = 2.5833 (0.651 sec/step)\n",
            "I0915 13:48:57.701632 139848901543808 learning.py:507] global step 303: loss = 2.5833 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 304: loss = 3.6475 (0.615 sec/step)\n",
            "I0915 13:48:58.318417 139848901543808 learning.py:507] global step 304: loss = 3.6475 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 305: loss = 3.1361 (0.637 sec/step)\n",
            "I0915 13:48:58.957518 139848901543808 learning.py:507] global step 305: loss = 3.1361 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 306: loss = 4.2418 (0.638 sec/step)\n",
            "I0915 13:48:59.597509 139848901543808 learning.py:507] global step 306: loss = 4.2418 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 307: loss = 4.2142 (0.621 sec/step)\n",
            "I0915 13:49:00.220502 139848901543808 learning.py:507] global step 307: loss = 4.2142 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 308: loss = 2.9253 (0.628 sec/step)\n",
            "I0915 13:49:00.850960 139848901543808 learning.py:507] global step 308: loss = 2.9253 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 309: loss = 3.1347 (0.642 sec/step)\n",
            "I0915 13:49:01.495448 139848901543808 learning.py:507] global step 309: loss = 3.1347 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 310: loss = 2.5947 (0.611 sec/step)\n",
            "I0915 13:49:02.108069 139848901543808 learning.py:507] global step 310: loss = 2.5947 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 311: loss = 3.1584 (0.646 sec/step)\n",
            "I0915 13:49:02.755633 139848901543808 learning.py:507] global step 311: loss = 3.1584 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 312: loss = 2.9771 (0.622 sec/step)\n",
            "I0915 13:49:03.380625 139848901543808 learning.py:507] global step 312: loss = 2.9771 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 313: loss = 4.1838 (0.625 sec/step)\n",
            "I0915 13:49:04.010269 139848901543808 learning.py:507] global step 313: loss = 4.1838 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 314: loss = 3.1078 (0.652 sec/step)\n",
            "I0915 13:49:04.663876 139848901543808 learning.py:507] global step 314: loss = 3.1078 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 315: loss = 3.9221 (0.628 sec/step)\n",
            "I0915 13:49:05.293567 139848901543808 learning.py:507] global step 315: loss = 3.9221 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 316: loss = 3.4837 (0.637 sec/step)\n",
            "I0915 13:49:05.932479 139848901543808 learning.py:507] global step 316: loss = 3.4837 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 317: loss = 3.9836 (0.638 sec/step)\n",
            "I0915 13:49:06.572621 139848901543808 learning.py:507] global step 317: loss = 3.9836 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 318: loss = 3.1834 (0.632 sec/step)\n",
            "I0915 13:49:07.206624 139848901543808 learning.py:507] global step 318: loss = 3.1834 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 319: loss = 3.3279 (0.648 sec/step)\n",
            "I0915 13:49:07.856891 139848901543808 learning.py:507] global step 319: loss = 3.3279 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 320: loss = 3.7341 (0.631 sec/step)\n",
            "I0915 13:49:08.490062 139848901543808 learning.py:507] global step 320: loss = 3.7341 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 321: loss = 3.4306 (0.628 sec/step)\n",
            "I0915 13:49:09.119521 139848901543808 learning.py:507] global step 321: loss = 3.4306 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 322: loss = 3.9932 (0.648 sec/step)\n",
            "I0915 13:49:09.769141 139848901543808 learning.py:507] global step 322: loss = 3.9932 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 323: loss = 3.3358 (0.654 sec/step)\n",
            "I0915 13:49:10.425129 139848901543808 learning.py:507] global step 323: loss = 3.3358 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 324: loss = 3.4956 (0.634 sec/step)\n",
            "I0915 13:49:11.061633 139848901543808 learning.py:507] global step 324: loss = 3.4956 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 325: loss = 2.8989 (0.638 sec/step)\n",
            "I0915 13:49:11.701382 139848901543808 learning.py:507] global step 325: loss = 2.8989 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 326: loss = 2.5207 (0.635 sec/step)\n",
            "I0915 13:49:12.340871 139848901543808 learning.py:507] global step 326: loss = 2.5207 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 327: loss = 3.4802 (0.645 sec/step)\n",
            "I0915 13:49:12.988220 139848901543808 learning.py:507] global step 327: loss = 3.4802 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 328: loss = 4.3295 (0.657 sec/step)\n",
            "I0915 13:49:13.647077 139848901543808 learning.py:507] global step 328: loss = 4.3295 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 329: loss = 3.4344 (0.660 sec/step)\n",
            "I0915 13:49:14.309159 139848901543808 learning.py:507] global step 329: loss = 3.4344 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 330: loss = 3.3299 (0.622 sec/step)\n",
            "I0915 13:49:14.933187 139848901543808 learning.py:507] global step 330: loss = 3.3299 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 331: loss = 3.0185 (0.615 sec/step)\n",
            "I0915 13:49:15.550452 139848901543808 learning.py:507] global step 331: loss = 3.0185 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 332: loss = 3.7047 (0.645 sec/step)\n",
            "I0915 13:49:16.197827 139848901543808 learning.py:507] global step 332: loss = 3.7047 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 333: loss = 3.1333 (0.651 sec/step)\n",
            "I0915 13:49:16.850436 139848901543808 learning.py:507] global step 333: loss = 3.1333 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 334: loss = 3.1962 (0.633 sec/step)\n",
            "I0915 13:49:17.485714 139848901543808 learning.py:507] global step 334: loss = 3.1962 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 335: loss = 3.8410 (0.644 sec/step)\n",
            "I0915 13:49:18.131757 139848901543808 learning.py:507] global step 335: loss = 3.8410 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 336: loss = 3.1861 (0.634 sec/step)\n",
            "I0915 13:49:18.768094 139848901543808 learning.py:507] global step 336: loss = 3.1861 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 337: loss = 3.1396 (0.639 sec/step)\n",
            "I0915 13:49:19.409674 139848901543808 learning.py:507] global step 337: loss = 3.1396 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 338: loss = 4.6512 (0.645 sec/step)\n",
            "I0915 13:49:20.056767 139848901543808 learning.py:507] global step 338: loss = 4.6512 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 339: loss = 2.8537 (0.679 sec/step)\n",
            "I0915 13:49:20.737581 139848901543808 learning.py:507] global step 339: loss = 2.8537 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 340: loss = 3.1392 (0.632 sec/step)\n",
            "I0915 13:49:21.371564 139848901543808 learning.py:507] global step 340: loss = 3.1392 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 341: loss = 3.3385 (0.648 sec/step)\n",
            "I0915 13:49:22.022000 139848901543808 learning.py:507] global step 341: loss = 3.3385 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 342: loss = 3.7901 (0.624 sec/step)\n",
            "I0915 13:49:22.648447 139848901543808 learning.py:507] global step 342: loss = 3.7901 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 343: loss = 4.0185 (0.851 sec/step)\n",
            "I0915 13:49:23.503859 139848901543808 learning.py:507] global step 343: loss = 4.0185 (0.851 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 343.\n",
            "I0915 13:49:24.056668 139845834331904 supervisor.py:1050] Recording summary at step 343.\n",
            "INFO:tensorflow:global step 344: loss = 3.7189 (0.781 sec/step)\n",
            "I0915 13:49:24.389385 139848901543808 learning.py:507] global step 344: loss = 3.7189 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 345: loss = 3.5359 (0.643 sec/step)\n",
            "I0915 13:49:25.035032 139848901543808 learning.py:507] global step 345: loss = 3.5359 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 346: loss = 2.5208 (0.655 sec/step)\n",
            "I0915 13:49:25.691899 139848901543808 learning.py:507] global step 346: loss = 2.5208 (0.655 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.55188\n",
            "I0915 13:49:25.849107 139845842724608 supervisor.py:1099] global_step/sec: 1.55188\n",
            "INFO:tensorflow:global step 347: loss = 4.2639 (0.625 sec/step)\n",
            "I0915 13:49:26.319302 139848901543808 learning.py:507] global step 347: loss = 4.2639 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 348: loss = 4.3661 (0.622 sec/step)\n",
            "I0915 13:49:26.943287 139848901543808 learning.py:507] global step 348: loss = 4.3661 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 349: loss = 2.9214 (0.640 sec/step)\n",
            "I0915 13:49:27.585524 139848901543808 learning.py:507] global step 349: loss = 2.9214 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 350: loss = 2.5605 (0.630 sec/step)\n",
            "I0915 13:49:28.217872 139848901543808 learning.py:507] global step 350: loss = 2.5605 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 351: loss = 3.8375 (0.630 sec/step)\n",
            "I0915 13:49:28.850179 139848901543808 learning.py:507] global step 351: loss = 3.8375 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 352: loss = 3.4688 (0.625 sec/step)\n",
            "I0915 13:49:29.476812 139848901543808 learning.py:507] global step 352: loss = 3.4688 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 353: loss = 3.2335 (0.623 sec/step)\n",
            "I0915 13:49:30.101716 139848901543808 learning.py:507] global step 353: loss = 3.2335 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 354: loss = 3.8320 (0.619 sec/step)\n",
            "I0915 13:49:30.722554 139848901543808 learning.py:507] global step 354: loss = 3.8320 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 355: loss = 2.7468 (0.628 sec/step)\n",
            "I0915 13:49:31.352680 139848901543808 learning.py:507] global step 355: loss = 2.7468 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 356: loss = 3.4607 (0.621 sec/step)\n",
            "I0915 13:49:31.975844 139848901543808 learning.py:507] global step 356: loss = 3.4607 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 357: loss = 3.7336 (0.643 sec/step)\n",
            "I0915 13:49:32.621061 139848901543808 learning.py:507] global step 357: loss = 3.7336 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 358: loss = 3.4787 (0.628 sec/step)\n",
            "I0915 13:49:33.251334 139848901543808 learning.py:507] global step 358: loss = 3.4787 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 359: loss = 3.3318 (0.619 sec/step)\n",
            "I0915 13:49:33.872909 139848901543808 learning.py:507] global step 359: loss = 3.3318 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 360: loss = 3.2207 (0.668 sec/step)\n",
            "I0915 13:49:34.542297 139848901543808 learning.py:507] global step 360: loss = 3.2207 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 361: loss = 4.0009 (0.633 sec/step)\n",
            "I0915 13:49:35.177082 139848901543808 learning.py:507] global step 361: loss = 4.0009 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 362: loss = 2.7111 (0.634 sec/step)\n",
            "I0915 13:49:35.812695 139848901543808 learning.py:507] global step 362: loss = 2.7111 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 363: loss = 4.1082 (0.637 sec/step)\n",
            "I0915 13:49:36.452028 139848901543808 learning.py:507] global step 363: loss = 4.1082 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 364: loss = 3.9413 (0.627 sec/step)\n",
            "I0915 13:49:37.080906 139848901543808 learning.py:507] global step 364: loss = 3.9413 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 365: loss = 3.5614 (0.618 sec/step)\n",
            "I0915 13:49:37.700597 139848901543808 learning.py:507] global step 365: loss = 3.5614 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 366: loss = 3.2802 (0.627 sec/step)\n",
            "I0915 13:49:38.329202 139848901543808 learning.py:507] global step 366: loss = 3.2802 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 367: loss = 4.2989 (0.638 sec/step)\n",
            "I0915 13:49:38.969555 139848901543808 learning.py:507] global step 367: loss = 4.2989 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 368: loss = 3.8632 (0.631 sec/step)\n",
            "I0915 13:49:39.602441 139848901543808 learning.py:507] global step 368: loss = 3.8632 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 369: loss = 3.3381 (0.617 sec/step)\n",
            "I0915 13:49:40.221503 139848901543808 learning.py:507] global step 369: loss = 3.3381 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 370: loss = 3.4222 (0.642 sec/step)\n",
            "I0915 13:49:40.865395 139848901543808 learning.py:507] global step 370: loss = 3.4222 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 371: loss = 3.0602 (0.629 sec/step)\n",
            "I0915 13:49:41.495700 139848901543808 learning.py:507] global step 371: loss = 3.0602 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 372: loss = 3.1922 (0.610 sec/step)\n",
            "I0915 13:49:42.107684 139848901543808 learning.py:507] global step 372: loss = 3.1922 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 373: loss = 3.6874 (0.635 sec/step)\n",
            "I0915 13:49:42.744276 139848901543808 learning.py:507] global step 373: loss = 3.6874 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 374: loss = 3.7970 (0.630 sec/step)\n",
            "I0915 13:49:43.376436 139848901543808 learning.py:507] global step 374: loss = 3.7970 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 375: loss = 3.5628 (0.623 sec/step)\n",
            "I0915 13:49:44.001889 139848901543808 learning.py:507] global step 375: loss = 3.5628 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 376: loss = 3.4908 (0.635 sec/step)\n",
            "I0915 13:49:44.638499 139848901543808 learning.py:507] global step 376: loss = 3.4908 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 377: loss = 2.9940 (0.627 sec/step)\n",
            "I0915 13:49:45.268783 139848901543808 learning.py:507] global step 377: loss = 2.9940 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 378: loss = 3.6721 (0.635 sec/step)\n",
            "I0915 13:49:45.905527 139848901543808 learning.py:507] global step 378: loss = 3.6721 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 379: loss = 3.2231 (0.648 sec/step)\n",
            "I0915 13:49:46.554792 139848901543808 learning.py:507] global step 379: loss = 3.2231 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 380: loss = 3.1711 (0.623 sec/step)\n",
            "I0915 13:49:47.180468 139848901543808 learning.py:507] global step 380: loss = 3.1711 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 381: loss = 3.2429 (0.640 sec/step)\n",
            "I0915 13:49:47.823089 139848901543808 learning.py:507] global step 381: loss = 3.2429 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 382: loss = 3.9574 (0.644 sec/step)\n",
            "I0915 13:49:48.468983 139848901543808 learning.py:507] global step 382: loss = 3.9574 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 383: loss = 3.1514 (0.629 sec/step)\n",
            "I0915 13:49:49.100401 139848901543808 learning.py:507] global step 383: loss = 3.1514 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 384: loss = 2.7624 (0.617 sec/step)\n",
            "I0915 13:49:49.719139 139848901543808 learning.py:507] global step 384: loss = 2.7624 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 385: loss = 2.3131 (0.623 sec/step)\n",
            "I0915 13:49:50.344433 139848901543808 learning.py:507] global step 385: loss = 2.3131 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 386: loss = 2.6799 (0.632 sec/step)\n",
            "I0915 13:49:50.978615 139848901543808 learning.py:507] global step 386: loss = 2.6799 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 387: loss = 3.9557 (0.625 sec/step)\n",
            "I0915 13:49:51.605144 139848901543808 learning.py:507] global step 387: loss = 3.9557 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 388: loss = 2.9586 (0.639 sec/step)\n",
            "I0915 13:49:52.246138 139848901543808 learning.py:507] global step 388: loss = 2.9586 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 389: loss = 4.2439 (0.622 sec/step)\n",
            "I0915 13:49:52.869765 139848901543808 learning.py:507] global step 389: loss = 4.2439 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 390: loss = 2.5133 (0.634 sec/step)\n",
            "I0915 13:49:53.505725 139848901543808 learning.py:507] global step 390: loss = 2.5133 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 391: loss = 3.9817 (0.631 sec/step)\n",
            "I0915 13:49:54.138805 139848901543808 learning.py:507] global step 391: loss = 3.9817 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 392: loss = 2.3378 (0.652 sec/step)\n",
            "I0915 13:49:54.793724 139848901543808 learning.py:507] global step 392: loss = 2.3378 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 393: loss = 4.3969 (0.630 sec/step)\n",
            "I0915 13:49:55.425677 139848901543808 learning.py:507] global step 393: loss = 4.3969 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 394: loss = 2.8607 (0.617 sec/step)\n",
            "I0915 13:49:56.044828 139848901543808 learning.py:507] global step 394: loss = 2.8607 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 395: loss = 3.9980 (0.623 sec/step)\n",
            "I0915 13:49:56.669479 139848901543808 learning.py:507] global step 395: loss = 3.9980 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 396: loss = 3.3867 (0.652 sec/step)\n",
            "I0915 13:49:57.323017 139848901543808 learning.py:507] global step 396: loss = 3.3867 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 397: loss = 3.2316 (0.629 sec/step)\n",
            "I0915 13:49:57.954049 139848901543808 learning.py:507] global step 397: loss = 3.2316 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 398: loss = 3.0552 (0.616 sec/step)\n",
            "I0915 13:49:58.572304 139848901543808 learning.py:507] global step 398: loss = 3.0552 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 399: loss = 2.9549 (0.642 sec/step)\n",
            "I0915 13:49:59.216383 139848901543808 learning.py:507] global step 399: loss = 2.9549 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 3.2873 (0.614 sec/step)\n",
            "I0915 13:49:59.832575 139848901543808 learning.py:507] global step 400: loss = 3.2873 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 401: loss = 2.9788 (0.622 sec/step)\n",
            "I0915 13:50:00.456224 139848901543808 learning.py:507] global step 401: loss = 2.9788 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 402: loss = 3.2886 (0.622 sec/step)\n",
            "I0915 13:50:01.079880 139848901543808 learning.py:507] global step 402: loss = 3.2886 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 403: loss = 3.2594 (0.615 sec/step)\n",
            "I0915 13:50:01.697443 139848901543808 learning.py:507] global step 403: loss = 3.2594 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 404: loss = 4.3278 (0.625 sec/step)\n",
            "I0915 13:50:02.324510 139848901543808 learning.py:507] global step 404: loss = 4.3278 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 405: loss = 4.8938 (0.631 sec/step)\n",
            "I0915 13:50:02.957119 139848901543808 learning.py:507] global step 405: loss = 4.8938 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 406: loss = 3.1979 (0.654 sec/step)\n",
            "I0915 13:50:03.613232 139848901543808 learning.py:507] global step 406: loss = 3.1979 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 407: loss = 3.3975 (0.629 sec/step)\n",
            "I0915 13:50:04.244525 139848901543808 learning.py:507] global step 407: loss = 3.3975 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 408: loss = 3.4323 (0.646 sec/step)\n",
            "I0915 13:50:04.893021 139848901543808 learning.py:507] global step 408: loss = 3.4323 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 409: loss = 2.9621 (0.643 sec/step)\n",
            "I0915 13:50:05.538388 139848901543808 learning.py:507] global step 409: loss = 2.9621 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 410: loss = 3.4082 (0.630 sec/step)\n",
            "I0915 13:50:06.170621 139848901543808 learning.py:507] global step 410: loss = 3.4082 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 411: loss = 2.4074 (0.634 sec/step)\n",
            "I0915 13:50:06.806549 139848901543808 learning.py:507] global step 411: loss = 2.4074 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 412: loss = 3.0771 (0.638 sec/step)\n",
            "I0915 13:50:07.446205 139848901543808 learning.py:507] global step 412: loss = 3.0771 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 413: loss = 3.2107 (0.640 sec/step)\n",
            "I0915 13:50:08.088469 139848901543808 learning.py:507] global step 413: loss = 3.2107 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 414: loss = 2.8353 (0.619 sec/step)\n",
            "I0915 13:50:08.709264 139848901543808 learning.py:507] global step 414: loss = 2.8353 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 415: loss = 3.7192 (0.629 sec/step)\n",
            "I0915 13:50:09.339766 139848901543808 learning.py:507] global step 415: loss = 3.7192 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 416: loss = 4.2378 (0.611 sec/step)\n",
            "I0915 13:50:09.953020 139848901543808 learning.py:507] global step 416: loss = 4.2378 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 417: loss = 3.3960 (0.642 sec/step)\n",
            "I0915 13:50:10.597613 139848901543808 learning.py:507] global step 417: loss = 3.3960 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 418: loss = 3.5316 (0.622 sec/step)\n",
            "I0915 13:50:11.221687 139848901543808 learning.py:507] global step 418: loss = 3.5316 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 419: loss = 3.6739 (0.652 sec/step)\n",
            "I0915 13:50:11.875985 139848901543808 learning.py:507] global step 419: loss = 3.6739 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 420: loss = 3.1853 (0.629 sec/step)\n",
            "I0915 13:50:12.507454 139848901543808 learning.py:507] global step 420: loss = 3.1853 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 421: loss = 2.2763 (0.629 sec/step)\n",
            "I0915 13:50:13.138387 139848901543808 learning.py:507] global step 421: loss = 2.2763 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 422: loss = 3.6521 (0.662 sec/step)\n",
            "I0915 13:50:13.802382 139848901543808 learning.py:507] global step 422: loss = 3.6521 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 423: loss = 3.1951 (0.631 sec/step)\n",
            "I0915 13:50:14.435435 139848901543808 learning.py:507] global step 423: loss = 3.1951 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 424: loss = 2.9232 (0.621 sec/step)\n",
            "I0915 13:50:15.058233 139848901543808 learning.py:507] global step 424: loss = 2.9232 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 425: loss = 3.3138 (0.645 sec/step)\n",
            "I0915 13:50:15.705157 139848901543808 learning.py:507] global step 425: loss = 3.3138 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 426: loss = 2.9698 (0.629 sec/step)\n",
            "I0915 13:50:16.336076 139848901543808 learning.py:507] global step 426: loss = 2.9698 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 427: loss = 3.1159 (0.643 sec/step)\n",
            "I0915 13:50:16.981478 139848901543808 learning.py:507] global step 427: loss = 3.1159 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 428: loss = 3.5704 (0.652 sec/step)\n",
            "I0915 13:50:17.635437 139848901543808 learning.py:507] global step 428: loss = 3.5704 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 429: loss = 3.2713 (0.621 sec/step)\n",
            "I0915 13:50:18.258900 139848901543808 learning.py:507] global step 429: loss = 3.2713 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 430: loss = 3.5694 (0.648 sec/step)\n",
            "I0915 13:50:18.908892 139848901543808 learning.py:507] global step 430: loss = 3.5694 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 431: loss = 2.8394 (0.639 sec/step)\n",
            "I0915 13:50:19.550460 139848901543808 learning.py:507] global step 431: loss = 2.8394 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 432: loss = 2.4619 (0.640 sec/step)\n",
            "I0915 13:50:20.192752 139848901543808 learning.py:507] global step 432: loss = 2.4619 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 433: loss = 3.9694 (0.614 sec/step)\n",
            "I0915 13:50:20.809192 139848901543808 learning.py:507] global step 433: loss = 3.9694 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 434: loss = 3.3030 (0.597 sec/step)\n",
            "I0915 13:50:21.407781 139848901543808 learning.py:507] global step 434: loss = 3.3030 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 435: loss = 3.6618 (0.627 sec/step)\n",
            "I0915 13:50:22.036217 139848901543808 learning.py:507] global step 435: loss = 3.6618 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 436: loss = 2.5840 (0.622 sec/step)\n",
            "I0915 13:50:22.660436 139848901543808 learning.py:507] global step 436: loss = 2.5840 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 437: loss = 3.2784 (0.612 sec/step)\n",
            "I0915 13:50:23.275007 139848901543808 learning.py:507] global step 437: loss = 3.2784 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 438: loss = 3.2430 (0.612 sec/step)\n",
            "I0915 13:50:23.888427 139848901543808 learning.py:507] global step 438: loss = 3.2430 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 439: loss = 3.1378 (0.627 sec/step)\n",
            "I0915 13:50:24.517333 139848901543808 learning.py:507] global step 439: loss = 3.1378 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 440: loss = 3.0885 (0.647 sec/step)\n",
            "I0915 13:50:25.166652 139848901543808 learning.py:507] global step 440: loss = 3.0885 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 441: loss = 3.2153 (0.636 sec/step)\n",
            "I0915 13:50:25.804790 139848901543808 learning.py:507] global step 441: loss = 3.2153 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 442: loss = 3.6350 (0.616 sec/step)\n",
            "I0915 13:50:26.422841 139848901543808 learning.py:507] global step 442: loss = 3.6350 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 443: loss = 2.6270 (0.661 sec/step)\n",
            "I0915 13:50:27.085733 139848901543808 learning.py:507] global step 443: loss = 2.6270 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 444: loss = 2.9662 (0.620 sec/step)\n",
            "I0915 13:50:27.708233 139848901543808 learning.py:507] global step 444: loss = 2.9662 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 445: loss = 3.2782 (0.616 sec/step)\n",
            "I0915 13:50:28.326511 139848901543808 learning.py:507] global step 445: loss = 3.2782 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 446: loss = 3.2484 (0.652 sec/step)\n",
            "I0915 13:50:28.980754 139848901543808 learning.py:507] global step 446: loss = 3.2484 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 447: loss = 2.9120 (0.633 sec/step)\n",
            "I0915 13:50:29.615102 139848901543808 learning.py:507] global step 447: loss = 2.9120 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 448: loss = 3.4970 (0.647 sec/step)\n",
            "I0915 13:50:30.264461 139848901543808 learning.py:507] global step 448: loss = 3.4970 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 449: loss = 2.1625 (0.636 sec/step)\n",
            "I0915 13:50:30.902435 139848901543808 learning.py:507] global step 449: loss = 2.1625 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 450: loss = 2.7227 (0.637 sec/step)\n",
            "I0915 13:50:31.541597 139848901543808 learning.py:507] global step 450: loss = 2.7227 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 451: loss = 3.1401 (0.641 sec/step)\n",
            "I0915 13:50:32.184400 139848901543808 learning.py:507] global step 451: loss = 3.1401 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 452: loss = 3.3057 (0.653 sec/step)\n",
            "I0915 13:50:32.841336 139848901543808 learning.py:507] global step 452: loss = 3.3057 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 453: loss = 3.7722 (0.618 sec/step)\n",
            "I0915 13:50:33.462574 139848901543808 learning.py:507] global step 453: loss = 3.7722 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 454: loss = 2.2379 (0.632 sec/step)\n",
            "I0915 13:50:34.096396 139848901543808 learning.py:507] global step 454: loss = 2.2379 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 455: loss = 3.3346 (0.646 sec/step)\n",
            "I0915 13:50:34.744053 139848901543808 learning.py:507] global step 455: loss = 3.3346 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 456: loss = 2.9934 (0.654 sec/step)\n",
            "I0915 13:50:35.400257 139848901543808 learning.py:507] global step 456: loss = 2.9934 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 457: loss = 2.9690 (0.602 sec/step)\n",
            "I0915 13:50:36.004380 139848901543808 learning.py:507] global step 457: loss = 2.9690 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 458: loss = 2.8089 (0.633 sec/step)\n",
            "I0915 13:50:36.639578 139848901543808 learning.py:507] global step 458: loss = 2.8089 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 459: loss = 2.9261 (0.646 sec/step)\n",
            "I0915 13:50:37.287372 139848901543808 learning.py:507] global step 459: loss = 2.9261 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 460: loss = 4.1151 (0.626 sec/step)\n",
            "I0915 13:50:37.915545 139848901543808 learning.py:507] global step 460: loss = 4.1151 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 461: loss = 2.5319 (0.632 sec/step)\n",
            "I0915 13:50:38.549771 139848901543808 learning.py:507] global step 461: loss = 2.5319 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 462: loss = 2.8590 (0.644 sec/step)\n",
            "I0915 13:50:39.195910 139848901543808 learning.py:507] global step 462: loss = 2.8590 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 463: loss = 2.7470 (0.614 sec/step)\n",
            "I0915 13:50:39.812571 139848901543808 learning.py:507] global step 463: loss = 2.7470 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 464: loss = 3.4436 (0.639 sec/step)\n",
            "I0915 13:50:40.453811 139848901543808 learning.py:507] global step 464: loss = 3.4436 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 465: loss = 2.4932 (0.635 sec/step)\n",
            "I0915 13:50:41.090834 139848901543808 learning.py:507] global step 465: loss = 2.4932 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 466: loss = 3.3563 (0.624 sec/step)\n",
            "I0915 13:50:41.717118 139848901543808 learning.py:507] global step 466: loss = 3.3563 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 467: loss = 4.5674 (0.626 sec/step)\n",
            "I0915 13:50:42.344994 139848901543808 learning.py:507] global step 467: loss = 4.5674 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 468: loss = 3.1760 (0.639 sec/step)\n",
            "I0915 13:50:42.986004 139848901543808 learning.py:507] global step 468: loss = 3.1760 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 469: loss = 3.5757 (0.639 sec/step)\n",
            "I0915 13:50:43.627211 139848901543808 learning.py:507] global step 469: loss = 3.5757 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 470: loss = 2.5665 (0.620 sec/step)\n",
            "I0915 13:50:44.249289 139848901543808 learning.py:507] global step 470: loss = 2.5665 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 471: loss = 3.1682 (0.632 sec/step)\n",
            "I0915 13:50:44.883083 139848901543808 learning.py:507] global step 471: loss = 3.1682 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 472: loss = 2.5773 (0.675 sec/step)\n",
            "I0915 13:50:45.559874 139848901543808 learning.py:507] global step 472: loss = 2.5773 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 473: loss = 4.2878 (0.627 sec/step)\n",
            "I0915 13:50:46.188737 139848901543808 learning.py:507] global step 473: loss = 4.2878 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 474: loss = 2.5487 (0.645 sec/step)\n",
            "I0915 13:50:46.836443 139848901543808 learning.py:507] global step 474: loss = 2.5487 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 475: loss = 2.0971 (0.633 sec/step)\n",
            "I0915 13:50:47.471768 139848901543808 learning.py:507] global step 475: loss = 2.0971 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 476: loss = 3.8031 (0.642 sec/step)\n",
            "I0915 13:50:48.115161 139848901543808 learning.py:507] global step 476: loss = 3.8031 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 477: loss = 3.8964 (0.643 sec/step)\n",
            "I0915 13:50:48.759829 139848901543808 learning.py:507] global step 477: loss = 3.8964 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 478: loss = 2.9678 (0.629 sec/step)\n",
            "I0915 13:50:49.390572 139848901543808 learning.py:507] global step 478: loss = 2.9678 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 479: loss = 4.0024 (0.614 sec/step)\n",
            "I0915 13:50:50.007040 139848901543808 learning.py:507] global step 479: loss = 4.0024 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 480: loss = 3.2099 (0.652 sec/step)\n",
            "I0915 13:50:50.660743 139848901543808 learning.py:507] global step 480: loss = 3.2099 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 481: loss = 2.5198 (0.615 sec/step)\n",
            "I0915 13:50:51.277395 139848901543808 learning.py:507] global step 481: loss = 2.5198 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 482: loss = 2.4157 (0.639 sec/step)\n",
            "I0915 13:50:51.918142 139848901543808 learning.py:507] global step 482: loss = 2.4157 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 483: loss = 2.5036 (0.612 sec/step)\n",
            "I0915 13:50:52.532326 139848901543808 learning.py:507] global step 483: loss = 2.5036 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 484: loss = 2.8910 (0.633 sec/step)\n",
            "I0915 13:50:53.167565 139848901543808 learning.py:507] global step 484: loss = 2.8910 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 485: loss = 2.1043 (0.628 sec/step)\n",
            "I0915 13:50:53.797149 139848901543808 learning.py:507] global step 485: loss = 2.1043 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 486: loss = 3.6924 (0.648 sec/step)\n",
            "I0915 13:50:54.447887 139848901543808 learning.py:507] global step 486: loss = 3.6924 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 487: loss = 2.9431 (0.638 sec/step)\n",
            "I0915 13:50:55.088523 139848901543808 learning.py:507] global step 487: loss = 2.9431 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 488: loss = 2.9089 (0.637 sec/step)\n",
            "I0915 13:50:55.727966 139848901543808 learning.py:507] global step 488: loss = 2.9089 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 489: loss = 2.9533 (0.625 sec/step)\n",
            "I0915 13:50:56.355165 139848901543808 learning.py:507] global step 489: loss = 2.9533 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 490: loss = 3.1491 (0.627 sec/step)\n",
            "I0915 13:50:56.984125 139848901543808 learning.py:507] global step 490: loss = 3.1491 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 491: loss = 3.2317 (0.636 sec/step)\n",
            "I0915 13:50:57.622597 139848901543808 learning.py:507] global step 491: loss = 3.2317 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 492: loss = 2.8676 (0.645 sec/step)\n",
            "I0915 13:50:58.270188 139848901543808 learning.py:507] global step 492: loss = 2.8676 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 493: loss = 3.1632 (0.626 sec/step)\n",
            "I0915 13:50:58.898039 139848901543808 learning.py:507] global step 493: loss = 3.1632 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 494: loss = 3.0377 (0.636 sec/step)\n",
            "I0915 13:50:59.536213 139848901543808 learning.py:507] global step 494: loss = 3.0377 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 495: loss = 2.6509 (0.640 sec/step)\n",
            "I0915 13:51:00.178459 139848901543808 learning.py:507] global step 495: loss = 2.6509 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 496: loss = 2.9870 (0.626 sec/step)\n",
            "I0915 13:51:00.806160 139848901543808 learning.py:507] global step 496: loss = 2.9870 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 497: loss = 3.2354 (0.611 sec/step)\n",
            "I0915 13:51:01.418906 139848901543808 learning.py:507] global step 497: loss = 3.2354 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 498: loss = 2.7102 (0.615 sec/step)\n",
            "I0915 13:51:02.035529 139848901543808 learning.py:507] global step 498: loss = 2.7102 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 499: loss = 2.7264 (0.605 sec/step)\n",
            "I0915 13:51:02.642418 139848901543808 learning.py:507] global step 499: loss = 2.7264 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 500: loss = 2.0983 (0.639 sec/step)\n",
            "I0915 13:51:03.282839 139848901543808 learning.py:507] global step 500: loss = 2.0983 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 501: loss = 2.6997 (0.634 sec/step)\n",
            "I0915 13:51:03.919340 139848901543808 learning.py:507] global step 501: loss = 2.6997 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 502: loss = 3.1885 (0.592 sec/step)\n",
            "I0915 13:51:04.513026 139848901543808 learning.py:507] global step 502: loss = 3.1885 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 503: loss = 2.7148 (0.605 sec/step)\n",
            "I0915 13:51:05.119535 139848901543808 learning.py:507] global step 503: loss = 2.7148 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 504: loss = 3.5621 (0.618 sec/step)\n",
            "I0915 13:51:05.739946 139848901543808 learning.py:507] global step 504: loss = 3.5621 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 505: loss = 2.9665 (0.637 sec/step)\n",
            "I0915 13:51:06.378830 139848901543808 learning.py:507] global step 505: loss = 2.9665 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 506: loss = 2.5171 (0.610 sec/step)\n",
            "I0915 13:51:06.991791 139848901543808 learning.py:507] global step 506: loss = 2.5171 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 507: loss = 2.4832 (0.617 sec/step)\n",
            "I0915 13:51:07.610432 139848901543808 learning.py:507] global step 507: loss = 2.4832 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 508: loss = 3.4832 (0.596 sec/step)\n",
            "I0915 13:51:08.208473 139848901543808 learning.py:507] global step 508: loss = 3.4832 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 509: loss = 2.9283 (0.636 sec/step)\n",
            "I0915 13:51:08.851324 139848901543808 learning.py:507] global step 509: loss = 2.9283 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 510: loss = 4.0057 (0.630 sec/step)\n",
            "I0915 13:51:09.482791 139848901543808 learning.py:507] global step 510: loss = 4.0057 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 511: loss = 3.2718 (0.623 sec/step)\n",
            "I0915 13:51:10.108287 139848901543808 learning.py:507] global step 511: loss = 3.2718 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 512: loss = 4.5542 (0.634 sec/step)\n",
            "I0915 13:51:10.744075 139848901543808 learning.py:507] global step 512: loss = 4.5542 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 513: loss = 3.1040 (0.611 sec/step)\n",
            "I0915 13:51:11.356787 139848901543808 learning.py:507] global step 513: loss = 3.1040 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 514: loss = 4.1037 (0.611 sec/step)\n",
            "I0915 13:51:11.969815 139848901543808 learning.py:507] global step 514: loss = 4.1037 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 515: loss = 3.7787 (0.612 sec/step)\n",
            "I0915 13:51:12.583727 139848901543808 learning.py:507] global step 515: loss = 3.7787 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 516: loss = 3.1830 (0.626 sec/step)\n",
            "I0915 13:51:13.211456 139848901543808 learning.py:507] global step 516: loss = 3.1830 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 517: loss = 3.5250 (0.636 sec/step)\n",
            "I0915 13:51:13.849027 139848901543808 learning.py:507] global step 517: loss = 3.5250 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 518: loss = 3.5873 (0.625 sec/step)\n",
            "I0915 13:51:14.475809 139848901543808 learning.py:507] global step 518: loss = 3.5873 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 519: loss = 2.9900 (0.612 sec/step)\n",
            "I0915 13:51:15.089790 139848901543808 learning.py:507] global step 519: loss = 2.9900 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 520: loss = 3.1216 (0.622 sec/step)\n",
            "I0915 13:51:15.713567 139848901543808 learning.py:507] global step 520: loss = 3.1216 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 521: loss = 3.3447 (0.648 sec/step)\n",
            "I0915 13:51:16.363210 139848901543808 learning.py:507] global step 521: loss = 3.3447 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 522: loss = 3.7321 (0.610 sec/step)\n",
            "I0915 13:51:16.975456 139848901543808 learning.py:507] global step 522: loss = 3.7321 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 523: loss = 2.5629 (0.603 sec/step)\n",
            "I0915 13:51:17.580161 139848901543808 learning.py:507] global step 523: loss = 2.5629 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 524: loss = 3.3499 (0.624 sec/step)\n",
            "I0915 13:51:18.205718 139848901543808 learning.py:507] global step 524: loss = 3.3499 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 525: loss = 3.1045 (0.603 sec/step)\n",
            "I0915 13:51:18.810451 139848901543808 learning.py:507] global step 525: loss = 3.1045 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 526: loss = 3.2233 (0.654 sec/step)\n",
            "I0915 13:51:19.466884 139848901543808 learning.py:507] global step 526: loss = 3.2233 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 527: loss = 2.8767 (0.614 sec/step)\n",
            "I0915 13:51:20.083136 139848901543808 learning.py:507] global step 527: loss = 2.8767 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 528: loss = 2.9023 (0.608 sec/step)\n",
            "I0915 13:51:20.693340 139848901543808 learning.py:507] global step 528: loss = 2.9023 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 529: loss = 3.0804 (0.632 sec/step)\n",
            "I0915 13:51:21.327683 139848901543808 learning.py:507] global step 529: loss = 3.0804 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 530: loss = 2.7504 (0.616 sec/step)\n",
            "I0915 13:51:21.945521 139848901543808 learning.py:507] global step 530: loss = 2.7504 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 531: loss = 3.5062 (0.614 sec/step)\n",
            "I0915 13:51:22.562466 139848901543808 learning.py:507] global step 531: loss = 3.5062 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 532: loss = 2.8004 (0.714 sec/step)\n",
            "I0915 13:51:23.278957 139848901543808 learning.py:507] global step 532: loss = 2.8004 (0.714 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 532.\n",
            "I0915 13:51:24.073931 139845834331904 supervisor.py:1050] Recording summary at step 532.\n",
            "INFO:tensorflow:global step 533: loss = 3.1454 (1.019 sec/step)\n",
            "I0915 13:51:24.300466 139848901543808 learning.py:507] global step 533: loss = 3.1454 (1.019 sec/step)\n",
            "INFO:tensorflow:global step 534: loss = 2.3438 (0.617 sec/step)\n",
            "I0915 13:51:24.919540 139848901543808 learning.py:507] global step 534: loss = 2.3438 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 535: loss = 2.4513 (0.657 sec/step)\n",
            "I0915 13:51:25.579033 139848901543808 learning.py:507] global step 535: loss = 2.4513 (0.657 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.5733\n",
            "I0915 13:51:25.979162 139845842724608 supervisor.py:1099] global_step/sec: 1.5733\n",
            "INFO:tensorflow:global step 536: loss = 1.9142 (0.610 sec/step)\n",
            "I0915 13:51:26.190723 139848901543808 learning.py:507] global step 536: loss = 1.9142 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 537: loss = 3.0792 (0.644 sec/step)\n",
            "I0915 13:51:26.837408 139848901543808 learning.py:507] global step 537: loss = 3.0792 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 538: loss = 2.8685 (0.602 sec/step)\n",
            "I0915 13:51:27.441807 139848901543808 learning.py:507] global step 538: loss = 2.8685 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 539: loss = 2.3938 (0.643 sec/step)\n",
            "I0915 13:51:28.086331 139848901543808 learning.py:507] global step 539: loss = 2.3938 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 540: loss = 2.7291 (0.641 sec/step)\n",
            "I0915 13:51:28.728921 139848901543808 learning.py:507] global step 540: loss = 2.7291 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 541: loss = 1.8156 (0.624 sec/step)\n",
            "I0915 13:51:29.354764 139848901543808 learning.py:507] global step 541: loss = 1.8156 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 542: loss = 3.0958 (0.641 sec/step)\n",
            "I0915 13:51:29.997795 139848901543808 learning.py:507] global step 542: loss = 3.0958 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 543: loss = 2.7911 (0.623 sec/step)\n",
            "I0915 13:51:30.622570 139848901543808 learning.py:507] global step 543: loss = 2.7911 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 544: loss = 1.9434 (0.626 sec/step)\n",
            "I0915 13:51:31.250199 139848901543808 learning.py:507] global step 544: loss = 1.9434 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 545: loss = 2.5670 (0.643 sec/step)\n",
            "I0915 13:51:31.895436 139848901543808 learning.py:507] global step 545: loss = 2.5670 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 546: loss = 3.4936 (0.637 sec/step)\n",
            "I0915 13:51:32.534207 139848901543808 learning.py:507] global step 546: loss = 3.4936 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 547: loss = 1.9728 (0.614 sec/step)\n",
            "I0915 13:51:33.151950 139848901543808 learning.py:507] global step 547: loss = 1.9728 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 548: loss = 4.1621 (0.635 sec/step)\n",
            "I0915 13:51:33.788897 139848901543808 learning.py:507] global step 548: loss = 4.1621 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 549: loss = 4.0306 (0.642 sec/step)\n",
            "I0915 13:51:34.432393 139848901543808 learning.py:507] global step 549: loss = 4.0306 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 550: loss = 2.6994 (0.644 sec/step)\n",
            "I0915 13:51:35.078727 139848901543808 learning.py:507] global step 550: loss = 2.6994 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 551: loss = 2.6840 (0.620 sec/step)\n",
            "I0915 13:51:35.700626 139848901543808 learning.py:507] global step 551: loss = 2.6840 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 552: loss = 2.9066 (0.637 sec/step)\n",
            "I0915 13:51:36.338835 139848901543808 learning.py:507] global step 552: loss = 2.9066 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 553: loss = 3.3038 (0.655 sec/step)\n",
            "I0915 13:51:36.996040 139848901543808 learning.py:507] global step 553: loss = 3.3038 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 554: loss = 2.9126 (0.603 sec/step)\n",
            "I0915 13:51:37.601024 139848901543808 learning.py:507] global step 554: loss = 2.9126 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 555: loss = 2.6699 (0.610 sec/step)\n",
            "I0915 13:51:38.212541 139848901543808 learning.py:507] global step 555: loss = 2.6699 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 556: loss = 2.5571 (0.633 sec/step)\n",
            "I0915 13:51:38.847140 139848901543808 learning.py:507] global step 556: loss = 2.5571 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 557: loss = 3.6022 (0.613 sec/step)\n",
            "I0915 13:51:39.461904 139848901543808 learning.py:507] global step 557: loss = 3.6022 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 558: loss = 3.1760 (0.641 sec/step)\n",
            "I0915 13:51:40.105151 139848901543808 learning.py:507] global step 558: loss = 3.1760 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 559: loss = 3.2936 (0.639 sec/step)\n",
            "I0915 13:51:40.746188 139848901543808 learning.py:507] global step 559: loss = 3.2936 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 560: loss = 1.7871 (0.605 sec/step)\n",
            "I0915 13:51:41.353783 139848901543808 learning.py:507] global step 560: loss = 1.7871 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 561: loss = 2.4002 (0.625 sec/step)\n",
            "I0915 13:51:41.980797 139848901543808 learning.py:507] global step 561: loss = 2.4002 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 562: loss = 2.5808 (0.611 sec/step)\n",
            "I0915 13:51:42.593961 139848901543808 learning.py:507] global step 562: loss = 2.5808 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 563: loss = 3.4317 (0.617 sec/step)\n",
            "I0915 13:51:43.213305 139848901543808 learning.py:507] global step 563: loss = 3.4317 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 564: loss = 3.2640 (0.612 sec/step)\n",
            "I0915 13:51:43.827745 139848901543808 learning.py:507] global step 564: loss = 3.2640 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 565: loss = 2.7227 (0.617 sec/step)\n",
            "I0915 13:51:44.446399 139848901543808 learning.py:507] global step 565: loss = 2.7227 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 566: loss = 3.0002 (0.632 sec/step)\n",
            "I0915 13:51:45.080548 139848901543808 learning.py:507] global step 566: loss = 3.0002 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 567: loss = 2.0172 (0.623 sec/step)\n",
            "I0915 13:51:45.705151 139848901543808 learning.py:507] global step 567: loss = 2.0172 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 568: loss = 2.8926 (0.649 sec/step)\n",
            "I0915 13:51:46.355858 139848901543808 learning.py:507] global step 568: loss = 2.8926 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 569: loss = 2.9252 (0.628 sec/step)\n",
            "I0915 13:51:46.986112 139848901543808 learning.py:507] global step 569: loss = 2.9252 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 570: loss = 2.8562 (0.616 sec/step)\n",
            "I0915 13:51:47.603867 139848901543808 learning.py:507] global step 570: loss = 2.8562 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 571: loss = 2.5164 (0.616 sec/step)\n",
            "I0915 13:51:48.221718 139848901543808 learning.py:507] global step 571: loss = 2.5164 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 572: loss = 3.2106 (0.622 sec/step)\n",
            "I0915 13:51:48.845899 139848901543808 learning.py:507] global step 572: loss = 3.2106 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 573: loss = 3.6128 (0.630 sec/step)\n",
            "I0915 13:51:49.477824 139848901543808 learning.py:507] global step 573: loss = 3.6128 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 574: loss = 3.8394 (0.616 sec/step)\n",
            "I0915 13:51:50.096312 139848901543808 learning.py:507] global step 574: loss = 3.8394 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 575: loss = 2.7333 (0.625 sec/step)\n",
            "I0915 13:51:50.722929 139848901543808 learning.py:507] global step 575: loss = 2.7333 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 576: loss = 2.8545 (0.636 sec/step)\n",
            "I0915 13:51:51.360298 139848901543808 learning.py:507] global step 576: loss = 2.8545 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 577: loss = 3.8577 (0.616 sec/step)\n",
            "I0915 13:51:51.978255 139848901543808 learning.py:507] global step 577: loss = 3.8577 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 578: loss = 2.0345 (0.632 sec/step)\n",
            "I0915 13:51:52.611529 139848901543808 learning.py:507] global step 578: loss = 2.0345 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 579: loss = 2.3737 (0.616 sec/step)\n",
            "I0915 13:51:53.229382 139848901543808 learning.py:507] global step 579: loss = 2.3737 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 580: loss = 2.6549 (0.631 sec/step)\n",
            "I0915 13:51:53.862386 139848901543808 learning.py:507] global step 580: loss = 2.6549 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 581: loss = 3.1846 (0.621 sec/step)\n",
            "I0915 13:51:54.484942 139848901543808 learning.py:507] global step 581: loss = 3.1846 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 582: loss = 3.3328 (0.641 sec/step)\n",
            "I0915 13:51:55.128923 139848901543808 learning.py:507] global step 582: loss = 3.3328 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 583: loss = 3.1364 (0.630 sec/step)\n",
            "I0915 13:51:55.761295 139848901543808 learning.py:507] global step 583: loss = 3.1364 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 584: loss = 3.2470 (0.622 sec/step)\n",
            "I0915 13:51:56.386267 139848901543808 learning.py:507] global step 584: loss = 3.2470 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 585: loss = 2.3841 (0.628 sec/step)\n",
            "I0915 13:51:57.016452 139848901543808 learning.py:507] global step 585: loss = 2.3841 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 586: loss = 3.4400 (0.637 sec/step)\n",
            "I0915 13:51:57.655308 139848901543808 learning.py:507] global step 586: loss = 3.4400 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 587: loss = 3.3354 (0.619 sec/step)\n",
            "I0915 13:51:58.276556 139848901543808 learning.py:507] global step 587: loss = 3.3354 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 588: loss = 2.0164 (0.608 sec/step)\n",
            "I0915 13:51:58.886575 139848901543808 learning.py:507] global step 588: loss = 2.0164 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 589: loss = 3.3574 (0.636 sec/step)\n",
            "I0915 13:51:59.524363 139848901543808 learning.py:507] global step 589: loss = 3.3574 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 590: loss = 2.4876 (0.645 sec/step)\n",
            "I0915 13:52:00.171735 139848901543808 learning.py:507] global step 590: loss = 2.4876 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 591: loss = 2.4045 (0.658 sec/step)\n",
            "I0915 13:52:00.831711 139848901543808 learning.py:507] global step 591: loss = 2.4045 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 592: loss = 2.4555 (0.645 sec/step)\n",
            "I0915 13:52:01.478956 139848901543808 learning.py:507] global step 592: loss = 2.4555 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 593: loss = 3.1363 (0.644 sec/step)\n",
            "I0915 13:52:02.124694 139848901543808 learning.py:507] global step 593: loss = 3.1363 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 594: loss = 3.7396 (0.640 sec/step)\n",
            "I0915 13:52:02.767145 139848901543808 learning.py:507] global step 594: loss = 3.7396 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 595: loss = 2.5619 (0.647 sec/step)\n",
            "I0915 13:52:03.416103 139848901543808 learning.py:507] global step 595: loss = 2.5619 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 596: loss = 1.9861 (0.632 sec/step)\n",
            "I0915 13:52:04.049548 139848901543808 learning.py:507] global step 596: loss = 1.9861 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 597: loss = 2.3703 (0.625 sec/step)\n",
            "I0915 13:52:04.676131 139848901543808 learning.py:507] global step 597: loss = 2.3703 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 598: loss = 4.0448 (0.622 sec/step)\n",
            "I0915 13:52:05.299826 139848901543808 learning.py:507] global step 598: loss = 4.0448 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 599: loss = 2.9022 (0.623 sec/step)\n",
            "I0915 13:52:05.924574 139848901543808 learning.py:507] global step 599: loss = 2.9022 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 3.2425 (0.613 sec/step)\n",
            "I0915 13:52:06.539762 139848901543808 learning.py:507] global step 600: loss = 3.2425 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 601: loss = 1.9244 (0.638 sec/step)\n",
            "I0915 13:52:07.179657 139848901543808 learning.py:507] global step 601: loss = 1.9244 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 602: loss = 2.2527 (0.639 sec/step)\n",
            "I0915 13:52:07.820079 139848901543808 learning.py:507] global step 602: loss = 2.2527 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 603: loss = 2.5565 (0.612 sec/step)\n",
            "I0915 13:52:08.433702 139848901543808 learning.py:507] global step 603: loss = 2.5565 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 604: loss = 1.9686 (0.641 sec/step)\n",
            "I0915 13:52:09.076229 139848901543808 learning.py:507] global step 604: loss = 1.9686 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 605: loss = 2.3004 (0.633 sec/step)\n",
            "I0915 13:52:09.711251 139848901543808 learning.py:507] global step 605: loss = 2.3004 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 606: loss = 2.4253 (0.611 sec/step)\n",
            "I0915 13:52:10.324015 139848901543808 learning.py:507] global step 606: loss = 2.4253 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 607: loss = 2.9763 (0.614 sec/step)\n",
            "I0915 13:52:10.939724 139848901543808 learning.py:507] global step 607: loss = 2.9763 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 608: loss = 3.8855 (0.616 sec/step)\n",
            "I0915 13:52:11.557263 139848901543808 learning.py:507] global step 608: loss = 3.8855 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 609: loss = 2.9026 (0.629 sec/step)\n",
            "I0915 13:52:12.188207 139848901543808 learning.py:507] global step 609: loss = 2.9026 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 610: loss = 2.7932 (0.631 sec/step)\n",
            "I0915 13:52:12.821328 139848901543808 learning.py:507] global step 610: loss = 2.7932 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 611: loss = 3.0306 (0.627 sec/step)\n",
            "I0915 13:52:13.450075 139848901543808 learning.py:507] global step 611: loss = 3.0306 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 612: loss = 2.7235 (0.638 sec/step)\n",
            "I0915 13:52:14.089720 139848901543808 learning.py:507] global step 612: loss = 2.7235 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 613: loss = 2.4304 (0.611 sec/step)\n",
            "I0915 13:52:14.704868 139848901543808 learning.py:507] global step 613: loss = 2.4304 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 614: loss = 4.2597 (0.635 sec/step)\n",
            "I0915 13:52:15.343167 139848901543808 learning.py:507] global step 614: loss = 4.2597 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 615: loss = 3.3435 (0.615 sec/step)\n",
            "I0915 13:52:15.959608 139848901543808 learning.py:507] global step 615: loss = 3.3435 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 616: loss = 2.7301 (0.620 sec/step)\n",
            "I0915 13:52:16.581209 139848901543808 learning.py:507] global step 616: loss = 2.7301 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 617: loss = 3.4636 (0.636 sec/step)\n",
            "I0915 13:52:17.219014 139848901543808 learning.py:507] global step 617: loss = 3.4636 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 618: loss = 2.7587 (0.638 sec/step)\n",
            "I0915 13:52:17.858634 139848901543808 learning.py:507] global step 618: loss = 2.7587 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 619: loss = 3.4550 (0.625 sec/step)\n",
            "I0915 13:52:18.485949 139848901543808 learning.py:507] global step 619: loss = 3.4550 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 620: loss = 2.7973 (0.645 sec/step)\n",
            "I0915 13:52:19.132340 139848901543808 learning.py:507] global step 620: loss = 2.7973 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 621: loss = 2.2989 (0.612 sec/step)\n",
            "I0915 13:52:19.746284 139848901543808 learning.py:507] global step 621: loss = 2.2989 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 622: loss = 1.8079 (0.657 sec/step)\n",
            "I0915 13:52:20.405393 139848901543808 learning.py:507] global step 622: loss = 1.8079 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 623: loss = 2.6781 (0.623 sec/step)\n",
            "I0915 13:52:21.029981 139848901543808 learning.py:507] global step 623: loss = 2.6781 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 624: loss = 2.8574 (0.625 sec/step)\n",
            "I0915 13:52:21.656907 139848901543808 learning.py:507] global step 624: loss = 2.8574 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 625: loss = 2.2060 (0.617 sec/step)\n",
            "I0915 13:52:22.276588 139848901543808 learning.py:507] global step 625: loss = 2.2060 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 626: loss = 1.9184 (0.629 sec/step)\n",
            "I0915 13:52:22.907632 139848901543808 learning.py:507] global step 626: loss = 1.9184 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 627: loss = 3.4871 (0.631 sec/step)\n",
            "I0915 13:52:23.543919 139848901543808 learning.py:507] global step 627: loss = 3.4871 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 628: loss = 2.1267 (0.608 sec/step)\n",
            "I0915 13:52:24.153870 139848901543808 learning.py:507] global step 628: loss = 2.1267 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 629: loss = 3.5399 (0.604 sec/step)\n",
            "I0915 13:52:24.759598 139848901543808 learning.py:507] global step 629: loss = 3.5399 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 630: loss = 3.3822 (0.646 sec/step)\n",
            "I0915 13:52:25.407454 139848901543808 learning.py:507] global step 630: loss = 3.3822 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 631: loss = 2.1343 (0.617 sec/step)\n",
            "I0915 13:52:26.026678 139848901543808 learning.py:507] global step 631: loss = 2.1343 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 632: loss = 3.0057 (0.613 sec/step)\n",
            "I0915 13:52:26.641390 139848901543808 learning.py:507] global step 632: loss = 3.0057 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 633: loss = 3.3444 (0.630 sec/step)\n",
            "I0915 13:52:27.273647 139848901543808 learning.py:507] global step 633: loss = 3.3444 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 634: loss = 3.4672 (0.627 sec/step)\n",
            "I0915 13:52:27.901875 139848901543808 learning.py:507] global step 634: loss = 3.4672 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 635: loss = 3.2750 (0.670 sec/step)\n",
            "I0915 13:52:28.573616 139848901543808 learning.py:507] global step 635: loss = 3.2750 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 636: loss = 2.9271 (0.589 sec/step)\n",
            "I0915 13:52:29.164680 139848901543808 learning.py:507] global step 636: loss = 2.9271 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 637: loss = 2.6604 (0.648 sec/step)\n",
            "I0915 13:52:29.814699 139848901543808 learning.py:507] global step 637: loss = 2.6604 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 638: loss = 3.4015 (0.631 sec/step)\n",
            "I0915 13:52:30.447264 139848901543808 learning.py:507] global step 638: loss = 3.4015 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 639: loss = 2.0789 (0.617 sec/step)\n",
            "I0915 13:52:31.066812 139848901543808 learning.py:507] global step 639: loss = 2.0789 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 640: loss = 2.7691 (0.633 sec/step)\n",
            "I0915 13:52:31.701927 139848901543808 learning.py:507] global step 640: loss = 2.7691 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 641: loss = 2.0145 (0.641 sec/step)\n",
            "I0915 13:52:32.344686 139848901543808 learning.py:507] global step 641: loss = 2.0145 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 642: loss = 3.6407 (0.624 sec/step)\n",
            "I0915 13:52:32.970658 139848901543808 learning.py:507] global step 642: loss = 3.6407 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 643: loss = 2.9276 (0.633 sec/step)\n",
            "I0915 13:52:33.605052 139848901543808 learning.py:507] global step 643: loss = 2.9276 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 644: loss = 2.1008 (0.619 sec/step)\n",
            "I0915 13:52:34.225735 139848901543808 learning.py:507] global step 644: loss = 2.1008 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 645: loss = 2.4105 (0.621 sec/step)\n",
            "I0915 13:52:34.848305 139848901543808 learning.py:507] global step 645: loss = 2.4105 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 646: loss = 2.5926 (0.618 sec/step)\n",
            "I0915 13:52:35.468122 139848901543808 learning.py:507] global step 646: loss = 2.5926 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 647: loss = 2.6694 (0.644 sec/step)\n",
            "I0915 13:52:36.114655 139848901543808 learning.py:507] global step 647: loss = 2.6694 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 648: loss = 2.0217 (0.627 sec/step)\n",
            "I0915 13:52:36.743188 139848901543808 learning.py:507] global step 648: loss = 2.0217 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 649: loss = 2.8781 (0.618 sec/step)\n",
            "I0915 13:52:37.363455 139848901543808 learning.py:507] global step 649: loss = 2.8781 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 650: loss = 2.9705 (0.607 sec/step)\n",
            "I0915 13:52:37.971853 139848901543808 learning.py:507] global step 650: loss = 2.9705 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 651: loss = 2.4539 (0.635 sec/step)\n",
            "I0915 13:52:38.608571 139848901543808 learning.py:507] global step 651: loss = 2.4539 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 652: loss = 2.1814 (0.635 sec/step)\n",
            "I0915 13:52:39.246111 139848901543808 learning.py:507] global step 652: loss = 2.1814 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 653: loss = 2.2051 (0.620 sec/step)\n",
            "I0915 13:52:39.868075 139848901543808 learning.py:507] global step 653: loss = 2.2051 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 654: loss = 3.0898 (0.610 sec/step)\n",
            "I0915 13:52:40.480368 139848901543808 learning.py:507] global step 654: loss = 3.0898 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 655: loss = 4.3449 (0.621 sec/step)\n",
            "I0915 13:52:41.103567 139848901543808 learning.py:507] global step 655: loss = 4.3449 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 656: loss = 2.8387 (0.606 sec/step)\n",
            "I0915 13:52:41.712863 139848901543808 learning.py:507] global step 656: loss = 2.8387 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 657: loss = 3.0419 (0.611 sec/step)\n",
            "I0915 13:52:42.326436 139848901543808 learning.py:507] global step 657: loss = 3.0419 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 658: loss = 3.0374 (0.628 sec/step)\n",
            "I0915 13:52:42.956824 139848901543808 learning.py:507] global step 658: loss = 3.0374 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 659: loss = 3.2810 (0.628 sec/step)\n",
            "I0915 13:52:43.586453 139848901543808 learning.py:507] global step 659: loss = 3.2810 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 660: loss = 2.1902 (0.614 sec/step)\n",
            "I0915 13:52:44.202689 139848901543808 learning.py:507] global step 660: loss = 2.1902 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 661: loss = 2.6216 (0.625 sec/step)\n",
            "I0915 13:52:44.829648 139848901543808 learning.py:507] global step 661: loss = 2.6216 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 662: loss = 3.4505 (0.622 sec/step)\n",
            "I0915 13:52:45.453849 139848901543808 learning.py:507] global step 662: loss = 3.4505 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 663: loss = 2.4098 (0.650 sec/step)\n",
            "I0915 13:52:46.106433 139848901543808 learning.py:507] global step 663: loss = 2.4098 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 664: loss = 2.3528 (0.619 sec/step)\n",
            "I0915 13:52:46.727653 139848901543808 learning.py:507] global step 664: loss = 2.3528 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 665: loss = 3.3096 (0.622 sec/step)\n",
            "I0915 13:52:47.351972 139848901543808 learning.py:507] global step 665: loss = 3.3096 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 666: loss = 4.2626 (0.633 sec/step)\n",
            "I0915 13:52:47.987176 139848901543808 learning.py:507] global step 666: loss = 4.2626 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 667: loss = 3.2678 (0.636 sec/step)\n",
            "I0915 13:52:48.625691 139848901543808 learning.py:507] global step 667: loss = 3.2678 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 668: loss = 2.7550 (0.644 sec/step)\n",
            "I0915 13:52:49.273258 139848901543808 learning.py:507] global step 668: loss = 2.7550 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 669: loss = 3.6518 (0.601 sec/step)\n",
            "I0915 13:52:49.876185 139848901543808 learning.py:507] global step 669: loss = 3.6518 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 670: loss = 3.2258 (0.612 sec/step)\n",
            "I0915 13:52:50.490060 139848901543808 learning.py:507] global step 670: loss = 3.2258 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 671: loss = 3.6415 (0.614 sec/step)\n",
            "I0915 13:52:51.105523 139848901543808 learning.py:507] global step 671: loss = 3.6415 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 672: loss = 3.1614 (0.625 sec/step)\n",
            "I0915 13:52:51.732729 139848901543808 learning.py:507] global step 672: loss = 3.1614 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 673: loss = 2.4265 (0.612 sec/step)\n",
            "I0915 13:52:52.346651 139848901543808 learning.py:507] global step 673: loss = 2.4265 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 674: loss = 2.7612 (0.617 sec/step)\n",
            "I0915 13:52:52.965485 139848901543808 learning.py:507] global step 674: loss = 2.7612 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 675: loss = 3.0943 (0.612 sec/step)\n",
            "I0915 13:52:53.579890 139848901543808 learning.py:507] global step 675: loss = 3.0943 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 676: loss = 3.2598 (0.621 sec/step)\n",
            "I0915 13:52:54.202768 139848901543808 learning.py:507] global step 676: loss = 3.2598 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 677: loss = 2.3245 (0.612 sec/step)\n",
            "I0915 13:52:54.817247 139848901543808 learning.py:507] global step 677: loss = 2.3245 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 678: loss = 2.9443 (0.611 sec/step)\n",
            "I0915 13:52:55.430402 139848901543808 learning.py:507] global step 678: loss = 2.9443 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 679: loss = 2.4955 (0.625 sec/step)\n",
            "I0915 13:52:56.057294 139848901543808 learning.py:507] global step 679: loss = 2.4955 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 680: loss = 2.6574 (0.620 sec/step)\n",
            "I0915 13:52:56.679152 139848901543808 learning.py:507] global step 680: loss = 2.6574 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 681: loss = 3.4272 (0.625 sec/step)\n",
            "I0915 13:52:57.305506 139848901543808 learning.py:507] global step 681: loss = 3.4272 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 682: loss = 2.5319 (0.610 sec/step)\n",
            "I0915 13:52:57.917512 139848901543808 learning.py:507] global step 682: loss = 2.5319 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 683: loss = 2.4855 (0.597 sec/step)\n",
            "I0915 13:52:58.516932 139848901543808 learning.py:507] global step 683: loss = 2.4855 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 684: loss = 3.0540 (0.621 sec/step)\n",
            "I0915 13:52:59.140088 139848901543808 learning.py:507] global step 684: loss = 3.0540 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 685: loss = 2.9016 (0.633 sec/step)\n",
            "I0915 13:52:59.774801 139848901543808 learning.py:507] global step 685: loss = 2.9016 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 686: loss = 3.4573 (0.614 sec/step)\n",
            "I0915 13:53:00.391224 139848901543808 learning.py:507] global step 686: loss = 3.4573 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 687: loss = 2.3144 (0.594 sec/step)\n",
            "I0915 13:53:00.987068 139848901543808 learning.py:507] global step 687: loss = 2.3144 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 688: loss = 3.1220 (0.613 sec/step)\n",
            "I0915 13:53:01.602041 139848901543808 learning.py:507] global step 688: loss = 3.1220 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 689: loss = 3.9192 (0.622 sec/step)\n",
            "I0915 13:53:02.225555 139848901543808 learning.py:507] global step 689: loss = 3.9192 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 690: loss = 2.2855 (0.624 sec/step)\n",
            "I0915 13:53:02.852099 139848901543808 learning.py:507] global step 690: loss = 2.2855 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 691: loss = 2.3393 (0.622 sec/step)\n",
            "I0915 13:53:03.476165 139848901543808 learning.py:507] global step 691: loss = 2.3393 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 692: loss = 2.8502 (0.626 sec/step)\n",
            "I0915 13:53:04.104233 139848901543808 learning.py:507] global step 692: loss = 2.8502 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 693: loss = 2.4827 (0.634 sec/step)\n",
            "I0915 13:53:04.740418 139848901543808 learning.py:507] global step 693: loss = 2.4827 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 694: loss = 2.0294 (0.630 sec/step)\n",
            "I0915 13:53:05.372281 139848901543808 learning.py:507] global step 694: loss = 2.0294 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 695: loss = 2.4109 (0.614 sec/step)\n",
            "I0915 13:53:05.988082 139848901543808 learning.py:507] global step 695: loss = 2.4109 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 696: loss = 3.0756 (0.631 sec/step)\n",
            "I0915 13:53:06.621252 139848901543808 learning.py:507] global step 696: loss = 3.0756 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 697: loss = 4.6965 (0.625 sec/step)\n",
            "I0915 13:53:07.248577 139848901543808 learning.py:507] global step 697: loss = 4.6965 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 698: loss = 3.2701 (0.634 sec/step)\n",
            "I0915 13:53:07.883964 139848901543808 learning.py:507] global step 698: loss = 3.2701 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 699: loss = 2.0611 (0.638 sec/step)\n",
            "I0915 13:53:08.524340 139848901543808 learning.py:507] global step 699: loss = 2.0611 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 2.4379 (0.639 sec/step)\n",
            "I0915 13:53:09.164806 139848901543808 learning.py:507] global step 700: loss = 2.4379 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 701: loss = 2.1335 (0.625 sec/step)\n",
            "I0915 13:53:09.792685 139848901543808 learning.py:507] global step 701: loss = 2.1335 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 702: loss = 3.0792 (0.646 sec/step)\n",
            "I0915 13:53:10.441276 139848901543808 learning.py:507] global step 702: loss = 3.0792 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 703: loss = 2.3602 (0.609 sec/step)\n",
            "I0915 13:53:11.051861 139848901543808 learning.py:507] global step 703: loss = 2.3602 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 704: loss = 3.3051 (0.614 sec/step)\n",
            "I0915 13:53:11.667421 139848901543808 learning.py:507] global step 704: loss = 3.3051 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 705: loss = 2.4305 (0.609 sec/step)\n",
            "I0915 13:53:12.278034 139848901543808 learning.py:507] global step 705: loss = 2.4305 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 706: loss = 2.3871 (0.616 sec/step)\n",
            "I0915 13:53:12.895547 139848901543808 learning.py:507] global step 706: loss = 2.3871 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 707: loss = 3.5176 (0.623 sec/step)\n",
            "I0915 13:53:13.520203 139848901543808 learning.py:507] global step 707: loss = 3.5176 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 708: loss = 2.8606 (0.605 sec/step)\n",
            "I0915 13:53:14.127382 139848901543808 learning.py:507] global step 708: loss = 2.8606 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 709: loss = 3.0062 (0.617 sec/step)\n",
            "I0915 13:53:14.745556 139848901543808 learning.py:507] global step 709: loss = 3.0062 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 710: loss = 2.2035 (0.633 sec/step)\n",
            "I0915 13:53:15.380975 139848901543808 learning.py:507] global step 710: loss = 2.2035 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 711: loss = 2.9679 (0.612 sec/step)\n",
            "I0915 13:53:15.994941 139848901543808 learning.py:507] global step 711: loss = 2.9679 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 712: loss = 2.7738 (0.621 sec/step)\n",
            "I0915 13:53:16.618263 139848901543808 learning.py:507] global step 712: loss = 2.7738 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 713: loss = 2.4991 (0.615 sec/step)\n",
            "I0915 13:53:17.235215 139848901543808 learning.py:507] global step 713: loss = 2.4991 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 714: loss = 2.5685 (0.640 sec/step)\n",
            "I0915 13:53:17.877530 139848901543808 learning.py:507] global step 714: loss = 2.5685 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 715: loss = 1.8940 (0.615 sec/step)\n",
            "I0915 13:53:18.494792 139848901543808 learning.py:507] global step 715: loss = 1.8940 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 716: loss = 2.8142 (0.602 sec/step)\n",
            "I0915 13:53:19.098595 139848901543808 learning.py:507] global step 716: loss = 2.8142 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 717: loss = 3.5526 (0.630 sec/step)\n",
            "I0915 13:53:19.730821 139848901543808 learning.py:507] global step 717: loss = 3.5526 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 718: loss = 2.8051 (0.663 sec/step)\n",
            "I0915 13:53:20.395796 139848901543808 learning.py:507] global step 718: loss = 2.8051 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 719: loss = 2.3905 (0.606 sec/step)\n",
            "I0915 13:53:21.003758 139848901543808 learning.py:507] global step 719: loss = 2.3905 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 720: loss = 2.4870 (0.621 sec/step)\n",
            "I0915 13:53:21.626175 139848901543808 learning.py:507] global step 720: loss = 2.4870 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 721: loss = 2.7788 (0.618 sec/step)\n",
            "I0915 13:53:22.246232 139848901543808 learning.py:507] global step 721: loss = 2.7788 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 722: loss = 1.9844 (0.655 sec/step)\n",
            "I0915 13:53:22.911267 139848901543808 learning.py:507] global step 722: loss = 1.9844 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 723: loss = 2.9620 (1.036 sec/step)\n",
            "I0915 13:53:23.958199 139848901543808 learning.py:507] global step 723: loss = 2.9620 (1.036 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 723.\n",
            "I0915 13:53:24.110773 139845834331904 supervisor.py:1050] Recording summary at step 723.\n",
            "INFO:tensorflow:global step 724: loss = 2.9792 (0.618 sec/step)\n",
            "I0915 13:53:24.579620 139848901543808 learning.py:507] global step 724: loss = 2.9792 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 725: loss = 1.9167 (0.628 sec/step)\n",
            "I0915 13:53:25.209233 139848901543808 learning.py:507] global step 725: loss = 1.9167 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 726: loss = 3.1885 (0.624 sec/step)\n",
            "I0915 13:53:25.834676 139848901543808 learning.py:507] global step 726: loss = 3.1885 (0.624 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.59317\n",
            "I0915 13:53:25.865790 139845842724608 supervisor.py:1099] global_step/sec: 1.59317\n",
            "INFO:tensorflow:global step 727: loss = 2.5067 (0.609 sec/step)\n",
            "I0915 13:53:26.445800 139848901543808 learning.py:507] global step 727: loss = 2.5067 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 728: loss = 3.1390 (0.615 sec/step)\n",
            "I0915 13:53:27.062277 139848901543808 learning.py:507] global step 728: loss = 3.1390 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 729: loss = 2.0608 (0.617 sec/step)\n",
            "I0915 13:53:27.681662 139848901543808 learning.py:507] global step 729: loss = 2.0608 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 730: loss = 3.8441 (0.609 sec/step)\n",
            "I0915 13:53:28.292607 139848901543808 learning.py:507] global step 730: loss = 3.8441 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 731: loss = 2.3815 (0.616 sec/step)\n",
            "I0915 13:53:28.910601 139848901543808 learning.py:507] global step 731: loss = 2.3815 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 732: loss = 2.1930 (0.606 sec/step)\n",
            "I0915 13:53:29.518412 139848901543808 learning.py:507] global step 732: loss = 2.1930 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 733: loss = 2.5642 (0.656 sec/step)\n",
            "I0915 13:53:30.176394 139848901543808 learning.py:507] global step 733: loss = 2.5642 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 734: loss = 2.1799 (0.643 sec/step)\n",
            "I0915 13:53:30.820908 139848901543808 learning.py:507] global step 734: loss = 2.1799 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 735: loss = 2.0609 (0.606 sec/step)\n",
            "I0915 13:53:31.428704 139848901543808 learning.py:507] global step 735: loss = 2.0609 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 736: loss = 2.7242 (0.619 sec/step)\n",
            "I0915 13:53:32.049725 139848901543808 learning.py:507] global step 736: loss = 2.7242 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 737: loss = 2.6493 (0.628 sec/step)\n",
            "I0915 13:53:32.679778 139848901543808 learning.py:507] global step 737: loss = 2.6493 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 738: loss = 2.9067 (0.627 sec/step)\n",
            "I0915 13:53:33.308755 139848901543808 learning.py:507] global step 738: loss = 2.9067 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 739: loss = 2.3343 (0.626 sec/step)\n",
            "I0915 13:53:33.936989 139848901543808 learning.py:507] global step 739: loss = 2.3343 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 740: loss = 1.5167 (0.614 sec/step)\n",
            "I0915 13:53:34.553116 139848901543808 learning.py:507] global step 740: loss = 1.5167 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 741: loss = 3.1736 (0.621 sec/step)\n",
            "I0915 13:53:35.175690 139848901543808 learning.py:507] global step 741: loss = 3.1736 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 742: loss = 2.7980 (0.619 sec/step)\n",
            "I0915 13:53:35.796161 139848901543808 learning.py:507] global step 742: loss = 2.7980 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 743: loss = 2.9356 (0.620 sec/step)\n",
            "I0915 13:53:36.418421 139848901543808 learning.py:507] global step 743: loss = 2.9356 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 744: loss = 2.7194 (0.613 sec/step)\n",
            "I0915 13:53:37.033518 139848901543808 learning.py:507] global step 744: loss = 2.7194 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 745: loss = 3.5168 (0.634 sec/step)\n",
            "I0915 13:53:37.669712 139848901543808 learning.py:507] global step 745: loss = 3.5168 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 746: loss = 2.1198 (0.625 sec/step)\n",
            "I0915 13:53:38.296622 139848901543808 learning.py:507] global step 746: loss = 2.1198 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 747: loss = 3.0169 (0.609 sec/step)\n",
            "I0915 13:53:38.907690 139848901543808 learning.py:507] global step 747: loss = 3.0169 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 748: loss = 3.1325 (0.641 sec/step)\n",
            "I0915 13:53:39.550080 139848901543808 learning.py:507] global step 748: loss = 3.1325 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 749: loss = 2.2023 (0.622 sec/step)\n",
            "I0915 13:53:40.174388 139848901543808 learning.py:507] global step 749: loss = 2.2023 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 750: loss = 2.9854 (0.635 sec/step)\n",
            "I0915 13:53:40.811334 139848901543808 learning.py:507] global step 750: loss = 2.9854 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 751: loss = 3.1119 (0.618 sec/step)\n",
            "I0915 13:53:41.430960 139848901543808 learning.py:507] global step 751: loss = 3.1119 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 752: loss = 2.4835 (0.618 sec/step)\n",
            "I0915 13:53:42.050482 139848901543808 learning.py:507] global step 752: loss = 2.4835 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 753: loss = 3.6309 (0.637 sec/step)\n",
            "I0915 13:53:42.689965 139848901543808 learning.py:507] global step 753: loss = 3.6309 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 754: loss = 2.0737 (0.647 sec/step)\n",
            "I0915 13:53:43.338644 139848901543808 learning.py:507] global step 754: loss = 2.0737 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 755: loss = 2.7905 (0.616 sec/step)\n",
            "I0915 13:53:43.957029 139848901543808 learning.py:507] global step 755: loss = 2.7905 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 756: loss = 2.9581 (0.621 sec/step)\n",
            "I0915 13:53:44.580226 139848901543808 learning.py:507] global step 756: loss = 2.9581 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 757: loss = 2.5297 (0.619 sec/step)\n",
            "I0915 13:53:45.201075 139848901543808 learning.py:507] global step 757: loss = 2.5297 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 758: loss = 2.3814 (0.627 sec/step)\n",
            "I0915 13:53:45.830279 139848901543808 learning.py:507] global step 758: loss = 2.3814 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 759: loss = 2.4659 (0.636 sec/step)\n",
            "I0915 13:53:46.468238 139848901543808 learning.py:507] global step 759: loss = 2.4659 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 760: loss = 2.2232 (0.628 sec/step)\n",
            "I0915 13:53:47.097545 139848901543808 learning.py:507] global step 760: loss = 2.2232 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 761: loss = 2.5364 (0.613 sec/step)\n",
            "I0915 13:53:47.712560 139848901543808 learning.py:507] global step 761: loss = 2.5364 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 762: loss = 2.8555 (0.606 sec/step)\n",
            "I0915 13:53:48.320759 139848901543808 learning.py:507] global step 762: loss = 2.8555 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 763: loss = 3.1788 (0.616 sec/step)\n",
            "I0915 13:53:48.938425 139848901543808 learning.py:507] global step 763: loss = 3.1788 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 764: loss = 3.7469 (0.619 sec/step)\n",
            "I0915 13:53:49.560250 139848901543808 learning.py:507] global step 764: loss = 3.7469 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 765: loss = 1.9261 (0.600 sec/step)\n",
            "I0915 13:53:50.161983 139848901543808 learning.py:507] global step 765: loss = 1.9261 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 766: loss = 2.6443 (0.634 sec/step)\n",
            "I0915 13:53:50.798392 139848901543808 learning.py:507] global step 766: loss = 2.6443 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 767: loss = 3.5064 (0.633 sec/step)\n",
            "I0915 13:53:51.433739 139848901543808 learning.py:507] global step 767: loss = 3.5064 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 768: loss = 2.4950 (0.607 sec/step)\n",
            "I0915 13:53:52.042661 139848901543808 learning.py:507] global step 768: loss = 2.4950 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 769: loss = 3.3620 (0.633 sec/step)\n",
            "I0915 13:53:52.677718 139848901543808 learning.py:507] global step 769: loss = 3.3620 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 770: loss = 2.6978 (0.626 sec/step)\n",
            "I0915 13:53:53.304935 139848901543808 learning.py:507] global step 770: loss = 2.6978 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 771: loss = 3.6872 (0.652 sec/step)\n",
            "I0915 13:53:53.958545 139848901543808 learning.py:507] global step 771: loss = 3.6872 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 772: loss = 2.8921 (0.626 sec/step)\n",
            "I0915 13:53:54.586442 139848901543808 learning.py:507] global step 772: loss = 2.8921 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 773: loss = 2.3935 (0.656 sec/step)\n",
            "I0915 13:53:55.244641 139848901543808 learning.py:507] global step 773: loss = 2.3935 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 774: loss = 2.5443 (0.619 sec/step)\n",
            "I0915 13:53:55.865306 139848901543808 learning.py:507] global step 774: loss = 2.5443 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 775: loss = 2.3106 (0.622 sec/step)\n",
            "I0915 13:53:56.488850 139848901543808 learning.py:507] global step 775: loss = 2.3106 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 776: loss = 3.2935 (0.611 sec/step)\n",
            "I0915 13:53:57.101874 139848901543808 learning.py:507] global step 776: loss = 3.2935 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 777: loss = 1.9888 (0.621 sec/step)\n",
            "I0915 13:53:57.724754 139848901543808 learning.py:507] global step 777: loss = 1.9888 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 778: loss = 3.1120 (0.613 sec/step)\n",
            "I0915 13:53:58.339036 139848901543808 learning.py:507] global step 778: loss = 3.1120 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 779: loss = 2.3796 (0.617 sec/step)\n",
            "I0915 13:53:58.959122 139848901543808 learning.py:507] global step 779: loss = 2.3796 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 780: loss = 3.4172 (0.618 sec/step)\n",
            "I0915 13:53:59.579411 139848901543808 learning.py:507] global step 780: loss = 3.4172 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 781: loss = 2.7768 (0.624 sec/step)\n",
            "I0915 13:54:00.205742 139848901543808 learning.py:507] global step 781: loss = 2.7768 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 782: loss = 2.3364 (0.621 sec/step)\n",
            "I0915 13:54:00.829561 139848901543808 learning.py:507] global step 782: loss = 2.3364 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 783: loss = 2.8662 (0.632 sec/step)\n",
            "I0915 13:54:01.463101 139848901543808 learning.py:507] global step 783: loss = 2.8662 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 784: loss = 3.4936 (0.603 sec/step)\n",
            "I0915 13:54:02.068404 139848901543808 learning.py:507] global step 784: loss = 3.4936 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 785: loss = 2.3527 (0.607 sec/step)\n",
            "I0915 13:54:02.677230 139848901543808 learning.py:507] global step 785: loss = 2.3527 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 786: loss = 2.4251 (0.612 sec/step)\n",
            "I0915 13:54:03.291306 139848901543808 learning.py:507] global step 786: loss = 2.4251 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 787: loss = 2.2335 (0.607 sec/step)\n",
            "I0915 13:54:03.899789 139848901543808 learning.py:507] global step 787: loss = 2.2335 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 788: loss = 2.5340 (0.619 sec/step)\n",
            "I0915 13:54:04.520896 139848901543808 learning.py:507] global step 788: loss = 2.5340 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 789: loss = 2.2498 (0.610 sec/step)\n",
            "I0915 13:54:05.133419 139848901543808 learning.py:507] global step 789: loss = 2.2498 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 790: loss = 2.0079 (0.589 sec/step)\n",
            "I0915 13:54:05.724162 139848901543808 learning.py:507] global step 790: loss = 2.0079 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 791: loss = 2.0744 (0.605 sec/step)\n",
            "I0915 13:54:06.331179 139848901543808 learning.py:507] global step 791: loss = 2.0744 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 792: loss = 3.1439 (0.631 sec/step)\n",
            "I0915 13:54:06.963683 139848901543808 learning.py:507] global step 792: loss = 3.1439 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 793: loss = 2.0682 (0.605 sec/step)\n",
            "I0915 13:54:07.570479 139848901543808 learning.py:507] global step 793: loss = 2.0682 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 794: loss = 2.6933 (0.619 sec/step)\n",
            "I0915 13:54:08.190987 139848901543808 learning.py:507] global step 794: loss = 2.6933 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 795: loss = 2.6105 (0.599 sec/step)\n",
            "I0915 13:54:08.791553 139848901543808 learning.py:507] global step 795: loss = 2.6105 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 796: loss = 2.5076 (0.600 sec/step)\n",
            "I0915 13:54:09.393829 139848901543808 learning.py:507] global step 796: loss = 2.5076 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 797: loss = 2.8571 (0.666 sec/step)\n",
            "I0915 13:54:10.061662 139848901543808 learning.py:507] global step 797: loss = 2.8571 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 798: loss = 2.5114 (0.637 sec/step)\n",
            "I0915 13:54:10.700538 139848901543808 learning.py:507] global step 798: loss = 2.5114 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 799: loss = 2.7590 (0.628 sec/step)\n",
            "I0915 13:54:11.330253 139848901543808 learning.py:507] global step 799: loss = 2.7590 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 2.3036 (0.647 sec/step)\n",
            "I0915 13:54:11.978634 139848901543808 learning.py:507] global step 800: loss = 2.3036 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 801: loss = 2.7475 (0.618 sec/step)\n",
            "I0915 13:54:12.599242 139848901543808 learning.py:507] global step 801: loss = 2.7475 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 802: loss = 3.2919 (0.636 sec/step)\n",
            "I0915 13:54:13.237210 139848901543808 learning.py:507] global step 802: loss = 3.2919 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 803: loss = 2.4523 (0.630 sec/step)\n",
            "I0915 13:54:13.869245 139848901543808 learning.py:507] global step 803: loss = 2.4523 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 804: loss = 2.5314 (0.618 sec/step)\n",
            "I0915 13:54:14.489134 139848901543808 learning.py:507] global step 804: loss = 2.5314 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 805: loss = 1.7782 (0.662 sec/step)\n",
            "I0915 13:54:15.152821 139848901543808 learning.py:507] global step 805: loss = 1.7782 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 806: loss = 2.2043 (0.627 sec/step)\n",
            "I0915 13:54:15.782112 139848901543808 learning.py:507] global step 806: loss = 2.2043 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 807: loss = 2.5165 (0.635 sec/step)\n",
            "I0915 13:54:16.418886 139848901543808 learning.py:507] global step 807: loss = 2.5165 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 808: loss = 3.5378 (0.616 sec/step)\n",
            "I0915 13:54:17.037184 139848901543808 learning.py:507] global step 808: loss = 3.5378 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 809: loss = 2.4773 (0.622 sec/step)\n",
            "I0915 13:54:17.660679 139848901543808 learning.py:507] global step 809: loss = 2.4773 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 810: loss = 2.8941 (0.632 sec/step)\n",
            "I0915 13:54:18.294953 139848901543808 learning.py:507] global step 810: loss = 2.8941 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 811: loss = 2.4536 (0.634 sec/step)\n",
            "I0915 13:54:18.930792 139848901543808 learning.py:507] global step 811: loss = 2.4536 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 812: loss = 3.3500 (0.650 sec/step)\n",
            "I0915 13:54:19.582828 139848901543808 learning.py:507] global step 812: loss = 3.3500 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 813: loss = 2.5286 (0.616 sec/step)\n",
            "I0915 13:54:20.200132 139848901543808 learning.py:507] global step 813: loss = 2.5286 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 814: loss = 2.3547 (0.649 sec/step)\n",
            "I0915 13:54:20.850539 139848901543808 learning.py:507] global step 814: loss = 2.3547 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 815: loss = 3.0418 (0.640 sec/step)\n",
            "I0915 13:54:21.492274 139848901543808 learning.py:507] global step 815: loss = 3.0418 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 816: loss = 2.2545 (0.646 sec/step)\n",
            "I0915 13:54:22.140805 139848901543808 learning.py:507] global step 816: loss = 2.2545 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 817: loss = 2.5326 (0.628 sec/step)\n",
            "I0915 13:54:22.770549 139848901543808 learning.py:507] global step 817: loss = 2.5326 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 818: loss = 2.9222 (0.614 sec/step)\n",
            "I0915 13:54:23.386289 139848901543808 learning.py:507] global step 818: loss = 2.9222 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 819: loss = 2.1293 (0.654 sec/step)\n",
            "I0915 13:54:24.042672 139848901543808 learning.py:507] global step 819: loss = 2.1293 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 820: loss = 1.9147 (0.652 sec/step)\n",
            "I0915 13:54:24.696212 139848901543808 learning.py:507] global step 820: loss = 1.9147 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 821: loss = 2.7289 (0.622 sec/step)\n",
            "I0915 13:54:25.320177 139848901543808 learning.py:507] global step 821: loss = 2.7289 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 822: loss = 2.2913 (0.635 sec/step)\n",
            "I0915 13:54:25.957781 139848901543808 learning.py:507] global step 822: loss = 2.2913 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 823: loss = 2.4070 (0.634 sec/step)\n",
            "I0915 13:54:26.593492 139848901543808 learning.py:507] global step 823: loss = 2.4070 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 824: loss = 4.2726 (0.645 sec/step)\n",
            "I0915 13:54:27.240453 139848901543808 learning.py:507] global step 824: loss = 4.2726 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 825: loss = 3.6406 (0.679 sec/step)\n",
            "I0915 13:54:27.921876 139848901543808 learning.py:507] global step 825: loss = 3.6406 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 826: loss = 2.6382 (0.638 sec/step)\n",
            "I0915 13:54:28.561416 139848901543808 learning.py:507] global step 826: loss = 2.6382 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 827: loss = 2.7036 (0.636 sec/step)\n",
            "I0915 13:54:29.199424 139848901543808 learning.py:507] global step 827: loss = 2.7036 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 828: loss = 2.6277 (0.647 sec/step)\n",
            "I0915 13:54:29.848948 139848901543808 learning.py:507] global step 828: loss = 2.6277 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 829: loss = 2.6545 (0.637 sec/step)\n",
            "I0915 13:54:30.488256 139848901543808 learning.py:507] global step 829: loss = 2.6545 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 830: loss = 2.2118 (0.638 sec/step)\n",
            "I0915 13:54:31.128825 139848901543808 learning.py:507] global step 830: loss = 2.2118 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 831: loss = 2.6702 (0.641 sec/step)\n",
            "I0915 13:54:31.771557 139848901543808 learning.py:507] global step 831: loss = 2.6702 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 832: loss = 3.0642 (0.643 sec/step)\n",
            "I0915 13:54:32.416203 139848901543808 learning.py:507] global step 832: loss = 3.0642 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 833: loss = 1.8224 (0.640 sec/step)\n",
            "I0915 13:54:33.058757 139848901543808 learning.py:507] global step 833: loss = 1.8224 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 834: loss = 2.4316 (0.638 sec/step)\n",
            "I0915 13:54:33.698991 139848901543808 learning.py:507] global step 834: loss = 2.4316 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 835: loss = 3.0464 (0.643 sec/step)\n",
            "I0915 13:54:34.344412 139848901543808 learning.py:507] global step 835: loss = 3.0464 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 836: loss = 3.3510 (0.642 sec/step)\n",
            "I0915 13:54:34.987977 139848901543808 learning.py:507] global step 836: loss = 3.3510 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 837: loss = 1.8969 (0.637 sec/step)\n",
            "I0915 13:54:35.627286 139848901543808 learning.py:507] global step 837: loss = 1.8969 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 838: loss = 2.6802 (0.652 sec/step)\n",
            "I0915 13:54:36.281785 139848901543808 learning.py:507] global step 838: loss = 2.6802 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 839: loss = 2.4004 (0.628 sec/step)\n",
            "I0915 13:54:36.911924 139848901543808 learning.py:507] global step 839: loss = 2.4004 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 840: loss = 1.6160 (0.638 sec/step)\n",
            "I0915 13:54:37.551822 139848901543808 learning.py:507] global step 840: loss = 1.6160 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 841: loss = 2.8265 (0.649 sec/step)\n",
            "I0915 13:54:38.202465 139848901543808 learning.py:507] global step 841: loss = 2.8265 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 842: loss = 3.9428 (0.642 sec/step)\n",
            "I0915 13:54:38.846659 139848901543808 learning.py:507] global step 842: loss = 3.9428 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 843: loss = 2.0818 (0.634 sec/step)\n",
            "I0915 13:54:39.482950 139848901543808 learning.py:507] global step 843: loss = 2.0818 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 844: loss = 2.6218 (0.660 sec/step)\n",
            "I0915 13:54:40.145151 139848901543808 learning.py:507] global step 844: loss = 2.6218 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 845: loss = 2.8995 (0.634 sec/step)\n",
            "I0915 13:54:40.781081 139848901543808 learning.py:507] global step 845: loss = 2.8995 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 846: loss = 1.9768 (0.627 sec/step)\n",
            "I0915 13:54:41.409413 139848901543808 learning.py:507] global step 846: loss = 1.9768 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 847: loss = 3.0142 (0.639 sec/step)\n",
            "I0915 13:54:42.050742 139848901543808 learning.py:507] global step 847: loss = 3.0142 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 848: loss = 2.5805 (0.640 sec/step)\n",
            "I0915 13:54:42.692484 139848901543808 learning.py:507] global step 848: loss = 2.5805 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 849: loss = 1.9165 (0.647 sec/step)\n",
            "I0915 13:54:43.341116 139848901543808 learning.py:507] global step 849: loss = 1.9165 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 850: loss = 2.2208 (0.643 sec/step)\n",
            "I0915 13:54:43.986239 139848901543808 learning.py:507] global step 850: loss = 2.2208 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 851: loss = 2.0804 (0.632 sec/step)\n",
            "I0915 13:54:44.620300 139848901543808 learning.py:507] global step 851: loss = 2.0804 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 852: loss = 2.8142 (0.636 sec/step)\n",
            "I0915 13:54:45.258334 139848901543808 learning.py:507] global step 852: loss = 2.8142 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 853: loss = 2.1109 (0.643 sec/step)\n",
            "I0915 13:54:45.903031 139848901543808 learning.py:507] global step 853: loss = 2.1109 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 854: loss = 2.3761 (0.667 sec/step)\n",
            "I0915 13:54:46.571884 139848901543808 learning.py:507] global step 854: loss = 2.3761 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 855: loss = 2.0328 (0.629 sec/step)\n",
            "I0915 13:54:47.202996 139848901543808 learning.py:507] global step 855: loss = 2.0328 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 856: loss = 2.3664 (0.658 sec/step)\n",
            "I0915 13:54:47.863418 139848901543808 learning.py:507] global step 856: loss = 2.3664 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 857: loss = 2.7227 (0.632 sec/step)\n",
            "I0915 13:54:48.497300 139848901543808 learning.py:507] global step 857: loss = 2.7227 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 858: loss = 2.8467 (0.645 sec/step)\n",
            "I0915 13:54:49.143725 139848901543808 learning.py:507] global step 858: loss = 2.8467 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 859: loss = 1.9537 (0.610 sec/step)\n",
            "I0915 13:54:49.755899 139848901543808 learning.py:507] global step 859: loss = 1.9537 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 860: loss = 2.5044 (0.640 sec/step)\n",
            "I0915 13:54:50.398368 139848901543808 learning.py:507] global step 860: loss = 2.5044 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 861: loss = 2.8124 (0.642 sec/step)\n",
            "I0915 13:54:51.042141 139848901543808 learning.py:507] global step 861: loss = 2.8124 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 862: loss = 2.6188 (0.635 sec/step)\n",
            "I0915 13:54:51.678649 139848901543808 learning.py:507] global step 862: loss = 2.6188 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 863: loss = 2.5165 (0.654 sec/step)\n",
            "I0915 13:54:52.334555 139848901543808 learning.py:507] global step 863: loss = 2.5165 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 864: loss = 3.3625 (0.622 sec/step)\n",
            "I0915 13:54:52.958137 139848901543808 learning.py:507] global step 864: loss = 3.3625 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 865: loss = 3.0207 (0.627 sec/step)\n",
            "I0915 13:54:53.586879 139848901543808 learning.py:507] global step 865: loss = 3.0207 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 866: loss = 2.5138 (0.643 sec/step)\n",
            "I0915 13:54:54.231871 139848901543808 learning.py:507] global step 866: loss = 2.5138 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 867: loss = 2.1530 (0.645 sec/step)\n",
            "I0915 13:54:54.878701 139848901543808 learning.py:507] global step 867: loss = 2.1530 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 868: loss = 2.2742 (0.641 sec/step)\n",
            "I0915 13:54:55.521828 139848901543808 learning.py:507] global step 868: loss = 2.2742 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 869: loss = 1.7255 (0.646 sec/step)\n",
            "I0915 13:54:56.169731 139848901543808 learning.py:507] global step 869: loss = 1.7255 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 870: loss = 2.8388 (0.627 sec/step)\n",
            "I0915 13:54:56.799149 139848901543808 learning.py:507] global step 870: loss = 2.8388 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 871: loss = 3.0709 (0.625 sec/step)\n",
            "I0915 13:54:57.426438 139848901543808 learning.py:507] global step 871: loss = 3.0709 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 872: loss = 2.2920 (0.630 sec/step)\n",
            "I0915 13:54:58.058379 139848901543808 learning.py:507] global step 872: loss = 2.2920 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 873: loss = 4.0055 (0.638 sec/step)\n",
            "I0915 13:54:58.699032 139848901543808 learning.py:507] global step 873: loss = 4.0055 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 874: loss = 2.8331 (0.636 sec/step)\n",
            "I0915 13:54:59.336749 139848901543808 learning.py:507] global step 874: loss = 2.8331 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 875: loss = 2.9409 (0.638 sec/step)\n",
            "I0915 13:54:59.976500 139848901543808 learning.py:507] global step 875: loss = 2.9409 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 876: loss = 3.6981 (0.641 sec/step)\n",
            "I0915 13:55:00.619609 139848901543808 learning.py:507] global step 876: loss = 3.6981 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 877: loss = 2.7545 (0.644 sec/step)\n",
            "I0915 13:55:01.266762 139848901543808 learning.py:507] global step 877: loss = 2.7545 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 878: loss = 3.3896 (0.627 sec/step)\n",
            "I0915 13:55:01.895632 139848901543808 learning.py:507] global step 878: loss = 3.3896 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 879: loss = 2.7101 (0.644 sec/step)\n",
            "I0915 13:55:02.541169 139848901543808 learning.py:507] global step 879: loss = 2.7101 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 880: loss = 2.9173 (0.621 sec/step)\n",
            "I0915 13:55:03.163551 139848901543808 learning.py:507] global step 880: loss = 2.9173 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 881: loss = 3.4274 (0.681 sec/step)\n",
            "I0915 13:55:03.846369 139848901543808 learning.py:507] global step 881: loss = 3.4274 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 882: loss = 2.8715 (0.629 sec/step)\n",
            "I0915 13:55:04.477458 139848901543808 learning.py:507] global step 882: loss = 2.8715 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 883: loss = 3.0032 (0.648 sec/step)\n",
            "I0915 13:55:05.127285 139848901543808 learning.py:507] global step 883: loss = 3.0032 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 884: loss = 2.4461 (0.650 sec/step)\n",
            "I0915 13:55:05.778694 139848901543808 learning.py:507] global step 884: loss = 2.4461 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 885: loss = 2.3875 (0.629 sec/step)\n",
            "I0915 13:55:06.409896 139848901543808 learning.py:507] global step 885: loss = 2.3875 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 886: loss = 2.7912 (0.618 sec/step)\n",
            "I0915 13:55:07.030880 139848901543808 learning.py:507] global step 886: loss = 2.7912 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 887: loss = 2.2828 (0.635 sec/step)\n",
            "I0915 13:55:07.668210 139848901543808 learning.py:507] global step 887: loss = 2.2828 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 888: loss = 2.2164 (0.630 sec/step)\n",
            "I0915 13:55:08.299515 139848901543808 learning.py:507] global step 888: loss = 2.2164 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 889: loss = 1.7651 (0.646 sec/step)\n",
            "I0915 13:55:08.947408 139848901543808 learning.py:507] global step 889: loss = 1.7651 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 890: loss = 2.8420 (0.639 sec/step)\n",
            "I0915 13:55:09.588447 139848901543808 learning.py:507] global step 890: loss = 2.8420 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 891: loss = 2.8544 (0.615 sec/step)\n",
            "I0915 13:55:10.204970 139848901543808 learning.py:507] global step 891: loss = 2.8544 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 892: loss = 2.2710 (0.641 sec/step)\n",
            "I0915 13:55:10.848421 139848901543808 learning.py:507] global step 892: loss = 2.2710 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 893: loss = 1.9597 (0.654 sec/step)\n",
            "I0915 13:55:11.504518 139848901543808 learning.py:507] global step 893: loss = 1.9597 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 894: loss = 2.4382 (0.637 sec/step)\n",
            "I0915 13:55:12.144066 139848901543808 learning.py:507] global step 894: loss = 2.4382 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 895: loss = 3.1543 (0.647 sec/step)\n",
            "I0915 13:55:12.793994 139848901543808 learning.py:507] global step 895: loss = 3.1543 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 896: loss = 2.3323 (0.633 sec/step)\n",
            "I0915 13:55:13.429638 139848901543808 learning.py:507] global step 896: loss = 2.3323 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 897: loss = 3.4161 (0.656 sec/step)\n",
            "I0915 13:55:14.087870 139848901543808 learning.py:507] global step 897: loss = 3.4161 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 898: loss = 1.8560 (0.629 sec/step)\n",
            "I0915 13:55:14.718482 139848901543808 learning.py:507] global step 898: loss = 1.8560 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 899: loss = 2.4560 (0.641 sec/step)\n",
            "I0915 13:55:15.361363 139848901543808 learning.py:507] global step 899: loss = 2.4560 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 2.7019 (0.655 sec/step)\n",
            "I0915 13:55:16.018507 139848901543808 learning.py:507] global step 900: loss = 2.7019 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 901: loss = 2.2996 (0.633 sec/step)\n",
            "I0915 13:55:16.653724 139848901543808 learning.py:507] global step 901: loss = 2.2996 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 902: loss = 3.0966 (0.637 sec/step)\n",
            "I0915 13:55:17.292484 139848901543808 learning.py:507] global step 902: loss = 3.0966 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 903: loss = 3.3555 (0.636 sec/step)\n",
            "I0915 13:55:17.930635 139848901543808 learning.py:507] global step 903: loss = 3.3555 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 904: loss = 3.6502 (0.644 sec/step)\n",
            "I0915 13:55:18.577995 139848901543808 learning.py:507] global step 904: loss = 3.6502 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 905: loss = 2.2121 (0.644 sec/step)\n",
            "I0915 13:55:19.224342 139848901543808 learning.py:507] global step 905: loss = 2.2121 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 906: loss = 1.6635 (0.643 sec/step)\n",
            "I0915 13:55:19.869126 139848901543808 learning.py:507] global step 906: loss = 1.6635 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 907: loss = 2.3433 (0.630 sec/step)\n",
            "I0915 13:55:20.501373 139848901543808 learning.py:507] global step 907: loss = 2.3433 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 908: loss = 3.3960 (0.647 sec/step)\n",
            "I0915 13:55:21.150305 139848901543808 learning.py:507] global step 908: loss = 3.3960 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 909: loss = 1.8668 (0.623 sec/step)\n",
            "I0915 13:55:21.774563 139848901543808 learning.py:507] global step 909: loss = 1.8668 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 910: loss = 1.9590 (0.640 sec/step)\n",
            "I0915 13:55:22.416455 139848901543808 learning.py:507] global step 910: loss = 1.9590 (0.640 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "I0915 13:55:22.837265 139845851117312 supervisor.py:1117] Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "INFO:tensorflow:global step 911: loss = 2.9216 (0.910 sec/step)\n",
            "I0915 13:55:23.356793 139848901543808 learning.py:507] global step 911: loss = 2.9216 (0.910 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 911.\n",
            "I0915 13:55:24.386166 139845834331904 supervisor.py:1050] Recording summary at step 911.\n",
            "INFO:tensorflow:global step 912: loss = 2.5006 (1.307 sec/step)\n",
            "I0915 13:55:24.671016 139848901543808 learning.py:507] global step 912: loss = 2.5006 (1.307 sec/step)\n",
            "INFO:tensorflow:global step 913: loss = 2.0145 (0.805 sec/step)\n",
            "I0915 13:55:25.493316 139848901543808 learning.py:507] global step 913: loss = 2.0145 (0.805 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.55797\n",
            "I0915 13:55:25.893692 139845842724608 supervisor.py:1099] global_step/sec: 1.55797\n",
            "INFO:tensorflow:global step 914: loss = 2.0533 (1.016 sec/step)\n",
            "I0915 13:55:26.782444 139848901543808 learning.py:507] global step 914: loss = 2.0533 (1.016 sec/step)\n",
            "INFO:tensorflow:global step 915: loss = 2.3236 (0.846 sec/step)\n",
            "I0915 13:55:27.631299 139848901543808 learning.py:507] global step 915: loss = 2.3236 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 916: loss = 2.8311 (0.647 sec/step)\n",
            "I0915 13:55:28.280552 139848901543808 learning.py:507] global step 916: loss = 2.8311 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 917: loss = 2.5624 (0.630 sec/step)\n",
            "I0915 13:55:28.913243 139848901543808 learning.py:507] global step 917: loss = 2.5624 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 918: loss = 2.6162 (0.644 sec/step)\n",
            "I0915 13:55:29.558754 139848901543808 learning.py:507] global step 918: loss = 2.6162 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 919: loss = 2.1899 (0.636 sec/step)\n",
            "I0915 13:55:30.196873 139848901543808 learning.py:507] global step 919: loss = 2.1899 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 920: loss = 2.1270 (0.629 sec/step)\n",
            "I0915 13:55:30.828097 139848901543808 learning.py:507] global step 920: loss = 2.1270 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 921: loss = 2.6945 (0.652 sec/step)\n",
            "I0915 13:55:31.482527 139848901543808 learning.py:507] global step 921: loss = 2.6945 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 922: loss = 2.7522 (0.620 sec/step)\n",
            "I0915 13:55:32.104823 139848901543808 learning.py:507] global step 922: loss = 2.7522 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 923: loss = 1.8370 (0.636 sec/step)\n",
            "I0915 13:55:32.742959 139848901543808 learning.py:507] global step 923: loss = 1.8370 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 924: loss = 2.6699 (0.643 sec/step)\n",
            "I0915 13:55:33.388218 139848901543808 learning.py:507] global step 924: loss = 2.6699 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 925: loss = 2.0491 (0.641 sec/step)\n",
            "I0915 13:55:34.031364 139848901543808 learning.py:507] global step 925: loss = 2.0491 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 926: loss = 3.4692 (0.680 sec/step)\n",
            "I0915 13:55:34.713559 139848901543808 learning.py:507] global step 926: loss = 3.4692 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 927: loss = 1.5354 (0.621 sec/step)\n",
            "I0915 13:55:35.336516 139848901543808 learning.py:507] global step 927: loss = 1.5354 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 928: loss = 1.9205 (0.657 sec/step)\n",
            "I0915 13:55:35.995734 139848901543808 learning.py:507] global step 928: loss = 1.9205 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 929: loss = 3.3550 (0.647 sec/step)\n",
            "I0915 13:55:36.644774 139848901543808 learning.py:507] global step 929: loss = 3.3550 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 930: loss = 2.7884 (0.626 sec/step)\n",
            "I0915 13:55:37.273309 139848901543808 learning.py:507] global step 930: loss = 2.7884 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 931: loss = 2.6119 (0.648 sec/step)\n",
            "I0915 13:55:37.924526 139848901543808 learning.py:507] global step 931: loss = 2.6119 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 932: loss = 2.4120 (0.664 sec/step)\n",
            "I0915 13:55:38.591307 139848901543808 learning.py:507] global step 932: loss = 2.4120 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 933: loss = 2.3388 (0.647 sec/step)\n",
            "I0915 13:55:39.240504 139848901543808 learning.py:507] global step 933: loss = 2.3388 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 934: loss = 2.7685 (0.630 sec/step)\n",
            "I0915 13:55:39.872684 139848901543808 learning.py:507] global step 934: loss = 2.7685 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 935: loss = 2.0268 (0.633 sec/step)\n",
            "I0915 13:55:40.507640 139848901543808 learning.py:507] global step 935: loss = 2.0268 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 936: loss = 3.0301 (0.629 sec/step)\n",
            "I0915 13:55:41.138168 139848901543808 learning.py:507] global step 936: loss = 3.0301 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 937: loss = 3.4744 (0.634 sec/step)\n",
            "I0915 13:55:41.774513 139848901543808 learning.py:507] global step 937: loss = 3.4744 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 938: loss = 1.3611 (0.629 sec/step)\n",
            "I0915 13:55:42.405622 139848901543808 learning.py:507] global step 938: loss = 1.3611 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 939: loss = 2.1220 (0.647 sec/step)\n",
            "I0915 13:55:43.054267 139848901543808 learning.py:507] global step 939: loss = 2.1220 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 940: loss = 2.7422 (0.633 sec/step)\n",
            "I0915 13:55:43.688848 139848901543808 learning.py:507] global step 940: loss = 2.7422 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 941: loss = 2.9166 (0.639 sec/step)\n",
            "I0915 13:55:44.329202 139848901543808 learning.py:507] global step 941: loss = 2.9166 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 942: loss = 3.4177 (0.654 sec/step)\n",
            "I0915 13:55:44.985333 139848901543808 learning.py:507] global step 942: loss = 3.4177 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 943: loss = 1.9094 (0.642 sec/step)\n",
            "I0915 13:55:45.628991 139848901543808 learning.py:507] global step 943: loss = 1.9094 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 944: loss = 1.8543 (0.654 sec/step)\n",
            "I0915 13:55:46.285239 139848901543808 learning.py:507] global step 944: loss = 1.8543 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 945: loss = 2.5053 (0.659 sec/step)\n",
            "I0915 13:55:46.947137 139848901543808 learning.py:507] global step 945: loss = 2.5053 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 946: loss = 2.2589 (0.622 sec/step)\n",
            "I0915 13:55:47.571239 139848901543808 learning.py:507] global step 946: loss = 2.2589 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 947: loss = 2.5932 (0.656 sec/step)\n",
            "I0915 13:55:48.229765 139848901543808 learning.py:507] global step 947: loss = 2.5932 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 948: loss = 3.0412 (0.638 sec/step)\n",
            "I0915 13:55:48.870481 139848901543808 learning.py:507] global step 948: loss = 3.0412 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 949: loss = 2.2617 (0.619 sec/step)\n",
            "I0915 13:55:49.491122 139848901543808 learning.py:507] global step 949: loss = 2.2617 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 950: loss = 2.3858 (0.625 sec/step)\n",
            "I0915 13:55:50.118453 139848901543808 learning.py:507] global step 950: loss = 2.3858 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 951: loss = 2.2531 (0.622 sec/step)\n",
            "I0915 13:55:50.742323 139848901543808 learning.py:507] global step 951: loss = 2.2531 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 952: loss = 2.7708 (0.626 sec/step)\n",
            "I0915 13:55:51.369901 139848901543808 learning.py:507] global step 952: loss = 2.7708 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 953: loss = 2.9694 (0.642 sec/step)\n",
            "I0915 13:55:52.013689 139848901543808 learning.py:507] global step 953: loss = 2.9694 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 954: loss = 2.0279 (0.621 sec/step)\n",
            "I0915 13:55:52.636620 139848901543808 learning.py:507] global step 954: loss = 2.0279 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 955: loss = 3.7524 (0.648 sec/step)\n",
            "I0915 13:55:53.286874 139848901543808 learning.py:507] global step 955: loss = 3.7524 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 956: loss = 1.9407 (0.628 sec/step)\n",
            "I0915 13:55:53.916527 139848901543808 learning.py:507] global step 956: loss = 1.9407 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 957: loss = 2.5116 (0.634 sec/step)\n",
            "I0915 13:55:54.552957 139848901543808 learning.py:507] global step 957: loss = 2.5116 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 958: loss = 2.9888 (0.666 sec/step)\n",
            "I0915 13:55:55.221079 139848901543808 learning.py:507] global step 958: loss = 2.9888 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 959: loss = 2.6437 (0.626 sec/step)\n",
            "I0915 13:55:55.849130 139848901543808 learning.py:507] global step 959: loss = 2.6437 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 960: loss = 2.8365 (0.631 sec/step)\n",
            "I0915 13:55:56.481606 139848901543808 learning.py:507] global step 960: loss = 2.8365 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 961: loss = 3.6847 (0.653 sec/step)\n",
            "I0915 13:55:57.136593 139848901543808 learning.py:507] global step 961: loss = 3.6847 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 962: loss = 2.3171 (0.617 sec/step)\n",
            "I0915 13:55:57.755853 139848901543808 learning.py:507] global step 962: loss = 2.3171 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 963: loss = 2.3514 (0.636 sec/step)\n",
            "I0915 13:55:58.394713 139848901543808 learning.py:507] global step 963: loss = 2.3514 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 964: loss = 2.3582 (0.626 sec/step)\n",
            "I0915 13:55:59.022430 139848901543808 learning.py:507] global step 964: loss = 2.3582 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 965: loss = 2.4623 (0.657 sec/step)\n",
            "I0915 13:55:59.681124 139848901543808 learning.py:507] global step 965: loss = 2.4623 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 966: loss = 2.5595 (0.632 sec/step)\n",
            "I0915 13:56:00.315623 139848901543808 learning.py:507] global step 966: loss = 2.5595 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 967: loss = 2.1236 (0.633 sec/step)\n",
            "I0915 13:56:00.951026 139848901543808 learning.py:507] global step 967: loss = 2.1236 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 968: loss = 2.8042 (0.645 sec/step)\n",
            "I0915 13:56:01.597988 139848901543808 learning.py:507] global step 968: loss = 2.8042 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 969: loss = 2.4628 (0.646 sec/step)\n",
            "I0915 13:56:02.245300 139848901543808 learning.py:507] global step 969: loss = 2.4628 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 970: loss = 2.8706 (0.636 sec/step)\n",
            "I0915 13:56:02.883060 139848901543808 learning.py:507] global step 970: loss = 2.8706 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 971: loss = 2.6294 (0.625 sec/step)\n",
            "I0915 13:56:03.509855 139848901543808 learning.py:507] global step 971: loss = 2.6294 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 972: loss = 2.7956 (0.617 sec/step)\n",
            "I0915 13:56:04.128449 139848901543808 learning.py:507] global step 972: loss = 2.7956 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 973: loss = 1.9647 (0.639 sec/step)\n",
            "I0915 13:56:04.769721 139848901543808 learning.py:507] global step 973: loss = 1.9647 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 974: loss = 1.5854 (0.631 sec/step)\n",
            "I0915 13:56:05.402951 139848901543808 learning.py:507] global step 974: loss = 1.5854 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 975: loss = 2.4262 (0.659 sec/step)\n",
            "I0915 13:56:06.064578 139848901543808 learning.py:507] global step 975: loss = 2.4262 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 976: loss = 3.5065 (0.621 sec/step)\n",
            "I0915 13:56:06.688507 139848901543808 learning.py:507] global step 976: loss = 3.5065 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 977: loss = 1.9560 (0.646 sec/step)\n",
            "I0915 13:56:07.336731 139848901543808 learning.py:507] global step 977: loss = 1.9560 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 978: loss = 1.5036 (0.640 sec/step)\n",
            "I0915 13:56:07.978792 139848901543808 learning.py:507] global step 978: loss = 1.5036 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 979: loss = 2.3641 (0.643 sec/step)\n",
            "I0915 13:56:08.624116 139848901543808 learning.py:507] global step 979: loss = 2.3641 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 980: loss = 2.8452 (0.629 sec/step)\n",
            "I0915 13:56:09.255546 139848901543808 learning.py:507] global step 980: loss = 2.8452 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 981: loss = 2.3852 (0.639 sec/step)\n",
            "I0915 13:56:09.896414 139848901543808 learning.py:507] global step 981: loss = 2.3852 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 982: loss = 2.2133 (0.651 sec/step)\n",
            "I0915 13:56:10.549609 139848901543808 learning.py:507] global step 982: loss = 2.2133 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 983: loss = 2.3309 (0.667 sec/step)\n",
            "I0915 13:56:11.218546 139848901543808 learning.py:507] global step 983: loss = 2.3309 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 984: loss = 1.7520 (0.662 sec/step)\n",
            "I0915 13:56:11.882400 139848901543808 learning.py:507] global step 984: loss = 1.7520 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 985: loss = 2.5039 (0.630 sec/step)\n",
            "I0915 13:56:12.514733 139848901543808 learning.py:507] global step 985: loss = 2.5039 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 986: loss = 2.2118 (0.637 sec/step)\n",
            "I0915 13:56:13.153956 139848901543808 learning.py:507] global step 986: loss = 2.2118 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 987: loss = 2.1511 (0.633 sec/step)\n",
            "I0915 13:56:13.789460 139848901543808 learning.py:507] global step 987: loss = 2.1511 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 988: loss = 2.0913 (0.633 sec/step)\n",
            "I0915 13:56:14.424471 139848901543808 learning.py:507] global step 988: loss = 2.0913 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 989: loss = 2.6413 (0.645 sec/step)\n",
            "I0915 13:56:15.071997 139848901543808 learning.py:507] global step 989: loss = 2.6413 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 990: loss = 1.8643 (0.640 sec/step)\n",
            "I0915 13:56:15.713714 139848901543808 learning.py:507] global step 990: loss = 1.8643 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 991: loss = 2.3086 (0.624 sec/step)\n",
            "I0915 13:56:16.340020 139848901543808 learning.py:507] global step 991: loss = 2.3086 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 992: loss = 2.5650 (0.606 sec/step)\n",
            "I0915 13:56:16.947371 139848901543808 learning.py:507] global step 992: loss = 2.5650 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 993: loss = 2.8938 (0.619 sec/step)\n",
            "I0915 13:56:17.568678 139848901543808 learning.py:507] global step 993: loss = 2.8938 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 994: loss = 2.2089 (0.607 sec/step)\n",
            "I0915 13:56:18.177304 139848901543808 learning.py:507] global step 994: loss = 2.2089 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 995: loss = 2.8172 (0.624 sec/step)\n",
            "I0915 13:56:18.803301 139848901543808 learning.py:507] global step 995: loss = 2.8172 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 996: loss = 3.5777 (0.621 sec/step)\n",
            "I0915 13:56:19.425933 139848901543808 learning.py:507] global step 996: loss = 3.5777 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 997: loss = 3.0921 (0.635 sec/step)\n",
            "I0915 13:56:20.063338 139848901543808 learning.py:507] global step 997: loss = 3.0921 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 998: loss = 2.0443 (0.620 sec/step)\n",
            "I0915 13:56:20.687306 139848901543808 learning.py:507] global step 998: loss = 2.0443 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 999: loss = 2.1178 (0.640 sec/step)\n",
            "I0915 13:56:21.328902 139848901543808 learning.py:507] global step 999: loss = 2.1178 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 2.2132 (0.653 sec/step)\n",
            "I0915 13:56:21.983899 139848901543808 learning.py:507] global step 1000: loss = 2.2132 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1001: loss = 3.0082 (0.651 sec/step)\n",
            "I0915 13:56:22.637688 139848901543808 learning.py:507] global step 1001: loss = 3.0082 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1002: loss = 2.8988 (0.623 sec/step)\n",
            "I0915 13:56:23.262643 139848901543808 learning.py:507] global step 1002: loss = 2.8988 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1003: loss = 2.0690 (0.629 sec/step)\n",
            "I0915 13:56:23.894255 139848901543808 learning.py:507] global step 1003: loss = 2.0690 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1004: loss = 2.3996 (0.628 sec/step)\n",
            "I0915 13:56:24.523999 139848901543808 learning.py:507] global step 1004: loss = 2.3996 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1005: loss = 2.4303 (0.625 sec/step)\n",
            "I0915 13:56:25.150527 139848901543808 learning.py:507] global step 1005: loss = 2.4303 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1006: loss = 2.9661 (0.649 sec/step)\n",
            "I0915 13:56:25.801149 139848901543808 learning.py:507] global step 1006: loss = 2.9661 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1007: loss = 2.6514 (0.651 sec/step)\n",
            "I0915 13:56:26.454084 139848901543808 learning.py:507] global step 1007: loss = 2.6514 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1008: loss = 2.6627 (0.647 sec/step)\n",
            "I0915 13:56:27.103785 139848901543808 learning.py:507] global step 1008: loss = 2.6627 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1009: loss = 1.9905 (0.622 sec/step)\n",
            "I0915 13:56:27.727866 139848901543808 learning.py:507] global step 1009: loss = 1.9905 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1010: loss = 2.4086 (0.622 sec/step)\n",
            "I0915 13:56:28.351664 139848901543808 learning.py:507] global step 1010: loss = 2.4086 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1011: loss = 1.9344 (0.671 sec/step)\n",
            "I0915 13:56:29.024919 139848901543808 learning.py:507] global step 1011: loss = 1.9344 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 1012: loss = 2.1572 (0.639 sec/step)\n",
            "I0915 13:56:29.665234 139848901543808 learning.py:507] global step 1012: loss = 2.1572 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1013: loss = 1.8578 (0.646 sec/step)\n",
            "I0915 13:56:30.313370 139848901543808 learning.py:507] global step 1013: loss = 1.8578 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1014: loss = 2.5649 (0.628 sec/step)\n",
            "I0915 13:56:30.943507 139848901543808 learning.py:507] global step 1014: loss = 2.5649 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1015: loss = 3.4050 (0.637 sec/step)\n",
            "I0915 13:56:31.582037 139848901543808 learning.py:507] global step 1015: loss = 3.4050 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1016: loss = 2.6659 (0.639 sec/step)\n",
            "I0915 13:56:32.222647 139848901543808 learning.py:507] global step 1016: loss = 2.6659 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1017: loss = 1.8727 (0.648 sec/step)\n",
            "I0915 13:56:32.872773 139848901543808 learning.py:507] global step 1017: loss = 1.8727 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1018: loss = 1.8056 (0.659 sec/step)\n",
            "I0915 13:56:33.534132 139848901543808 learning.py:507] global step 1018: loss = 1.8056 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1019: loss = 1.9595 (0.634 sec/step)\n",
            "I0915 13:56:34.170573 139848901543808 learning.py:507] global step 1019: loss = 1.9595 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1020: loss = 2.1685 (0.627 sec/step)\n",
            "I0915 13:56:34.799988 139848901543808 learning.py:507] global step 1020: loss = 2.1685 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1021: loss = 2.6387 (0.633 sec/step)\n",
            "I0915 13:56:35.435221 139848901543808 learning.py:507] global step 1021: loss = 2.6387 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1022: loss = 2.6011 (0.665 sec/step)\n",
            "I0915 13:56:36.102809 139848901543808 learning.py:507] global step 1022: loss = 2.6011 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1023: loss = 1.9463 (0.663 sec/step)\n",
            "I0915 13:56:36.767714 139848901543808 learning.py:507] global step 1023: loss = 1.9463 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1024: loss = 2.2380 (0.643 sec/step)\n",
            "I0915 13:56:37.413048 139848901543808 learning.py:507] global step 1024: loss = 2.2380 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1025: loss = 2.3853 (0.629 sec/step)\n",
            "I0915 13:56:38.044317 139848901543808 learning.py:507] global step 1025: loss = 2.3853 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1026: loss = 1.8830 (0.636 sec/step)\n",
            "I0915 13:56:38.682056 139848901543808 learning.py:507] global step 1026: loss = 1.8830 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1027: loss = 2.4430 (0.647 sec/step)\n",
            "I0915 13:56:39.331695 139848901543808 learning.py:507] global step 1027: loss = 2.4430 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1028: loss = 2.1800 (0.635 sec/step)\n",
            "I0915 13:56:39.968940 139848901543808 learning.py:507] global step 1028: loss = 2.1800 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1029: loss = 4.3952 (0.642 sec/step)\n",
            "I0915 13:56:40.613023 139848901543808 learning.py:507] global step 1029: loss = 4.3952 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1030: loss = 1.7694 (0.655 sec/step)\n",
            "I0915 13:56:41.270689 139848901543808 learning.py:507] global step 1030: loss = 1.7694 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1031: loss = 2.5905 (0.660 sec/step)\n",
            "I0915 13:56:41.932822 139848901543808 learning.py:507] global step 1031: loss = 2.5905 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1032: loss = 2.6163 (0.639 sec/step)\n",
            "I0915 13:56:42.573674 139848901543808 learning.py:507] global step 1032: loss = 2.6163 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1033: loss = 2.7068 (0.631 sec/step)\n",
            "I0915 13:56:43.206263 139848901543808 learning.py:507] global step 1033: loss = 2.7068 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1034: loss = 1.8766 (0.619 sec/step)\n",
            "I0915 13:56:43.828530 139848901543808 learning.py:507] global step 1034: loss = 1.8766 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1035: loss = 2.5027 (0.670 sec/step)\n",
            "I0915 13:56:44.500596 139848901543808 learning.py:507] global step 1035: loss = 2.5027 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1036: loss = 2.4173 (0.623 sec/step)\n",
            "I0915 13:56:45.125246 139848901543808 learning.py:507] global step 1036: loss = 2.4173 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1037: loss = 2.4158 (0.655 sec/step)\n",
            "I0915 13:56:45.781913 139848901543808 learning.py:507] global step 1037: loss = 2.4158 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1038: loss = 2.6623 (0.642 sec/step)\n",
            "I0915 13:56:46.426619 139848901543808 learning.py:507] global step 1038: loss = 2.6623 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1039: loss = 2.4679 (0.654 sec/step)\n",
            "I0915 13:56:47.082172 139848901543808 learning.py:507] global step 1039: loss = 2.4679 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1040: loss = 2.5690 (0.652 sec/step)\n",
            "I0915 13:56:47.736180 139848901543808 learning.py:507] global step 1040: loss = 2.5690 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1041: loss = 2.6656 (0.625 sec/step)\n",
            "I0915 13:56:48.362908 139848901543808 learning.py:507] global step 1041: loss = 2.6656 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1042: loss = 3.2430 (0.640 sec/step)\n",
            "I0915 13:56:49.004923 139848901543808 learning.py:507] global step 1042: loss = 3.2430 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1043: loss = 2.6635 (0.639 sec/step)\n",
            "I0915 13:56:49.646028 139848901543808 learning.py:507] global step 1043: loss = 2.6635 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1044: loss = 2.1249 (0.650 sec/step)\n",
            "I0915 13:56:50.298331 139848901543808 learning.py:507] global step 1044: loss = 2.1249 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1045: loss = 2.1775 (0.648 sec/step)\n",
            "I0915 13:56:50.948566 139848901543808 learning.py:507] global step 1045: loss = 2.1775 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1046: loss = 2.7545 (0.636 sec/step)\n",
            "I0915 13:56:51.586489 139848901543808 learning.py:507] global step 1046: loss = 2.7545 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1047: loss = 2.9594 (0.652 sec/step)\n",
            "I0915 13:56:52.240751 139848901543808 learning.py:507] global step 1047: loss = 2.9594 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1048: loss = 2.5118 (0.653 sec/step)\n",
            "I0915 13:56:52.895822 139848901543808 learning.py:507] global step 1048: loss = 2.5118 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1049: loss = 1.8214 (0.641 sec/step)\n",
            "I0915 13:56:53.538898 139848901543808 learning.py:507] global step 1049: loss = 1.8214 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1050: loss = 2.3439 (0.628 sec/step)\n",
            "I0915 13:56:54.169328 139848901543808 learning.py:507] global step 1050: loss = 2.3439 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1051: loss = 1.8483 (0.646 sec/step)\n",
            "I0915 13:56:54.818023 139848901543808 learning.py:507] global step 1051: loss = 1.8483 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1052: loss = 1.9385 (0.650 sec/step)\n",
            "I0915 13:56:55.470812 139848901543808 learning.py:507] global step 1052: loss = 1.9385 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1053: loss = 3.2734 (0.632 sec/step)\n",
            "I0915 13:56:56.104946 139848901543808 learning.py:507] global step 1053: loss = 3.2734 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1054: loss = 2.1198 (0.645 sec/step)\n",
            "I0915 13:56:56.752501 139848901543808 learning.py:507] global step 1054: loss = 2.1198 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1055: loss = 2.5365 (0.650 sec/step)\n",
            "I0915 13:56:57.404459 139848901543808 learning.py:507] global step 1055: loss = 2.5365 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1056: loss = 2.1946 (0.644 sec/step)\n",
            "I0915 13:56:58.050247 139848901543808 learning.py:507] global step 1056: loss = 2.1946 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1057: loss = 2.1119 (0.628 sec/step)\n",
            "I0915 13:56:58.679874 139848901543808 learning.py:507] global step 1057: loss = 2.1119 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1058: loss = 2.2687 (0.629 sec/step)\n",
            "I0915 13:56:59.310635 139848901543808 learning.py:507] global step 1058: loss = 2.2687 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1059: loss = 2.7540 (0.635 sec/step)\n",
            "I0915 13:56:59.948168 139848901543808 learning.py:507] global step 1059: loss = 2.7540 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1060: loss = 2.6697 (0.628 sec/step)\n",
            "I0915 13:57:00.578226 139848901543808 learning.py:507] global step 1060: loss = 2.6697 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1061: loss = 2.6124 (0.647 sec/step)\n",
            "I0915 13:57:01.227935 139848901543808 learning.py:507] global step 1061: loss = 2.6124 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1062: loss = 2.3600 (0.648 sec/step)\n",
            "I0915 13:57:01.877526 139848901543808 learning.py:507] global step 1062: loss = 2.3600 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1063: loss = 2.8419 (0.613 sec/step)\n",
            "I0915 13:57:02.492559 139848901543808 learning.py:507] global step 1063: loss = 2.8419 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 1064: loss = 2.4123 (0.658 sec/step)\n",
            "I0915 13:57:03.152566 139848901543808 learning.py:507] global step 1064: loss = 2.4123 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1065: loss = 2.6041 (0.640 sec/step)\n",
            "I0915 13:57:03.795410 139848901543808 learning.py:507] global step 1065: loss = 2.6041 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1066: loss = 3.3702 (0.629 sec/step)\n",
            "I0915 13:57:04.426821 139848901543808 learning.py:507] global step 1066: loss = 3.3702 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1067: loss = 2.4149 (0.658 sec/step)\n",
            "I0915 13:57:05.086523 139848901543808 learning.py:507] global step 1067: loss = 2.4149 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1068: loss = 1.9655 (0.626 sec/step)\n",
            "I0915 13:57:05.714962 139848901543808 learning.py:507] global step 1068: loss = 1.9655 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1069: loss = 2.1592 (0.647 sec/step)\n",
            "I0915 13:57:06.364496 139848901543808 learning.py:507] global step 1069: loss = 2.1592 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1070: loss = 1.7759 (0.656 sec/step)\n",
            "I0915 13:57:07.023519 139848901543808 learning.py:507] global step 1070: loss = 1.7759 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1071: loss = 3.1745 (0.648 sec/step)\n",
            "I0915 13:57:07.673519 139848901543808 learning.py:507] global step 1071: loss = 3.1745 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1072: loss = 2.9015 (0.646 sec/step)\n",
            "I0915 13:57:08.321529 139848901543808 learning.py:507] global step 1072: loss = 2.9015 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1073: loss = 2.3244 (0.645 sec/step)\n",
            "I0915 13:57:08.968539 139848901543808 learning.py:507] global step 1073: loss = 2.3244 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1074: loss = 3.0625 (0.656 sec/step)\n",
            "I0915 13:57:09.626947 139848901543808 learning.py:507] global step 1074: loss = 3.0625 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1075: loss = 1.6981 (0.648 sec/step)\n",
            "I0915 13:57:10.277084 139848901543808 learning.py:507] global step 1075: loss = 1.6981 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1076: loss = 2.1039 (0.644 sec/step)\n",
            "I0915 13:57:10.924868 139848901543808 learning.py:507] global step 1076: loss = 2.1039 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1077: loss = 2.4007 (0.657 sec/step)\n",
            "I0915 13:57:11.584501 139848901543808 learning.py:507] global step 1077: loss = 2.4007 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1078: loss = 2.2234 (0.641 sec/step)\n",
            "I0915 13:57:12.227462 139848901543808 learning.py:507] global step 1078: loss = 2.2234 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1079: loss = 3.2208 (0.631 sec/step)\n",
            "I0915 13:57:12.860719 139848901543808 learning.py:507] global step 1079: loss = 3.2208 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1080: loss = 2.2596 (0.648 sec/step)\n",
            "I0915 13:57:13.510841 139848901543808 learning.py:507] global step 1080: loss = 2.2596 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1081: loss = 3.0961 (0.628 sec/step)\n",
            "I0915 13:57:14.141227 139848901543808 learning.py:507] global step 1081: loss = 3.0961 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1082: loss = 2.2699 (0.636 sec/step)\n",
            "I0915 13:57:14.778992 139848901543808 learning.py:507] global step 1082: loss = 2.2699 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1083: loss = 2.7319 (0.632 sec/step)\n",
            "I0915 13:57:15.413443 139848901543808 learning.py:507] global step 1083: loss = 2.7319 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1084: loss = 2.9497 (0.630 sec/step)\n",
            "I0915 13:57:16.045649 139848901543808 learning.py:507] global step 1084: loss = 2.9497 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1085: loss = 2.4429 (0.638 sec/step)\n",
            "I0915 13:57:16.685954 139848901543808 learning.py:507] global step 1085: loss = 2.4429 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1086: loss = 2.4743 (0.665 sec/step)\n",
            "I0915 13:57:17.352722 139848901543808 learning.py:507] global step 1086: loss = 2.4743 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1087: loss = 3.2064 (0.675 sec/step)\n",
            "I0915 13:57:18.029807 139848901543808 learning.py:507] global step 1087: loss = 3.2064 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1088: loss = 2.2835 (0.635 sec/step)\n",
            "I0915 13:57:18.667191 139848901543808 learning.py:507] global step 1088: loss = 2.2835 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1089: loss = 1.8970 (0.629 sec/step)\n",
            "I0915 13:57:19.298814 139848901543808 learning.py:507] global step 1089: loss = 1.8970 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1090: loss = 2.6630 (0.622 sec/step)\n",
            "I0915 13:57:19.923774 139848901543808 learning.py:507] global step 1090: loss = 2.6630 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1091: loss = 2.1019 (0.633 sec/step)\n",
            "I0915 13:57:20.558666 139848901543808 learning.py:507] global step 1091: loss = 2.1019 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1092: loss = 1.7562 (0.637 sec/step)\n",
            "I0915 13:57:21.198003 139848901543808 learning.py:507] global step 1092: loss = 1.7562 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1093: loss = 2.2713 (0.644 sec/step)\n",
            "I0915 13:57:21.844002 139848901543808 learning.py:507] global step 1093: loss = 2.2713 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1094: loss = 2.9717 (0.650 sec/step)\n",
            "I0915 13:57:22.495488 139848901543808 learning.py:507] global step 1094: loss = 2.9717 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1095: loss = 1.8938 (0.768 sec/step)\n",
            "I0915 13:57:23.319255 139848901543808 learning.py:507] global step 1095: loss = 1.8938 (0.768 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1095.\n",
            "I0915 13:57:23.881933 139845834331904 supervisor.py:1050] Recording summary at step 1095.\n",
            "INFO:tensorflow:global step 1096: loss = 1.9946 (0.871 sec/step)\n",
            "I0915 13:57:24.193552 139848901543808 learning.py:507] global step 1096: loss = 1.9946 (0.871 sec/step)\n",
            "INFO:tensorflow:global step 1097: loss = 2.6023 (0.648 sec/step)\n",
            "I0915 13:57:24.843456 139848901543808 learning.py:507] global step 1097: loss = 2.6023 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1098: loss = 2.9377 (0.639 sec/step)\n",
            "I0915 13:57:25.484796 139848901543808 learning.py:507] global step 1098: loss = 2.9377 (0.639 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.54148\n",
            "I0915 13:57:25.908152 139845842724608 supervisor.py:1099] global_step/sec: 1.54148\n",
            "INFO:tensorflow:global step 1099: loss = 2.4798 (0.651 sec/step)\n",
            "I0915 13:57:26.137390 139848901543808 learning.py:507] global step 1099: loss = 2.4798 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1100: loss = 2.2979 (0.628 sec/step)\n",
            "I0915 13:57:26.767591 139848901543808 learning.py:507] global step 1100: loss = 2.2979 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1101: loss = 2.5720 (0.631 sec/step)\n",
            "I0915 13:57:27.400455 139848901543808 learning.py:507] global step 1101: loss = 2.5720 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1102: loss = 2.0309 (0.630 sec/step)\n",
            "I0915 13:57:28.032442 139848901543808 learning.py:507] global step 1102: loss = 2.0309 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1103: loss = 2.6643 (0.647 sec/step)\n",
            "I0915 13:57:28.681265 139848901543808 learning.py:507] global step 1103: loss = 2.6643 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1104: loss = 1.8138 (0.637 sec/step)\n",
            "I0915 13:57:29.319715 139848901543808 learning.py:507] global step 1104: loss = 1.8138 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1105: loss = 3.5394 (0.644 sec/step)\n",
            "I0915 13:57:29.966238 139848901543808 learning.py:507] global step 1105: loss = 3.5394 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1106: loss = 2.7484 (0.630 sec/step)\n",
            "I0915 13:57:30.598218 139848901543808 learning.py:507] global step 1106: loss = 2.7484 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1107: loss = 1.8489 (0.624 sec/step)\n",
            "I0915 13:57:31.224267 139848901543808 learning.py:507] global step 1107: loss = 1.8489 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1108: loss = 3.5960 (0.635 sec/step)\n",
            "I0915 13:57:31.861206 139848901543808 learning.py:507] global step 1108: loss = 3.5960 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1109: loss = 2.4064 (0.624 sec/step)\n",
            "I0915 13:57:32.487249 139848901543808 learning.py:507] global step 1109: loss = 2.4064 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1110: loss = 1.7872 (0.620 sec/step)\n",
            "I0915 13:57:33.109647 139848901543808 learning.py:507] global step 1110: loss = 1.7872 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1111: loss = 1.8391 (0.649 sec/step)\n",
            "I0915 13:57:33.760482 139848901543808 learning.py:507] global step 1111: loss = 1.8391 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1112: loss = 1.3487 (0.641 sec/step)\n",
            "I0915 13:57:34.403525 139848901543808 learning.py:507] global step 1112: loss = 1.3487 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1113: loss = 2.3424 (0.664 sec/step)\n",
            "I0915 13:57:35.069663 139848901543808 learning.py:507] global step 1113: loss = 2.3424 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1114: loss = 2.0298 (0.635 sec/step)\n",
            "I0915 13:57:35.706609 139848901543808 learning.py:507] global step 1114: loss = 2.0298 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1115: loss = 2.6719 (0.633 sec/step)\n",
            "I0915 13:57:36.341903 139848901543808 learning.py:507] global step 1115: loss = 2.6719 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1116: loss = 2.4469 (0.636 sec/step)\n",
            "I0915 13:57:36.979898 139848901543808 learning.py:507] global step 1116: loss = 2.4469 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1117: loss = 2.4642 (0.641 sec/step)\n",
            "I0915 13:57:37.622421 139848901543808 learning.py:507] global step 1117: loss = 2.4642 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1118: loss = 2.8234 (0.632 sec/step)\n",
            "I0915 13:57:38.256533 139848901543808 learning.py:507] global step 1118: loss = 2.8234 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1119: loss = 2.0607 (0.666 sec/step)\n",
            "I0915 13:57:38.924494 139848901543808 learning.py:507] global step 1119: loss = 2.0607 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1120: loss = 2.7037 (0.656 sec/step)\n",
            "I0915 13:57:39.582495 139848901543808 learning.py:507] global step 1120: loss = 2.7037 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1121: loss = 2.2600 (0.650 sec/step)\n",
            "I0915 13:57:40.234769 139848901543808 learning.py:507] global step 1121: loss = 2.2600 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1122: loss = 2.8339 (0.641 sec/step)\n",
            "I0915 13:57:40.878256 139848901543808 learning.py:507] global step 1122: loss = 2.8339 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1123: loss = 2.4809 (0.637 sec/step)\n",
            "I0915 13:57:41.517410 139848901543808 learning.py:507] global step 1123: loss = 2.4809 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1124: loss = 2.0496 (0.643 sec/step)\n",
            "I0915 13:57:42.162563 139848901543808 learning.py:507] global step 1124: loss = 2.0496 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1125: loss = 2.2785 (0.637 sec/step)\n",
            "I0915 13:57:42.802113 139848901543808 learning.py:507] global step 1125: loss = 2.2785 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1126: loss = 1.7526 (0.637 sec/step)\n",
            "I0915 13:57:43.441168 139848901543808 learning.py:507] global step 1126: loss = 1.7526 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1127: loss = 2.2083 (0.646 sec/step)\n",
            "I0915 13:57:44.089937 139848901543808 learning.py:507] global step 1127: loss = 2.2083 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1128: loss = 2.1085 (0.650 sec/step)\n",
            "I0915 13:57:44.742228 139848901543808 learning.py:507] global step 1128: loss = 2.1085 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1129: loss = 2.1785 (0.636 sec/step)\n",
            "I0915 13:57:45.380863 139848901543808 learning.py:507] global step 1129: loss = 2.1785 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1130: loss = 2.5247 (0.634 sec/step)\n",
            "I0915 13:57:46.016811 139848901543808 learning.py:507] global step 1130: loss = 2.5247 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1131: loss = 2.4989 (0.661 sec/step)\n",
            "I0915 13:57:46.679383 139848901543808 learning.py:507] global step 1131: loss = 2.4989 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1132: loss = 2.1298 (0.658 sec/step)\n",
            "I0915 13:57:47.339301 139848901543808 learning.py:507] global step 1132: loss = 2.1298 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1133: loss = 1.6715 (0.655 sec/step)\n",
            "I0915 13:57:47.996209 139848901543808 learning.py:507] global step 1133: loss = 1.6715 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1134: loss = 3.2762 (0.621 sec/step)\n",
            "I0915 13:57:48.619490 139848901543808 learning.py:507] global step 1134: loss = 3.2762 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1135: loss = 2.5544 (0.674 sec/step)\n",
            "I0915 13:57:49.295600 139848901543808 learning.py:507] global step 1135: loss = 2.5544 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 1136: loss = 2.3095 (0.652 sec/step)\n",
            "I0915 13:57:49.949713 139848901543808 learning.py:507] global step 1136: loss = 2.3095 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1137: loss = 2.0837 (0.656 sec/step)\n",
            "I0915 13:57:50.607239 139848901543808 learning.py:507] global step 1137: loss = 2.0837 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1138: loss = 2.3803 (0.657 sec/step)\n",
            "I0915 13:57:51.266222 139848901543808 learning.py:507] global step 1138: loss = 2.3803 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1139: loss = 3.0093 (0.634 sec/step)\n",
            "I0915 13:57:51.902288 139848901543808 learning.py:507] global step 1139: loss = 3.0093 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1140: loss = 2.5311 (0.645 sec/step)\n",
            "I0915 13:57:52.549664 139848901543808 learning.py:507] global step 1140: loss = 2.5311 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1141: loss = 2.0850 (0.620 sec/step)\n",
            "I0915 13:57:53.172113 139848901543808 learning.py:507] global step 1141: loss = 2.0850 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1142: loss = 1.5907 (0.634 sec/step)\n",
            "I0915 13:57:53.808455 139848901543808 learning.py:507] global step 1142: loss = 1.5907 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1143: loss = 2.0471 (0.658 sec/step)\n",
            "I0915 13:57:54.468322 139848901543808 learning.py:507] global step 1143: loss = 2.0471 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1144: loss = 3.1574 (0.638 sec/step)\n",
            "I0915 13:57:55.108149 139848901543808 learning.py:507] global step 1144: loss = 3.1574 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1145: loss = 2.3329 (0.634 sec/step)\n",
            "I0915 13:57:55.744404 139848901543808 learning.py:507] global step 1145: loss = 2.3329 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1146: loss = 2.4167 (0.634 sec/step)\n",
            "I0915 13:57:56.380272 139848901543808 learning.py:507] global step 1146: loss = 2.4167 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1147: loss = 1.3571 (0.639 sec/step)\n",
            "I0915 13:57:57.021492 139848901543808 learning.py:507] global step 1147: loss = 1.3571 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1148: loss = 2.1622 (0.619 sec/step)\n",
            "I0915 13:57:57.642483 139848901543808 learning.py:507] global step 1148: loss = 2.1622 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1149: loss = 2.5634 (0.647 sec/step)\n",
            "I0915 13:57:58.291313 139848901543808 learning.py:507] global step 1149: loss = 2.5634 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1150: loss = 2.8339 (0.639 sec/step)\n",
            "I0915 13:57:58.932526 139848901543808 learning.py:507] global step 1150: loss = 2.8339 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1151: loss = 1.7164 (0.670 sec/step)\n",
            "I0915 13:57:59.604304 139848901543808 learning.py:507] global step 1151: loss = 1.7164 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1152: loss = 2.3521 (0.642 sec/step)\n",
            "I0915 13:58:00.249072 139848901543808 learning.py:507] global step 1152: loss = 2.3521 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1153: loss = 1.9904 (0.633 sec/step)\n",
            "I0915 13:58:00.884739 139848901543808 learning.py:507] global step 1153: loss = 1.9904 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1154: loss = 1.3882 (0.628 sec/step)\n",
            "I0915 13:58:01.514997 139848901543808 learning.py:507] global step 1154: loss = 1.3882 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1155: loss = 3.3298 (0.640 sec/step)\n",
            "I0915 13:58:02.157116 139848901543808 learning.py:507] global step 1155: loss = 3.3298 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1156: loss = 2.2414 (0.656 sec/step)\n",
            "I0915 13:58:02.815338 139848901543808 learning.py:507] global step 1156: loss = 2.2414 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1157: loss = 2.0065 (0.653 sec/step)\n",
            "I0915 13:58:03.470727 139848901543808 learning.py:507] global step 1157: loss = 2.0065 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1158: loss = 2.6200 (0.641 sec/step)\n",
            "I0915 13:58:04.113798 139848901543808 learning.py:507] global step 1158: loss = 2.6200 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1159: loss = 3.6053 (0.642 sec/step)\n",
            "I0915 13:58:04.757499 139848901543808 learning.py:507] global step 1159: loss = 3.6053 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1160: loss = 2.0171 (0.623 sec/step)\n",
            "I0915 13:58:05.382853 139848901543808 learning.py:507] global step 1160: loss = 2.0171 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1161: loss = 2.8787 (0.627 sec/step)\n",
            "I0915 13:58:06.011967 139848901543808 learning.py:507] global step 1161: loss = 2.8787 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1162: loss = 1.8544 (0.643 sec/step)\n",
            "I0915 13:58:06.657200 139848901543808 learning.py:507] global step 1162: loss = 1.8544 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1163: loss = 1.6458 (0.656 sec/step)\n",
            "I0915 13:58:07.314931 139848901543808 learning.py:507] global step 1163: loss = 1.6458 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1164: loss = 2.9191 (0.632 sec/step)\n",
            "I0915 13:58:07.949259 139848901543808 learning.py:507] global step 1164: loss = 2.9191 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1165: loss = 1.9071 (0.640 sec/step)\n",
            "I0915 13:58:08.591412 139848901543808 learning.py:507] global step 1165: loss = 1.9071 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1166: loss = 2.8552 (0.629 sec/step)\n",
            "I0915 13:58:09.222333 139848901543808 learning.py:507] global step 1166: loss = 2.8552 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1167: loss = 2.4959 (0.666 sec/step)\n",
            "I0915 13:58:09.890128 139848901543808 learning.py:507] global step 1167: loss = 2.4959 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1168: loss = 2.5613 (0.639 sec/step)\n",
            "I0915 13:58:10.531505 139848901543808 learning.py:507] global step 1168: loss = 2.5613 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1169: loss = 2.2405 (0.651 sec/step)\n",
            "I0915 13:58:11.184716 139848901543808 learning.py:507] global step 1169: loss = 2.2405 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1170: loss = 1.9194 (0.652 sec/step)\n",
            "I0915 13:58:11.838411 139848901543808 learning.py:507] global step 1170: loss = 1.9194 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1171: loss = 2.2661 (0.651 sec/step)\n",
            "I0915 13:58:12.491861 139848901543808 learning.py:507] global step 1171: loss = 2.2661 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1172: loss = 3.5567 (0.649 sec/step)\n",
            "I0915 13:58:13.143026 139848901543808 learning.py:507] global step 1172: loss = 3.5567 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1173: loss = 2.0263 (0.640 sec/step)\n",
            "I0915 13:58:13.784827 139848901543808 learning.py:507] global step 1173: loss = 2.0263 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1174: loss = 2.2890 (0.639 sec/step)\n",
            "I0915 13:58:14.426287 139848901543808 learning.py:507] global step 1174: loss = 2.2890 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1175: loss = 1.9100 (0.630 sec/step)\n",
            "I0915 13:58:15.058594 139848901543808 learning.py:507] global step 1175: loss = 1.9100 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1176: loss = 2.3977 (0.645 sec/step)\n",
            "I0915 13:58:15.705280 139848901543808 learning.py:507] global step 1176: loss = 2.3977 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1177: loss = 2.0318 (0.649 sec/step)\n",
            "I0915 13:58:16.356571 139848901543808 learning.py:507] global step 1177: loss = 2.0318 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1178: loss = 2.2218 (0.623 sec/step)\n",
            "I0915 13:58:16.981631 139848901543808 learning.py:507] global step 1178: loss = 2.2218 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1179: loss = 2.3673 (0.618 sec/step)\n",
            "I0915 13:58:17.602072 139848901543808 learning.py:507] global step 1179: loss = 2.3673 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1180: loss = 1.7793 (0.669 sec/step)\n",
            "I0915 13:58:18.274382 139848901543808 learning.py:507] global step 1180: loss = 1.7793 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1181: loss = 1.7175 (0.638 sec/step)\n",
            "I0915 13:58:18.914794 139848901543808 learning.py:507] global step 1181: loss = 1.7175 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1182: loss = 2.6270 (0.644 sec/step)\n",
            "I0915 13:58:19.560747 139848901543808 learning.py:507] global step 1182: loss = 2.6270 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1183: loss = 1.7239 (0.654 sec/step)\n",
            "I0915 13:58:20.216917 139848901543808 learning.py:507] global step 1183: loss = 1.7239 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1184: loss = 1.7447 (0.630 sec/step)\n",
            "I0915 13:58:20.848662 139848901543808 learning.py:507] global step 1184: loss = 1.7447 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1185: loss = 3.0744 (0.644 sec/step)\n",
            "I0915 13:58:21.494569 139848901543808 learning.py:507] global step 1185: loss = 3.0744 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1186: loss = 2.0378 (0.644 sec/step)\n",
            "I0915 13:58:22.141240 139848901543808 learning.py:507] global step 1186: loss = 2.0378 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1187: loss = 2.2891 (0.642 sec/step)\n",
            "I0915 13:58:22.784824 139848901543808 learning.py:507] global step 1187: loss = 2.2891 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1188: loss = 1.6746 (0.643 sec/step)\n",
            "I0915 13:58:23.430212 139848901543808 learning.py:507] global step 1188: loss = 1.6746 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1189: loss = 2.0379 (0.635 sec/step)\n",
            "I0915 13:58:24.067316 139848901543808 learning.py:507] global step 1189: loss = 2.0379 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1190: loss = 2.8225 (0.637 sec/step)\n",
            "I0915 13:58:24.706433 139848901543808 learning.py:507] global step 1190: loss = 2.8225 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1191: loss = 2.2902 (0.651 sec/step)\n",
            "I0915 13:58:25.359657 139848901543808 learning.py:507] global step 1191: loss = 2.2902 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1192: loss = 1.6572 (0.620 sec/step)\n",
            "I0915 13:58:25.981871 139848901543808 learning.py:507] global step 1192: loss = 1.6572 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1193: loss = 2.6921 (0.654 sec/step)\n",
            "I0915 13:58:26.637644 139848901543808 learning.py:507] global step 1193: loss = 2.6921 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1194: loss = 3.2281 (0.646 sec/step)\n",
            "I0915 13:58:27.285189 139848901543808 learning.py:507] global step 1194: loss = 3.2281 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1195: loss = 2.7460 (0.623 sec/step)\n",
            "I0915 13:58:27.911113 139848901543808 learning.py:507] global step 1195: loss = 2.7460 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1196: loss = 1.3948 (0.651 sec/step)\n",
            "I0915 13:58:28.564795 139848901543808 learning.py:507] global step 1196: loss = 1.3948 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1197: loss = 2.1090 (0.634 sec/step)\n",
            "I0915 13:58:29.200917 139848901543808 learning.py:507] global step 1197: loss = 2.1090 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1198: loss = 3.2337 (0.643 sec/step)\n",
            "I0915 13:58:29.845782 139848901543808 learning.py:507] global step 1198: loss = 3.2337 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1199: loss = 2.4112 (0.662 sec/step)\n",
            "I0915 13:58:30.510063 139848901543808 learning.py:507] global step 1199: loss = 2.4112 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1200: loss = 2.7808 (0.636 sec/step)\n",
            "I0915 13:58:31.148444 139848901543808 learning.py:507] global step 1200: loss = 2.7808 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1201: loss = 2.6228 (0.624 sec/step)\n",
            "I0915 13:58:31.774729 139848901543808 learning.py:507] global step 1201: loss = 2.6228 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1202: loss = 2.4938 (0.649 sec/step)\n",
            "I0915 13:58:32.426059 139848901543808 learning.py:507] global step 1202: loss = 2.4938 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1203: loss = 2.7353 (0.634 sec/step)\n",
            "I0915 13:58:33.061512 139848901543808 learning.py:507] global step 1203: loss = 2.7353 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1204: loss = 1.4481 (0.639 sec/step)\n",
            "I0915 13:58:33.702641 139848901543808 learning.py:507] global step 1204: loss = 1.4481 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1205: loss = 2.7949 (0.632 sec/step)\n",
            "I0915 13:58:34.336795 139848901543808 learning.py:507] global step 1205: loss = 2.7949 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1206: loss = 2.1932 (0.633 sec/step)\n",
            "I0915 13:58:34.972460 139848901543808 learning.py:507] global step 1206: loss = 2.1932 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1207: loss = 2.0963 (0.639 sec/step)\n",
            "I0915 13:58:35.613137 139848901543808 learning.py:507] global step 1207: loss = 2.0963 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1208: loss = 2.8679 (0.632 sec/step)\n",
            "I0915 13:58:36.246708 139848901543808 learning.py:507] global step 1208: loss = 2.8679 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1209: loss = 2.0464 (0.649 sec/step)\n",
            "I0915 13:58:36.897754 139848901543808 learning.py:507] global step 1209: loss = 2.0464 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1210: loss = 2.2710 (0.616 sec/step)\n",
            "I0915 13:58:37.515523 139848901543808 learning.py:507] global step 1210: loss = 2.2710 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1211: loss = 2.5689 (0.643 sec/step)\n",
            "I0915 13:58:38.160944 139848901543808 learning.py:507] global step 1211: loss = 2.5689 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1212: loss = 1.7191 (0.644 sec/step)\n",
            "I0915 13:58:38.807584 139848901543808 learning.py:507] global step 1212: loss = 1.7191 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1213: loss = 3.9297 (0.651 sec/step)\n",
            "I0915 13:58:39.461185 139848901543808 learning.py:507] global step 1213: loss = 3.9297 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1214: loss = 3.7135 (0.641 sec/step)\n",
            "I0915 13:58:40.104415 139848901543808 learning.py:507] global step 1214: loss = 3.7135 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1215: loss = 3.5282 (0.648 sec/step)\n",
            "I0915 13:58:40.754621 139848901543808 learning.py:507] global step 1215: loss = 3.5282 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1216: loss = 1.6739 (0.635 sec/step)\n",
            "I0915 13:58:41.391074 139848901543808 learning.py:507] global step 1216: loss = 1.6739 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1217: loss = 2.2698 (0.648 sec/step)\n",
            "I0915 13:58:42.040656 139848901543808 learning.py:507] global step 1217: loss = 2.2698 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1218: loss = 2.3653 (0.654 sec/step)\n",
            "I0915 13:58:42.696502 139848901543808 learning.py:507] global step 1218: loss = 2.3653 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1219: loss = 2.0421 (0.626 sec/step)\n",
            "I0915 13:58:43.324670 139848901543808 learning.py:507] global step 1219: loss = 2.0421 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1220: loss = 1.8565 (0.625 sec/step)\n",
            "I0915 13:58:43.951042 139848901543808 learning.py:507] global step 1220: loss = 1.8565 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1221: loss = 3.7261 (0.624 sec/step)\n",
            "I0915 13:58:44.576466 139848901543808 learning.py:507] global step 1221: loss = 3.7261 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1222: loss = 2.6128 (0.640 sec/step)\n",
            "I0915 13:58:45.218246 139848901543808 learning.py:507] global step 1222: loss = 2.6128 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1223: loss = 1.3974 (0.646 sec/step)\n",
            "I0915 13:58:45.866229 139848901543808 learning.py:507] global step 1223: loss = 1.3974 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1224: loss = 4.2236 (0.646 sec/step)\n",
            "I0915 13:58:46.514439 139848901543808 learning.py:507] global step 1224: loss = 4.2236 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1225: loss = 2.0096 (0.655 sec/step)\n",
            "I0915 13:58:47.171980 139848901543808 learning.py:507] global step 1225: loss = 2.0096 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1226: loss = 2.1335 (0.639 sec/step)\n",
            "I0915 13:58:47.814476 139848901543808 learning.py:507] global step 1226: loss = 2.1335 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1227: loss = 3.5599 (0.649 sec/step)\n",
            "I0915 13:58:48.468290 139848901543808 learning.py:507] global step 1227: loss = 3.5599 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1228: loss = 1.8312 (0.655 sec/step)\n",
            "I0915 13:58:49.124961 139848901543808 learning.py:507] global step 1228: loss = 1.8312 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1229: loss = 2.7546 (0.636 sec/step)\n",
            "I0915 13:58:49.763286 139848901543808 learning.py:507] global step 1229: loss = 2.7546 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1230: loss = 2.3570 (0.656 sec/step)\n",
            "I0915 13:58:50.420953 139848901543808 learning.py:507] global step 1230: loss = 2.3570 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1231: loss = 1.5037 (0.653 sec/step)\n",
            "I0915 13:58:51.075846 139848901543808 learning.py:507] global step 1231: loss = 1.5037 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1232: loss = 1.4322 (0.647 sec/step)\n",
            "I0915 13:58:51.724428 139848901543808 learning.py:507] global step 1232: loss = 1.4322 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1233: loss = 2.7325 (0.644 sec/step)\n",
            "I0915 13:58:52.370479 139848901543808 learning.py:507] global step 1233: loss = 2.7325 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1234: loss = 1.6848 (0.648 sec/step)\n",
            "I0915 13:58:53.020615 139848901543808 learning.py:507] global step 1234: loss = 1.6848 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1235: loss = 1.6262 (0.627 sec/step)\n",
            "I0915 13:58:53.649145 139848901543808 learning.py:507] global step 1235: loss = 1.6262 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1236: loss = 2.4743 (0.658 sec/step)\n",
            "I0915 13:58:54.309258 139848901543808 learning.py:507] global step 1236: loss = 2.4743 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1237: loss = 2.4759 (0.627 sec/step)\n",
            "I0915 13:58:54.938636 139848901543808 learning.py:507] global step 1237: loss = 2.4759 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1238: loss = 2.5107 (0.620 sec/step)\n",
            "I0915 13:58:55.560558 139848901543808 learning.py:507] global step 1238: loss = 2.5107 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1239: loss = 2.1905 (0.635 sec/step)\n",
            "I0915 13:58:56.197224 139848901543808 learning.py:507] global step 1239: loss = 2.1905 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1240: loss = 2.6176 (0.653 sec/step)\n",
            "I0915 13:58:56.852510 139848901543808 learning.py:507] global step 1240: loss = 2.6176 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1241: loss = 2.5750 (0.633 sec/step)\n",
            "I0915 13:58:57.487399 139848901543808 learning.py:507] global step 1241: loss = 2.5750 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1242: loss = 2.4444 (0.654 sec/step)\n",
            "I0915 13:58:58.143727 139848901543808 learning.py:507] global step 1242: loss = 2.4444 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1243: loss = 2.0280 (0.628 sec/step)\n",
            "I0915 13:58:58.772999 139848901543808 learning.py:507] global step 1243: loss = 2.0280 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1244: loss = 2.7248 (0.649 sec/step)\n",
            "I0915 13:58:59.424056 139848901543808 learning.py:507] global step 1244: loss = 2.7248 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1245: loss = 2.6769 (0.634 sec/step)\n",
            "I0915 13:59:00.059586 139848901543808 learning.py:507] global step 1245: loss = 2.6769 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1246: loss = 2.4204 (0.625 sec/step)\n",
            "I0915 13:59:00.686931 139848901543808 learning.py:507] global step 1246: loss = 2.4204 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1247: loss = 2.4864 (0.673 sec/step)\n",
            "I0915 13:59:01.361666 139848901543808 learning.py:507] global step 1247: loss = 2.4864 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1248: loss = 1.4508 (0.629 sec/step)\n",
            "I0915 13:59:01.992841 139848901543808 learning.py:507] global step 1248: loss = 1.4508 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1249: loss = 2.0994 (0.644 sec/step)\n",
            "I0915 13:59:02.638415 139848901543808 learning.py:507] global step 1249: loss = 2.0994 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1250: loss = 1.9616 (0.637 sec/step)\n",
            "I0915 13:59:03.277728 139848901543808 learning.py:507] global step 1250: loss = 1.9616 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1251: loss = 2.6471 (0.640 sec/step)\n",
            "I0915 13:59:03.919575 139848901543808 learning.py:507] global step 1251: loss = 2.6471 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1252: loss = 2.2009 (0.633 sec/step)\n",
            "I0915 13:59:04.555139 139848901543808 learning.py:507] global step 1252: loss = 2.2009 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1253: loss = 1.9653 (0.667 sec/step)\n",
            "I0915 13:59:05.224282 139848901543808 learning.py:507] global step 1253: loss = 1.9653 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1254: loss = 2.2901 (0.638 sec/step)\n",
            "I0915 13:59:05.864811 139848901543808 learning.py:507] global step 1254: loss = 2.2901 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1255: loss = 1.7700 (0.628 sec/step)\n",
            "I0915 13:59:06.494531 139848901543808 learning.py:507] global step 1255: loss = 1.7700 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1256: loss = 3.2230 (0.632 sec/step)\n",
            "I0915 13:59:07.128161 139848901543808 learning.py:507] global step 1256: loss = 3.2230 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1257: loss = 2.3883 (0.642 sec/step)\n",
            "I0915 13:59:07.772704 139848901543808 learning.py:507] global step 1257: loss = 2.3883 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1258: loss = 1.6715 (0.637 sec/step)\n",
            "I0915 13:59:08.411749 139848901543808 learning.py:507] global step 1258: loss = 1.6715 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1259: loss = 1.9430 (0.622 sec/step)\n",
            "I0915 13:59:09.035685 139848901543808 learning.py:507] global step 1259: loss = 1.9430 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1260: loss = 2.1725 (0.670 sec/step)\n",
            "I0915 13:59:09.707762 139848901543808 learning.py:507] global step 1260: loss = 2.1725 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1261: loss = 2.3003 (0.646 sec/step)\n",
            "I0915 13:59:10.356593 139848901543808 learning.py:507] global step 1261: loss = 2.3003 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1262: loss = 2.8163 (0.628 sec/step)\n",
            "I0915 13:59:10.986402 139848901543808 learning.py:507] global step 1262: loss = 2.8163 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1263: loss = 1.8497 (0.666 sec/step)\n",
            "I0915 13:59:11.654833 139848901543808 learning.py:507] global step 1263: loss = 1.8497 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1264: loss = 2.7008 (0.628 sec/step)\n",
            "I0915 13:59:12.284766 139848901543808 learning.py:507] global step 1264: loss = 2.7008 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1265: loss = 2.0771 (0.676 sec/step)\n",
            "I0915 13:59:12.962391 139848901543808 learning.py:507] global step 1265: loss = 2.0771 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1266: loss = 1.3863 (0.635 sec/step)\n",
            "I0915 13:59:13.599178 139848901543808 learning.py:507] global step 1266: loss = 1.3863 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1267: loss = 2.1437 (0.642 sec/step)\n",
            "I0915 13:59:14.243576 139848901543808 learning.py:507] global step 1267: loss = 2.1437 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1268: loss = 2.8313 (0.644 sec/step)\n",
            "I0915 13:59:14.890418 139848901543808 learning.py:507] global step 1268: loss = 2.8313 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1269: loss = 2.1505 (0.635 sec/step)\n",
            "I0915 13:59:15.528076 139848901543808 learning.py:507] global step 1269: loss = 2.1505 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1270: loss = 2.9008 (0.630 sec/step)\n",
            "I0915 13:59:16.159902 139848901543808 learning.py:507] global step 1270: loss = 2.9008 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1271: loss = 2.2552 (0.644 sec/step)\n",
            "I0915 13:59:16.805483 139848901543808 learning.py:507] global step 1271: loss = 2.2552 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1272: loss = 2.4772 (0.622 sec/step)\n",
            "I0915 13:59:17.430011 139848901543808 learning.py:507] global step 1272: loss = 2.4772 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1273: loss = 2.9097 (0.632 sec/step)\n",
            "I0915 13:59:18.064399 139848901543808 learning.py:507] global step 1273: loss = 2.9097 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1274: loss = 3.4120 (0.623 sec/step)\n",
            "I0915 13:59:18.689970 139848901543808 learning.py:507] global step 1274: loss = 3.4120 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1275: loss = 2.5463 (0.627 sec/step)\n",
            "I0915 13:59:19.319003 139848901543808 learning.py:507] global step 1275: loss = 2.5463 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1276: loss = 2.1681 (0.646 sec/step)\n",
            "I0915 13:59:19.966516 139848901543808 learning.py:507] global step 1276: loss = 2.1681 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1277: loss = 1.9155 (0.639 sec/step)\n",
            "I0915 13:59:20.606900 139848901543808 learning.py:507] global step 1277: loss = 1.9155 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1278: loss = 2.8471 (0.646 sec/step)\n",
            "I0915 13:59:21.255209 139848901543808 learning.py:507] global step 1278: loss = 2.8471 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1279: loss = 2.3864 (0.641 sec/step)\n",
            "I0915 13:59:21.898456 139848901543808 learning.py:507] global step 1279: loss = 2.3864 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1280: loss = 2.1163 (0.633 sec/step)\n",
            "I0915 13:59:22.533586 139848901543808 learning.py:507] global step 1280: loss = 2.1163 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1281: loss = 2.0411 (0.730 sec/step)\n",
            "I0915 13:59:23.265498 139848901543808 learning.py:507] global step 1281: loss = 2.0411 (0.730 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1281.\n",
            "I0915 13:59:23.988099 139845834331904 supervisor.py:1050] Recording summary at step 1281.\n",
            "INFO:tensorflow:global step 1282: loss = 2.0079 (1.025 sec/step)\n",
            "I0915 13:59:24.317893 139848901543808 learning.py:507] global step 1282: loss = 2.0079 (1.025 sec/step)\n",
            "INFO:tensorflow:global step 1283: loss = 1.6320 (0.656 sec/step)\n",
            "I0915 13:59:24.975808 139848901543808 learning.py:507] global step 1283: loss = 1.6320 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1284: loss = 1.6414 (0.633 sec/step)\n",
            "I0915 13:59:25.611049 139848901543808 learning.py:507] global step 1284: loss = 1.6414 (0.633 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.54866\n",
            "I0915 13:59:26.012041 139845842724608 supervisor.py:1099] global_step/sec: 1.54866\n",
            "INFO:tensorflow:global step 1285: loss = 2.2673 (0.657 sec/step)\n",
            "I0915 13:59:26.269772 139848901543808 learning.py:507] global step 1285: loss = 2.2673 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1286: loss = 1.6278 (0.622 sec/step)\n",
            "I0915 13:59:26.893860 139848901543808 learning.py:507] global step 1286: loss = 1.6278 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1287: loss = 1.7226 (0.625 sec/step)\n",
            "I0915 13:59:27.521260 139848901543808 learning.py:507] global step 1287: loss = 1.7226 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1288: loss = 3.0393 (0.636 sec/step)\n",
            "I0915 13:59:28.159972 139848901543808 learning.py:507] global step 1288: loss = 3.0393 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1289: loss = 2.3170 (0.639 sec/step)\n",
            "I0915 13:59:28.800923 139848901543808 learning.py:507] global step 1289: loss = 2.3170 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1290: loss = 3.2257 (0.638 sec/step)\n",
            "I0915 13:59:29.440704 139848901543808 learning.py:507] global step 1290: loss = 3.2257 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1291: loss = 1.9722 (0.651 sec/step)\n",
            "I0915 13:59:30.093782 139848901543808 learning.py:507] global step 1291: loss = 1.9722 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1292: loss = 1.4438 (0.645 sec/step)\n",
            "I0915 13:59:30.740710 139848901543808 learning.py:507] global step 1292: loss = 1.4438 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1293: loss = 2.7404 (0.641 sec/step)\n",
            "I0915 13:59:31.383518 139848901543808 learning.py:507] global step 1293: loss = 2.7404 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1294: loss = 2.1304 (0.656 sec/step)\n",
            "I0915 13:59:32.041468 139848901543808 learning.py:507] global step 1294: loss = 2.1304 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1295: loss = 2.0486 (0.647 sec/step)\n",
            "I0915 13:59:32.690438 139848901543808 learning.py:507] global step 1295: loss = 2.0486 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1296: loss = 2.9790 (0.666 sec/step)\n",
            "I0915 13:59:33.358363 139848901543808 learning.py:507] global step 1296: loss = 2.9790 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1297: loss = 1.7626 (0.630 sec/step)\n",
            "I0915 13:59:33.990328 139848901543808 learning.py:507] global step 1297: loss = 1.7626 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1298: loss = 1.2514 (0.642 sec/step)\n",
            "I0915 13:59:34.634965 139848901543808 learning.py:507] global step 1298: loss = 1.2514 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1299: loss = 1.7856 (0.654 sec/step)\n",
            "I0915 13:59:35.292125 139848901543808 learning.py:507] global step 1299: loss = 1.7856 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1300: loss = 1.3172 (0.644 sec/step)\n",
            "I0915 13:59:35.937955 139848901543808 learning.py:507] global step 1300: loss = 1.3172 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1301: loss = 1.4218 (0.654 sec/step)\n",
            "I0915 13:59:36.594055 139848901543808 learning.py:507] global step 1301: loss = 1.4218 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1302: loss = 2.4490 (0.627 sec/step)\n",
            "I0915 13:59:37.223504 139848901543808 learning.py:507] global step 1302: loss = 2.4490 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1303: loss = 2.2648 (0.628 sec/step)\n",
            "I0915 13:59:37.853768 139848901543808 learning.py:507] global step 1303: loss = 2.2648 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1304: loss = 2.9613 (0.654 sec/step)\n",
            "I0915 13:59:38.510514 139848901543808 learning.py:507] global step 1304: loss = 2.9613 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1305: loss = 2.3516 (0.625 sec/step)\n",
            "I0915 13:59:39.139523 139848901543808 learning.py:507] global step 1305: loss = 2.3516 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1306: loss = 2.5762 (0.619 sec/step)\n",
            "I0915 13:59:39.764856 139848901543808 learning.py:507] global step 1306: loss = 2.5762 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1307: loss = 2.3775 (0.671 sec/step)\n",
            "I0915 13:59:40.437902 139848901543808 learning.py:507] global step 1307: loss = 2.3775 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 1308: loss = 2.3798 (0.639 sec/step)\n",
            "I0915 13:59:41.078936 139848901543808 learning.py:507] global step 1308: loss = 2.3798 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1309: loss = 3.3024 (0.666 sec/step)\n",
            "I0915 13:59:41.747293 139848901543808 learning.py:507] global step 1309: loss = 3.3024 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1310: loss = 2.0827 (0.636 sec/step)\n",
            "I0915 13:59:42.385522 139848901543808 learning.py:507] global step 1310: loss = 2.0827 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1311: loss = 2.7894 (0.664 sec/step)\n",
            "I0915 13:59:43.051419 139848901543808 learning.py:507] global step 1311: loss = 2.7894 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1312: loss = 2.6118 (0.635 sec/step)\n",
            "I0915 13:59:43.688340 139848901543808 learning.py:507] global step 1312: loss = 2.6118 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1313: loss = 2.3469 (0.623 sec/step)\n",
            "I0915 13:59:44.312874 139848901543808 learning.py:507] global step 1313: loss = 2.3469 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1314: loss = 2.0828 (0.642 sec/step)\n",
            "I0915 13:59:44.956515 139848901543808 learning.py:507] global step 1314: loss = 2.0828 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1315: loss = 1.8826 (0.632 sec/step)\n",
            "I0915 13:59:45.590229 139848901543808 learning.py:507] global step 1315: loss = 1.8826 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1316: loss = 2.8429 (0.652 sec/step)\n",
            "I0915 13:59:46.243928 139848901543808 learning.py:507] global step 1316: loss = 2.8429 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1317: loss = 2.6137 (0.656 sec/step)\n",
            "I0915 13:59:46.901998 139848901543808 learning.py:507] global step 1317: loss = 2.6137 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1318: loss = 2.7435 (0.646 sec/step)\n",
            "I0915 13:59:47.550488 139848901543808 learning.py:507] global step 1318: loss = 2.7435 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1319: loss = 2.4462 (0.630 sec/step)\n",
            "I0915 13:59:48.182865 139848901543808 learning.py:507] global step 1319: loss = 2.4462 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1320: loss = 1.9199 (0.654 sec/step)\n",
            "I0915 13:59:48.839025 139848901543808 learning.py:507] global step 1320: loss = 1.9199 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1321: loss = 1.9101 (0.633 sec/step)\n",
            "I0915 13:59:49.474019 139848901543808 learning.py:507] global step 1321: loss = 1.9101 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1322: loss = 1.9196 (0.630 sec/step)\n",
            "I0915 13:59:50.105904 139848901543808 learning.py:507] global step 1322: loss = 1.9196 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1323: loss = 2.3122 (0.655 sec/step)\n",
            "I0915 13:59:50.763969 139848901543808 learning.py:507] global step 1323: loss = 2.3122 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1324: loss = 1.3013 (0.649 sec/step)\n",
            "I0915 13:59:51.414819 139848901543808 learning.py:507] global step 1324: loss = 1.3013 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1325: loss = 2.5804 (0.665 sec/step)\n",
            "I0915 13:59:52.081852 139848901543808 learning.py:507] global step 1325: loss = 2.5804 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1326: loss = 1.8616 (0.658 sec/step)\n",
            "I0915 13:59:52.741453 139848901543808 learning.py:507] global step 1326: loss = 1.8616 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1327: loss = 2.6901 (0.659 sec/step)\n",
            "I0915 13:59:53.402226 139848901543808 learning.py:507] global step 1327: loss = 2.6901 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1328: loss = 2.5934 (0.622 sec/step)\n",
            "I0915 13:59:54.026062 139848901543808 learning.py:507] global step 1328: loss = 2.5934 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1329: loss = 2.6629 (0.638 sec/step)\n",
            "I0915 13:59:54.665714 139848901543808 learning.py:507] global step 1329: loss = 2.6629 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1330: loss = 2.1319 (0.636 sec/step)\n",
            "I0915 13:59:55.303046 139848901543808 learning.py:507] global step 1330: loss = 2.1319 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1331: loss = 2.2858 (0.640 sec/step)\n",
            "I0915 13:59:55.945253 139848901543808 learning.py:507] global step 1331: loss = 2.2858 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1332: loss = 1.4247 (0.623 sec/step)\n",
            "I0915 13:59:56.570169 139848901543808 learning.py:507] global step 1332: loss = 1.4247 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1333: loss = 2.4628 (0.630 sec/step)\n",
            "I0915 13:59:57.202645 139848901543808 learning.py:507] global step 1333: loss = 2.4628 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1334: loss = 2.4403 (0.638 sec/step)\n",
            "I0915 13:59:57.842610 139848901543808 learning.py:507] global step 1334: loss = 2.4403 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1335: loss = 2.8443 (0.637 sec/step)\n",
            "I0915 13:59:58.481837 139848901543808 learning.py:507] global step 1335: loss = 2.8443 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1336: loss = 2.0438 (0.634 sec/step)\n",
            "I0915 13:59:59.118245 139848901543808 learning.py:507] global step 1336: loss = 2.0438 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1337: loss = 2.2135 (0.625 sec/step)\n",
            "I0915 13:59:59.745232 139848901543808 learning.py:507] global step 1337: loss = 2.2135 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1338: loss = 2.4020 (0.634 sec/step)\n",
            "I0915 14:00:00.381553 139848901543808 learning.py:507] global step 1338: loss = 2.4020 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1339: loss = 2.0707 (0.648 sec/step)\n",
            "I0915 14:00:01.031700 139848901543808 learning.py:507] global step 1339: loss = 2.0707 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1340: loss = 2.3106 (0.632 sec/step)\n",
            "I0915 14:00:01.665895 139848901543808 learning.py:507] global step 1340: loss = 2.3106 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1341: loss = 3.4199 (0.655 sec/step)\n",
            "I0915 14:00:02.323102 139848901543808 learning.py:507] global step 1341: loss = 3.4199 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1342: loss = 2.4485 (0.633 sec/step)\n",
            "I0915 14:00:02.958322 139848901543808 learning.py:507] global step 1342: loss = 2.4485 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1343: loss = 2.2941 (0.659 sec/step)\n",
            "I0915 14:00:03.618912 139848901543808 learning.py:507] global step 1343: loss = 2.2941 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1344: loss = 2.5118 (0.637 sec/step)\n",
            "I0915 14:00:04.257492 139848901543808 learning.py:507] global step 1344: loss = 2.5118 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1345: loss = 1.6287 (0.617 sec/step)\n",
            "I0915 14:00:04.879015 139848901543808 learning.py:507] global step 1345: loss = 1.6287 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1346: loss = 2.2204 (0.617 sec/step)\n",
            "I0915 14:00:05.499235 139848901543808 learning.py:507] global step 1346: loss = 2.2204 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1347: loss = 2.3999 (0.644 sec/step)\n",
            "I0915 14:00:06.144913 139848901543808 learning.py:507] global step 1347: loss = 2.3999 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1348: loss = 1.6378 (0.644 sec/step)\n",
            "I0915 14:00:06.791449 139848901543808 learning.py:507] global step 1348: loss = 1.6378 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1349: loss = 3.4446 (0.628 sec/step)\n",
            "I0915 14:00:07.421295 139848901543808 learning.py:507] global step 1349: loss = 3.4446 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1350: loss = 2.2784 (0.627 sec/step)\n",
            "I0915 14:00:08.050603 139848901543808 learning.py:507] global step 1350: loss = 2.2784 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1351: loss = 2.3736 (0.649 sec/step)\n",
            "I0915 14:00:08.702299 139848901543808 learning.py:507] global step 1351: loss = 2.3736 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1352: loss = 2.8896 (0.656 sec/step)\n",
            "I0915 14:00:09.360683 139848901543808 learning.py:507] global step 1352: loss = 2.8896 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1353: loss = 2.3956 (0.627 sec/step)\n",
            "I0915 14:00:09.990247 139848901543808 learning.py:507] global step 1353: loss = 2.3956 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1354: loss = 4.5098 (0.659 sec/step)\n",
            "I0915 14:00:10.651298 139848901543808 learning.py:507] global step 1354: loss = 4.5098 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1355: loss = 1.6939 (0.672 sec/step)\n",
            "I0915 14:00:11.325832 139848901543808 learning.py:507] global step 1355: loss = 1.6939 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1356: loss = 1.8959 (0.643 sec/step)\n",
            "I0915 14:00:11.970916 139848901543808 learning.py:507] global step 1356: loss = 1.8959 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1357: loss = 1.6518 (0.642 sec/step)\n",
            "I0915 14:00:12.614409 139848901543808 learning.py:507] global step 1357: loss = 1.6518 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1358: loss = 2.4442 (0.643 sec/step)\n",
            "I0915 14:00:13.258859 139848901543808 learning.py:507] global step 1358: loss = 2.4442 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1359: loss = 2.9463 (0.648 sec/step)\n",
            "I0915 14:00:13.908738 139848901543808 learning.py:507] global step 1359: loss = 2.9463 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1360: loss = 2.2738 (0.662 sec/step)\n",
            "I0915 14:00:14.572897 139848901543808 learning.py:507] global step 1360: loss = 2.2738 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1361: loss = 2.9307 (0.629 sec/step)\n",
            "I0915 14:00:15.203936 139848901543808 learning.py:507] global step 1361: loss = 2.9307 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1362: loss = 3.3092 (0.644 sec/step)\n",
            "I0915 14:00:15.850487 139848901543808 learning.py:507] global step 1362: loss = 3.3092 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1363: loss = 1.9850 (0.650 sec/step)\n",
            "I0915 14:00:16.503895 139848901543808 learning.py:507] global step 1363: loss = 1.9850 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1364: loss = 2.3004 (0.640 sec/step)\n",
            "I0915 14:00:17.146057 139848901543808 learning.py:507] global step 1364: loss = 2.3004 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1365: loss = 2.5001 (0.650 sec/step)\n",
            "I0915 14:00:17.797803 139848901543808 learning.py:507] global step 1365: loss = 2.5001 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1366: loss = 2.7953 (0.636 sec/step)\n",
            "I0915 14:00:18.435981 139848901543808 learning.py:507] global step 1366: loss = 2.7953 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1367: loss = 3.3004 (0.629 sec/step)\n",
            "I0915 14:00:19.066460 139848901543808 learning.py:507] global step 1367: loss = 3.3004 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1368: loss = 2.2012 (0.619 sec/step)\n",
            "I0915 14:00:19.687336 139848901543808 learning.py:507] global step 1368: loss = 2.2012 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1369: loss = 2.2964 (0.635 sec/step)\n",
            "I0915 14:00:20.324820 139848901543808 learning.py:507] global step 1369: loss = 2.2964 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1370: loss = 2.9487 (0.639 sec/step)\n",
            "I0915 14:00:20.966378 139848901543808 learning.py:507] global step 1370: loss = 2.9487 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1371: loss = 2.8047 (0.622 sec/step)\n",
            "I0915 14:00:21.590277 139848901543808 learning.py:507] global step 1371: loss = 2.8047 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1372: loss = 2.3664 (0.627 sec/step)\n",
            "I0915 14:00:22.219508 139848901543808 learning.py:507] global step 1372: loss = 2.3664 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1373: loss = 2.1142 (0.653 sec/step)\n",
            "I0915 14:00:22.874297 139848901543808 learning.py:507] global step 1373: loss = 2.1142 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1374: loss = 2.8391 (0.648 sec/step)\n",
            "I0915 14:00:23.524753 139848901543808 learning.py:507] global step 1374: loss = 2.8391 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1375: loss = 3.2381 (0.650 sec/step)\n",
            "I0915 14:00:24.176378 139848901543808 learning.py:507] global step 1375: loss = 3.2381 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1376: loss = 3.1959 (0.648 sec/step)\n",
            "I0915 14:00:24.826782 139848901543808 learning.py:507] global step 1376: loss = 3.1959 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1377: loss = 2.4204 (0.652 sec/step)\n",
            "I0915 14:00:25.481194 139848901543808 learning.py:507] global step 1377: loss = 2.4204 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1378: loss = 1.7342 (0.655 sec/step)\n",
            "I0915 14:00:26.137851 139848901543808 learning.py:507] global step 1378: loss = 1.7342 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1379: loss = 3.3292 (0.630 sec/step)\n",
            "I0915 14:00:26.769320 139848901543808 learning.py:507] global step 1379: loss = 3.3292 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1380: loss = 2.3842 (0.628 sec/step)\n",
            "I0915 14:00:27.399514 139848901543808 learning.py:507] global step 1380: loss = 2.3842 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1381: loss = 2.1236 (0.636 sec/step)\n",
            "I0915 14:00:28.038129 139848901543808 learning.py:507] global step 1381: loss = 2.1236 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1382: loss = 2.2836 (0.611 sec/step)\n",
            "I0915 14:00:28.650758 139848901543808 learning.py:507] global step 1382: loss = 2.2836 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1383: loss = 2.4384 (0.634 sec/step)\n",
            "I0915 14:00:29.286983 139848901543808 learning.py:507] global step 1383: loss = 2.4384 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1384: loss = 1.7205 (0.626 sec/step)\n",
            "I0915 14:00:29.915379 139848901543808 learning.py:507] global step 1384: loss = 1.7205 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1385: loss = 2.5453 (0.635 sec/step)\n",
            "I0915 14:00:30.552637 139848901543808 learning.py:507] global step 1385: loss = 2.5453 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1386: loss = 2.8647 (0.634 sec/step)\n",
            "I0915 14:00:31.188253 139848901543808 learning.py:507] global step 1386: loss = 2.8647 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1387: loss = 1.5077 (0.638 sec/step)\n",
            "I0915 14:00:31.827700 139848901543808 learning.py:507] global step 1387: loss = 1.5077 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1388: loss = 3.2837 (0.655 sec/step)\n",
            "I0915 14:00:32.484513 139848901543808 learning.py:507] global step 1388: loss = 3.2837 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1389: loss = 2.3126 (0.652 sec/step)\n",
            "I0915 14:00:33.138203 139848901543808 learning.py:507] global step 1389: loss = 2.3126 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1390: loss = 1.9399 (0.627 sec/step)\n",
            "I0915 14:00:33.767226 139848901543808 learning.py:507] global step 1390: loss = 1.9399 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1391: loss = 2.4254 (0.668 sec/step)\n",
            "I0915 14:00:34.437330 139848901543808 learning.py:507] global step 1391: loss = 2.4254 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1392: loss = 4.0410 (0.641 sec/step)\n",
            "I0915 14:00:35.080630 139848901543808 learning.py:507] global step 1392: loss = 4.0410 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1393: loss = 1.9883 (0.647 sec/step)\n",
            "I0915 14:00:35.729448 139848901543808 learning.py:507] global step 1393: loss = 1.9883 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1394: loss = 1.6966 (0.635 sec/step)\n",
            "I0915 14:00:36.366431 139848901543808 learning.py:507] global step 1394: loss = 1.6966 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1395: loss = 2.1014 (0.638 sec/step)\n",
            "I0915 14:00:37.006338 139848901543808 learning.py:507] global step 1395: loss = 2.1014 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1396: loss = 2.0794 (0.637 sec/step)\n",
            "I0915 14:00:37.645679 139848901543808 learning.py:507] global step 1396: loss = 2.0794 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1397: loss = 2.8337 (0.638 sec/step)\n",
            "I0915 14:00:38.285445 139848901543808 learning.py:507] global step 1397: loss = 2.8337 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1398: loss = 2.4035 (0.632 sec/step)\n",
            "I0915 14:00:38.919223 139848901543808 learning.py:507] global step 1398: loss = 2.4035 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1399: loss = 2.1633 (0.633 sec/step)\n",
            "I0915 14:00:39.554378 139848901543808 learning.py:507] global step 1399: loss = 2.1633 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1400: loss = 1.5347 (0.654 sec/step)\n",
            "I0915 14:00:40.210576 139848901543808 learning.py:507] global step 1400: loss = 1.5347 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1401: loss = 1.9959 (0.643 sec/step)\n",
            "I0915 14:00:40.855171 139848901543808 learning.py:507] global step 1401: loss = 1.9959 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1402: loss = 2.4006 (0.650 sec/step)\n",
            "I0915 14:00:41.507164 139848901543808 learning.py:507] global step 1402: loss = 2.4006 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1403: loss = 2.1684 (0.654 sec/step)\n",
            "I0915 14:00:42.163811 139848901543808 learning.py:507] global step 1403: loss = 2.1684 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1404: loss = 1.6992 (0.670 sec/step)\n",
            "I0915 14:00:42.836058 139848901543808 learning.py:507] global step 1404: loss = 1.6992 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1405: loss = 3.1123 (0.651 sec/step)\n",
            "I0915 14:00:43.489077 139848901543808 learning.py:507] global step 1405: loss = 3.1123 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1406: loss = 3.1848 (0.613 sec/step)\n",
            "I0915 14:00:44.104453 139848901543808 learning.py:507] global step 1406: loss = 3.1848 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 1407: loss = 2.2556 (0.656 sec/step)\n",
            "I0915 14:00:44.762638 139848901543808 learning.py:507] global step 1407: loss = 2.2556 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1408: loss = 2.0520 (0.657 sec/step)\n",
            "I0915 14:00:45.422321 139848901543808 learning.py:507] global step 1408: loss = 2.0520 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1409: loss = 2.3995 (0.627 sec/step)\n",
            "I0915 14:00:46.051525 139848901543808 learning.py:507] global step 1409: loss = 2.3995 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1410: loss = 1.6220 (0.629 sec/step)\n",
            "I0915 14:00:46.682479 139848901543808 learning.py:507] global step 1410: loss = 1.6220 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1411: loss = 1.7160 (0.651 sec/step)\n",
            "I0915 14:00:47.335160 139848901543808 learning.py:507] global step 1411: loss = 1.7160 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1412: loss = 3.1947 (0.615 sec/step)\n",
            "I0915 14:00:47.951960 139848901543808 learning.py:507] global step 1412: loss = 3.1947 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1413: loss = 2.3980 (0.669 sec/step)\n",
            "I0915 14:00:48.623371 139848901543808 learning.py:507] global step 1413: loss = 2.3980 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1414: loss = 1.8560 (0.650 sec/step)\n",
            "I0915 14:00:49.275572 139848901543808 learning.py:507] global step 1414: loss = 1.8560 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1415: loss = 2.1735 (0.634 sec/step)\n",
            "I0915 14:00:49.912018 139848901543808 learning.py:507] global step 1415: loss = 2.1735 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1416: loss = 3.1451 (0.645 sec/step)\n",
            "I0915 14:00:50.559123 139848901543808 learning.py:507] global step 1416: loss = 3.1451 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1417: loss = 2.2221 (0.629 sec/step)\n",
            "I0915 14:00:51.189735 139848901543808 learning.py:507] global step 1417: loss = 2.2221 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1418: loss = 2.3293 (0.646 sec/step)\n",
            "I0915 14:00:51.837844 139848901543808 learning.py:507] global step 1418: loss = 2.3293 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1419: loss = 2.3485 (0.632 sec/step)\n",
            "I0915 14:00:52.471585 139848901543808 learning.py:507] global step 1419: loss = 2.3485 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1420: loss = 3.0059 (0.616 sec/step)\n",
            "I0915 14:00:53.089846 139848901543808 learning.py:507] global step 1420: loss = 3.0059 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1421: loss = 2.2401 (0.642 sec/step)\n",
            "I0915 14:00:53.733865 139848901543808 learning.py:507] global step 1421: loss = 2.2401 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1422: loss = 1.7753 (0.645 sec/step)\n",
            "I0915 14:00:54.380740 139848901543808 learning.py:507] global step 1422: loss = 1.7753 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1423: loss = 3.5838 (0.633 sec/step)\n",
            "I0915 14:00:55.015441 139848901543808 learning.py:507] global step 1423: loss = 3.5838 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1424: loss = 2.1750 (0.632 sec/step)\n",
            "I0915 14:00:55.649734 139848901543808 learning.py:507] global step 1424: loss = 2.1750 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1425: loss = 2.6488 (0.626 sec/step)\n",
            "I0915 14:00:56.277546 139848901543808 learning.py:507] global step 1425: loss = 2.6488 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1426: loss = 2.2940 (0.649 sec/step)\n",
            "I0915 14:00:56.929139 139848901543808 learning.py:507] global step 1426: loss = 2.2940 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1427: loss = 2.3929 (0.620 sec/step)\n",
            "I0915 14:00:57.551501 139848901543808 learning.py:507] global step 1427: loss = 2.3929 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1428: loss = 2.3074 (0.643 sec/step)\n",
            "I0915 14:00:58.197463 139848901543808 learning.py:507] global step 1428: loss = 2.3074 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1429: loss = 2.7360 (0.633 sec/step)\n",
            "I0915 14:00:58.833021 139848901543808 learning.py:507] global step 1429: loss = 2.7360 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1430: loss = 2.3094 (0.630 sec/step)\n",
            "I0915 14:00:59.467562 139848901543808 learning.py:507] global step 1430: loss = 2.3094 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1431: loss = 1.9658 (0.634 sec/step)\n",
            "I0915 14:01:00.103586 139848901543808 learning.py:507] global step 1431: loss = 1.9658 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1432: loss = 2.3077 (0.638 sec/step)\n",
            "I0915 14:01:00.743286 139848901543808 learning.py:507] global step 1432: loss = 2.3077 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1433: loss = 3.0295 (0.628 sec/step)\n",
            "I0915 14:01:01.373169 139848901543808 learning.py:507] global step 1433: loss = 3.0295 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1434: loss = 3.9640 (0.645 sec/step)\n",
            "I0915 14:01:02.020589 139848901543808 learning.py:507] global step 1434: loss = 3.9640 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1435: loss = 2.6147 (0.641 sec/step)\n",
            "I0915 14:01:02.664104 139848901543808 learning.py:507] global step 1435: loss = 2.6147 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1436: loss = 2.9493 (0.642 sec/step)\n",
            "I0915 14:01:03.308403 139848901543808 learning.py:507] global step 1436: loss = 2.9493 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1437: loss = 2.6836 (0.667 sec/step)\n",
            "I0915 14:01:03.977924 139848901543808 learning.py:507] global step 1437: loss = 2.6836 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1438: loss = 2.0558 (0.618 sec/step)\n",
            "I0915 14:01:04.597915 139848901543808 learning.py:507] global step 1438: loss = 2.0558 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1439: loss = 2.3833 (0.660 sec/step)\n",
            "I0915 14:01:05.259401 139848901543808 learning.py:507] global step 1439: loss = 2.3833 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1440: loss = 2.2613 (0.628 sec/step)\n",
            "I0915 14:01:05.889434 139848901543808 learning.py:507] global step 1440: loss = 2.2613 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1441: loss = 1.9174 (0.641 sec/step)\n",
            "I0915 14:01:06.532795 139848901543808 learning.py:507] global step 1441: loss = 1.9174 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1442: loss = 2.0463 (0.641 sec/step)\n",
            "I0915 14:01:07.175608 139848901543808 learning.py:507] global step 1442: loss = 2.0463 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1443: loss = 1.4739 (0.651 sec/step)\n",
            "I0915 14:01:07.828116 139848901543808 learning.py:507] global step 1443: loss = 1.4739 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1444: loss = 2.2139 (0.665 sec/step)\n",
            "I0915 14:01:08.495299 139848901543808 learning.py:507] global step 1444: loss = 2.2139 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1445: loss = 2.4353 (0.638 sec/step)\n",
            "I0915 14:01:09.134804 139848901543808 learning.py:507] global step 1445: loss = 2.4353 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1446: loss = 2.3728 (0.621 sec/step)\n",
            "I0915 14:01:09.758263 139848901543808 learning.py:507] global step 1446: loss = 2.3728 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1447: loss = 2.0564 (0.631 sec/step)\n",
            "I0915 14:01:10.391750 139848901543808 learning.py:507] global step 1447: loss = 2.0564 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1448: loss = 2.2273 (0.676 sec/step)\n",
            "I0915 14:01:11.069384 139848901543808 learning.py:507] global step 1448: loss = 2.2273 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1449: loss = 1.8670 (0.652 sec/step)\n",
            "I0915 14:01:11.723514 139848901543808 learning.py:507] global step 1449: loss = 1.8670 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1450: loss = 1.6207 (0.631 sec/step)\n",
            "I0915 14:01:12.356906 139848901543808 learning.py:507] global step 1450: loss = 1.6207 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1451: loss = 1.8956 (0.638 sec/step)\n",
            "I0915 14:01:12.996662 139848901543808 learning.py:507] global step 1451: loss = 1.8956 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1452: loss = 2.1331 (0.642 sec/step)\n",
            "I0915 14:01:13.640943 139848901543808 learning.py:507] global step 1452: loss = 2.1331 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1453: loss = 2.7184 (0.654 sec/step)\n",
            "I0915 14:01:14.296454 139848901543808 learning.py:507] global step 1453: loss = 2.7184 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1454: loss = 2.5849 (0.634 sec/step)\n",
            "I0915 14:01:14.933182 139848901543808 learning.py:507] global step 1454: loss = 2.5849 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1455: loss = 3.0399 (0.673 sec/step)\n",
            "I0915 14:01:15.608180 139848901543808 learning.py:507] global step 1455: loss = 3.0399 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1456: loss = 3.0401 (0.631 sec/step)\n",
            "I0915 14:01:16.241042 139848901543808 learning.py:507] global step 1456: loss = 3.0401 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1457: loss = 3.1955 (0.641 sec/step)\n",
            "I0915 14:01:16.884267 139848901543808 learning.py:507] global step 1457: loss = 3.1955 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1458: loss = 3.1274 (0.634 sec/step)\n",
            "I0915 14:01:17.519547 139848901543808 learning.py:507] global step 1458: loss = 3.1274 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1459: loss = 1.6832 (0.660 sec/step)\n",
            "I0915 14:01:18.180938 139848901543808 learning.py:507] global step 1459: loss = 1.6832 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1460: loss = 2.8169 (0.640 sec/step)\n",
            "I0915 14:01:18.823193 139848901543808 learning.py:507] global step 1460: loss = 2.8169 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1461: loss = 2.2895 (0.663 sec/step)\n",
            "I0915 14:01:19.488082 139848901543808 learning.py:507] global step 1461: loss = 2.2895 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1462: loss = 2.0848 (0.623 sec/step)\n",
            "I0915 14:01:20.113068 139848901543808 learning.py:507] global step 1462: loss = 2.0848 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1463: loss = 2.3959 (0.621 sec/step)\n",
            "I0915 14:01:20.736293 139848901543808 learning.py:507] global step 1463: loss = 2.3959 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1464: loss = 3.3183 (0.633 sec/step)\n",
            "I0915 14:01:21.370909 139848901543808 learning.py:507] global step 1464: loss = 3.3183 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1465: loss = 2.1052 (0.621 sec/step)\n",
            "I0915 14:01:21.993709 139848901543808 learning.py:507] global step 1465: loss = 2.1052 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1466: loss = 1.5633 (0.623 sec/step)\n",
            "I0915 14:01:22.618745 139848901543808 learning.py:507] global step 1466: loss = 1.5633 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1467: loss = 2.1358 (1.082 sec/step)\n",
            "I0915 14:01:23.702767 139848901543808 learning.py:507] global step 1467: loss = 2.1358 (1.082 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1467.\n",
            "I0915 14:01:23.902806 139845834331904 supervisor.py:1050] Recording summary at step 1467.\n",
            "INFO:tensorflow:global step 1468: loss = 2.3751 (0.683 sec/step)\n",
            "I0915 14:01:24.387416 139848901543808 learning.py:507] global step 1468: loss = 2.3751 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1469: loss = 3.1826 (0.647 sec/step)\n",
            "I0915 14:01:25.036292 139848901543808 learning.py:507] global step 1469: loss = 3.1826 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1470: loss = 1.5991 (0.647 sec/step)\n",
            "I0915 14:01:25.685448 139848901543808 learning.py:507] global step 1470: loss = 1.5991 (0.647 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.55181\n",
            "I0915 14:01:25.872418 139845842724608 supervisor.py:1099] global_step/sec: 1.55181\n",
            "INFO:tensorflow:global step 1471: loss = 2.0233 (0.664 sec/step)\n",
            "I0915 14:01:26.351876 139848901543808 learning.py:507] global step 1471: loss = 2.0233 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1472: loss = 2.2781 (0.624 sec/step)\n",
            "I0915 14:01:26.977833 139848901543808 learning.py:507] global step 1472: loss = 2.2781 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1473: loss = 2.8947 (0.633 sec/step)\n",
            "I0915 14:01:27.612226 139848901543808 learning.py:507] global step 1473: loss = 2.8947 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1474: loss = 2.5828 (0.629 sec/step)\n",
            "I0915 14:01:28.243409 139848901543808 learning.py:507] global step 1474: loss = 2.5828 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1475: loss = 2.0199 (0.668 sec/step)\n",
            "I0915 14:01:28.912943 139848901543808 learning.py:507] global step 1475: loss = 2.0199 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1476: loss = 2.5656 (0.665 sec/step)\n",
            "I0915 14:01:29.579659 139848901543808 learning.py:507] global step 1476: loss = 2.5656 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1477: loss = 3.3272 (0.622 sec/step)\n",
            "I0915 14:01:30.203798 139848901543808 learning.py:507] global step 1477: loss = 3.3272 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1478: loss = 3.1955 (0.640 sec/step)\n",
            "I0915 14:01:30.845418 139848901543808 learning.py:507] global step 1478: loss = 3.1955 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1479: loss = 3.4393 (0.650 sec/step)\n",
            "I0915 14:01:31.497510 139848901543808 learning.py:507] global step 1479: loss = 3.4393 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1480: loss = 2.4808 (0.641 sec/step)\n",
            "I0915 14:01:32.140712 139848901543808 learning.py:507] global step 1480: loss = 2.4808 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1481: loss = 2.3806 (0.641 sec/step)\n",
            "I0915 14:01:32.783527 139848901543808 learning.py:507] global step 1481: loss = 2.3806 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1482: loss = 1.9926 (0.638 sec/step)\n",
            "I0915 14:01:33.423450 139848901543808 learning.py:507] global step 1482: loss = 1.9926 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1483: loss = 1.7411 (0.628 sec/step)\n",
            "I0915 14:01:34.053436 139848901543808 learning.py:507] global step 1483: loss = 1.7411 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1484: loss = 1.1536 (0.617 sec/step)\n",
            "I0915 14:01:34.671804 139848901543808 learning.py:507] global step 1484: loss = 1.1536 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1485: loss = 1.5701 (0.648 sec/step)\n",
            "I0915 14:01:35.321756 139848901543808 learning.py:507] global step 1485: loss = 1.5701 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1486: loss = 2.3064 (0.641 sec/step)\n",
            "I0915 14:01:35.965066 139848901543808 learning.py:507] global step 1486: loss = 2.3064 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1487: loss = 2.0149 (0.669 sec/step)\n",
            "I0915 14:01:36.636465 139848901543808 learning.py:507] global step 1487: loss = 2.0149 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1488: loss = 2.4583 (0.647 sec/step)\n",
            "I0915 14:01:37.285019 139848901543808 learning.py:507] global step 1488: loss = 2.4583 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1489: loss = 3.1092 (0.638 sec/step)\n",
            "I0915 14:01:37.925642 139848901543808 learning.py:507] global step 1489: loss = 3.1092 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1490: loss = 2.5906 (0.628 sec/step)\n",
            "I0915 14:01:38.555817 139848901543808 learning.py:507] global step 1490: loss = 2.5906 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1491: loss = 3.2909 (0.645 sec/step)\n",
            "I0915 14:01:39.202925 139848901543808 learning.py:507] global step 1491: loss = 3.2909 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1492: loss = 3.1023 (0.637 sec/step)\n",
            "I0915 14:01:39.842172 139848901543808 learning.py:507] global step 1492: loss = 3.1023 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1493: loss = 1.8976 (0.619 sec/step)\n",
            "I0915 14:01:40.463374 139848901543808 learning.py:507] global step 1493: loss = 1.8976 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1494: loss = 2.1057 (0.637 sec/step)\n",
            "I0915 14:01:41.102326 139848901543808 learning.py:507] global step 1494: loss = 2.1057 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1495: loss = 2.7516 (0.623 sec/step)\n",
            "I0915 14:01:41.726932 139848901543808 learning.py:507] global step 1495: loss = 2.7516 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1496: loss = 2.2773 (0.635 sec/step)\n",
            "I0915 14:01:42.363516 139848901543808 learning.py:507] global step 1496: loss = 2.2773 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1497: loss = 2.3876 (0.622 sec/step)\n",
            "I0915 14:01:42.987727 139848901543808 learning.py:507] global step 1497: loss = 2.3876 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1498: loss = 3.0401 (0.652 sec/step)\n",
            "I0915 14:01:43.641036 139848901543808 learning.py:507] global step 1498: loss = 3.0401 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1499: loss = 2.4211 (0.661 sec/step)\n",
            "I0915 14:01:44.304266 139848901543808 learning.py:507] global step 1499: loss = 2.4211 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1500: loss = 2.2852 (0.665 sec/step)\n",
            "I0915 14:01:44.971124 139848901543808 learning.py:507] global step 1500: loss = 2.2852 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1501: loss = 3.1989 (0.623 sec/step)\n",
            "I0915 14:01:45.596342 139848901543808 learning.py:507] global step 1501: loss = 3.1989 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1502: loss = 3.4700 (0.643 sec/step)\n",
            "I0915 14:01:46.241838 139848901543808 learning.py:507] global step 1502: loss = 3.4700 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1503: loss = 2.4998 (0.675 sec/step)\n",
            "I0915 14:01:46.919063 139848901543808 learning.py:507] global step 1503: loss = 2.4998 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1504: loss = 3.3963 (0.658 sec/step)\n",
            "I0915 14:01:47.579240 139848901543808 learning.py:507] global step 1504: loss = 3.3963 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1505: loss = 1.6885 (0.645 sec/step)\n",
            "I0915 14:01:48.226440 139848901543808 learning.py:507] global step 1505: loss = 1.6885 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1506: loss = 1.7425 (0.660 sec/step)\n",
            "I0915 14:01:48.888808 139848901543808 learning.py:507] global step 1506: loss = 1.7425 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1507: loss = 2.1888 (0.642 sec/step)\n",
            "I0915 14:01:49.533498 139848901543808 learning.py:507] global step 1507: loss = 2.1888 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1508: loss = 1.8987 (0.630 sec/step)\n",
            "I0915 14:01:50.165470 139848901543808 learning.py:507] global step 1508: loss = 1.8987 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1509: loss = 1.8688 (0.655 sec/step)\n",
            "I0915 14:01:50.823090 139848901543808 learning.py:507] global step 1509: loss = 1.8688 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1510: loss = 2.2727 (0.633 sec/step)\n",
            "I0915 14:01:51.458643 139848901543808 learning.py:507] global step 1510: loss = 2.2727 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1511: loss = 2.7144 (0.629 sec/step)\n",
            "I0915 14:01:52.090170 139848901543808 learning.py:507] global step 1511: loss = 2.7144 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1512: loss = 2.4090 (0.646 sec/step)\n",
            "I0915 14:01:52.737708 139848901543808 learning.py:507] global step 1512: loss = 2.4090 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1513: loss = 2.0273 (0.631 sec/step)\n",
            "I0915 14:01:53.370548 139848901543808 learning.py:507] global step 1513: loss = 2.0273 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1514: loss = 2.0102 (0.645 sec/step)\n",
            "I0915 14:01:54.017841 139848901543808 learning.py:507] global step 1514: loss = 2.0102 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1515: loss = 2.9763 (0.643 sec/step)\n",
            "I0915 14:01:54.663140 139848901543808 learning.py:507] global step 1515: loss = 2.9763 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1516: loss = 2.4458 (0.655 sec/step)\n",
            "I0915 14:01:55.320556 139848901543808 learning.py:507] global step 1516: loss = 2.4458 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1517: loss = 2.7357 (0.641 sec/step)\n",
            "I0915 14:01:55.963460 139848901543808 learning.py:507] global step 1517: loss = 2.7357 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1518: loss = 1.3017 (0.644 sec/step)\n",
            "I0915 14:01:56.609308 139848901543808 learning.py:507] global step 1518: loss = 1.3017 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1519: loss = 2.5525 (0.656 sec/step)\n",
            "I0915 14:01:57.267831 139848901543808 learning.py:507] global step 1519: loss = 2.5525 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1520: loss = 2.6178 (0.633 sec/step)\n",
            "I0915 14:01:57.903177 139848901543808 learning.py:507] global step 1520: loss = 2.6178 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1521: loss = 1.6334 (0.656 sec/step)\n",
            "I0915 14:01:58.560553 139848901543808 learning.py:507] global step 1521: loss = 1.6334 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1522: loss = 3.0764 (0.629 sec/step)\n",
            "I0915 14:01:59.192206 139848901543808 learning.py:507] global step 1522: loss = 3.0764 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1523: loss = 2.0672 (0.643 sec/step)\n",
            "I0915 14:01:59.838951 139848901543808 learning.py:507] global step 1523: loss = 2.0672 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1524: loss = 2.7413 (0.628 sec/step)\n",
            "I0915 14:02:00.468654 139848901543808 learning.py:507] global step 1524: loss = 2.7413 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1525: loss = 2.4675 (0.620 sec/step)\n",
            "I0915 14:02:01.089864 139848901543808 learning.py:507] global step 1525: loss = 2.4675 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1526: loss = 2.0822 (0.649 sec/step)\n",
            "I0915 14:02:01.740783 139848901543808 learning.py:507] global step 1526: loss = 2.0822 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1527: loss = 1.2534 (0.645 sec/step)\n",
            "I0915 14:02:02.387327 139848901543808 learning.py:507] global step 1527: loss = 1.2534 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1528: loss = 3.0568 (0.621 sec/step)\n",
            "I0915 14:02:03.010561 139848901543808 learning.py:507] global step 1528: loss = 3.0568 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1529: loss = 2.0927 (0.651 sec/step)\n",
            "I0915 14:02:03.663492 139848901543808 learning.py:507] global step 1529: loss = 2.0927 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1530: loss = 1.9211 (0.622 sec/step)\n",
            "I0915 14:02:04.287494 139848901543808 learning.py:507] global step 1530: loss = 1.9211 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1531: loss = 3.1863 (0.664 sec/step)\n",
            "I0915 14:02:04.953228 139848901543808 learning.py:507] global step 1531: loss = 3.1863 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1532: loss = 2.3349 (0.639 sec/step)\n",
            "I0915 14:02:05.594671 139848901543808 learning.py:507] global step 1532: loss = 2.3349 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1533: loss = 2.2794 (0.628 sec/step)\n",
            "I0915 14:02:06.224933 139848901543808 learning.py:507] global step 1533: loss = 2.2794 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1534: loss = 2.2032 (0.648 sec/step)\n",
            "I0915 14:02:06.875434 139848901543808 learning.py:507] global step 1534: loss = 2.2032 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1535: loss = 2.0480 (0.672 sec/step)\n",
            "I0915 14:02:07.549294 139848901543808 learning.py:507] global step 1535: loss = 2.0480 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1536: loss = 2.2924 (0.652 sec/step)\n",
            "I0915 14:02:08.203768 139848901543808 learning.py:507] global step 1536: loss = 2.2924 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1537: loss = 2.0342 (0.639 sec/step)\n",
            "I0915 14:02:08.845263 139848901543808 learning.py:507] global step 1537: loss = 2.0342 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1538: loss = 2.3880 (0.651 sec/step)\n",
            "I0915 14:02:09.498388 139848901543808 learning.py:507] global step 1538: loss = 2.3880 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1539: loss = 2.5881 (0.645 sec/step)\n",
            "I0915 14:02:10.145622 139848901543808 learning.py:507] global step 1539: loss = 2.5881 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1540: loss = 2.5446 (0.633 sec/step)\n",
            "I0915 14:02:10.780807 139848901543808 learning.py:507] global step 1540: loss = 2.5446 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1541: loss = 2.1197 (0.641 sec/step)\n",
            "I0915 14:02:11.423821 139848901543808 learning.py:507] global step 1541: loss = 2.1197 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1542: loss = 2.1858 (0.659 sec/step)\n",
            "I0915 14:02:12.085446 139848901543808 learning.py:507] global step 1542: loss = 2.1858 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1543: loss = 2.9999 (0.624 sec/step)\n",
            "I0915 14:02:12.711991 139848901543808 learning.py:507] global step 1543: loss = 2.9999 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1544: loss = 2.5565 (0.622 sec/step)\n",
            "I0915 14:02:13.335430 139848901543808 learning.py:507] global step 1544: loss = 2.5565 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1545: loss = 1.7604 (0.653 sec/step)\n",
            "I0915 14:02:13.990701 139848901543808 learning.py:507] global step 1545: loss = 1.7604 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1546: loss = 1.9089 (0.618 sec/step)\n",
            "I0915 14:02:14.611652 139848901543808 learning.py:507] global step 1546: loss = 1.9089 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1547: loss = 2.3699 (0.642 sec/step)\n",
            "I0915 14:02:15.255668 139848901543808 learning.py:507] global step 1547: loss = 2.3699 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1548: loss = 1.6910 (0.635 sec/step)\n",
            "I0915 14:02:15.892825 139848901543808 learning.py:507] global step 1548: loss = 1.6910 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1549: loss = 2.9938 (0.652 sec/step)\n",
            "I0915 14:02:16.546746 139848901543808 learning.py:507] global step 1549: loss = 2.9938 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1550: loss = 2.1673 (0.637 sec/step)\n",
            "I0915 14:02:17.185195 139848901543808 learning.py:507] global step 1550: loss = 2.1673 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1551: loss = 2.1061 (0.654 sec/step)\n",
            "I0915 14:02:17.840932 139848901543808 learning.py:507] global step 1551: loss = 2.1061 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1552: loss = 1.8072 (0.665 sec/step)\n",
            "I0915 14:02:18.508015 139848901543808 learning.py:507] global step 1552: loss = 1.8072 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1553: loss = 3.1836 (0.639 sec/step)\n",
            "I0915 14:02:19.148818 139848901543808 learning.py:507] global step 1553: loss = 3.1836 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1554: loss = 1.7152 (0.628 sec/step)\n",
            "I0915 14:02:19.779447 139848901543808 learning.py:507] global step 1554: loss = 1.7152 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1555: loss = 2.1597 (0.645 sec/step)\n",
            "I0915 14:02:20.426657 139848901543808 learning.py:507] global step 1555: loss = 2.1597 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1556: loss = 2.5848 (0.645 sec/step)\n",
            "I0915 14:02:21.073765 139848901543808 learning.py:507] global step 1556: loss = 2.5848 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1557: loss = 1.9449 (0.641 sec/step)\n",
            "I0915 14:02:21.716734 139848901543808 learning.py:507] global step 1557: loss = 1.9449 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1558: loss = 1.7805 (0.646 sec/step)\n",
            "I0915 14:02:22.364437 139848901543808 learning.py:507] global step 1558: loss = 1.7805 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1559: loss = 2.3816 (0.645 sec/step)\n",
            "I0915 14:02:23.012061 139848901543808 learning.py:507] global step 1559: loss = 2.3816 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1560: loss = 1.5977 (0.658 sec/step)\n",
            "I0915 14:02:23.672473 139848901543808 learning.py:507] global step 1560: loss = 1.5977 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1561: loss = 1.9191 (0.644 sec/step)\n",
            "I0915 14:02:24.318804 139848901543808 learning.py:507] global step 1561: loss = 1.9191 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1562: loss = 2.1169 (0.640 sec/step)\n",
            "I0915 14:02:24.960428 139848901543808 learning.py:507] global step 1562: loss = 2.1169 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1563: loss = 2.1215 (0.637 sec/step)\n",
            "I0915 14:02:25.599142 139848901543808 learning.py:507] global step 1563: loss = 2.1215 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1564: loss = 2.1137 (0.647 sec/step)\n",
            "I0915 14:02:26.248918 139848901543808 learning.py:507] global step 1564: loss = 2.1137 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1565: loss = 2.9170 (0.658 sec/step)\n",
            "I0915 14:02:26.909550 139848901543808 learning.py:507] global step 1565: loss = 2.9170 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1566: loss = 2.3427 (0.637 sec/step)\n",
            "I0915 14:02:27.548470 139848901543808 learning.py:507] global step 1566: loss = 2.3427 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1567: loss = 2.0196 (0.664 sec/step)\n",
            "I0915 14:02:28.214514 139848901543808 learning.py:507] global step 1567: loss = 2.0196 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1568: loss = 3.1638 (0.655 sec/step)\n",
            "I0915 14:02:28.871859 139848901543808 learning.py:507] global step 1568: loss = 3.1638 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1569: loss = 1.5929 (0.655 sec/step)\n",
            "I0915 14:02:29.528903 139848901543808 learning.py:507] global step 1569: loss = 1.5929 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1570: loss = 1.4001 (0.640 sec/step)\n",
            "I0915 14:02:30.171423 139848901543808 learning.py:507] global step 1570: loss = 1.4001 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1571: loss = 1.8023 (0.649 sec/step)\n",
            "I0915 14:02:30.822530 139848901543808 learning.py:507] global step 1571: loss = 1.8023 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1572: loss = 2.1194 (0.633 sec/step)\n",
            "I0915 14:02:31.456816 139848901543808 learning.py:507] global step 1572: loss = 2.1194 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1573: loss = 1.1866 (0.646 sec/step)\n",
            "I0915 14:02:32.104922 139848901543808 learning.py:507] global step 1573: loss = 1.1866 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1574: loss = 2.1111 (0.642 sec/step)\n",
            "I0915 14:02:32.748872 139848901543808 learning.py:507] global step 1574: loss = 2.1111 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1575: loss = 2.4113 (0.628 sec/step)\n",
            "I0915 14:02:33.378943 139848901543808 learning.py:507] global step 1575: loss = 2.4113 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1576: loss = 2.8426 (0.647 sec/step)\n",
            "I0915 14:02:34.029195 139848901543808 learning.py:507] global step 1576: loss = 2.8426 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1577: loss = 2.1306 (0.635 sec/step)\n",
            "I0915 14:02:34.666124 139848901543808 learning.py:507] global step 1577: loss = 2.1306 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1578: loss = 1.9370 (0.639 sec/step)\n",
            "I0915 14:02:35.306790 139848901543808 learning.py:507] global step 1578: loss = 1.9370 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1579: loss = 2.2286 (0.649 sec/step)\n",
            "I0915 14:02:35.958474 139848901543808 learning.py:507] global step 1579: loss = 2.2286 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1580: loss = 2.9168 (0.634 sec/step)\n",
            "I0915 14:02:36.594027 139848901543808 learning.py:507] global step 1580: loss = 2.9168 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1581: loss = 1.6182 (0.642 sec/step)\n",
            "I0915 14:02:37.238152 139848901543808 learning.py:507] global step 1581: loss = 1.6182 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1582: loss = 1.6553 (0.644 sec/step)\n",
            "I0915 14:02:37.884416 139848901543808 learning.py:507] global step 1582: loss = 1.6553 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1583: loss = 2.5575 (0.682 sec/step)\n",
            "I0915 14:02:38.568078 139848901543808 learning.py:507] global step 1583: loss = 2.5575 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1584: loss = 1.4428 (0.625 sec/step)\n",
            "I0915 14:02:39.195383 139848901543808 learning.py:507] global step 1584: loss = 1.4428 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1585: loss = 1.2550 (0.622 sec/step)\n",
            "I0915 14:02:39.819715 139848901543808 learning.py:507] global step 1585: loss = 1.2550 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1586: loss = 1.8371 (0.638 sec/step)\n",
            "I0915 14:02:40.459590 139848901543808 learning.py:507] global step 1586: loss = 1.8371 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1587: loss = 1.5749 (0.619 sec/step)\n",
            "I0915 14:02:41.080938 139848901543808 learning.py:507] global step 1587: loss = 1.5749 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1588: loss = 1.9063 (0.668 sec/step)\n",
            "I0915 14:02:41.751320 139848901543808 learning.py:507] global step 1588: loss = 1.9063 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1589: loss = 2.0327 (0.642 sec/step)\n",
            "I0915 14:02:42.395220 139848901543808 learning.py:507] global step 1589: loss = 2.0327 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1590: loss = 2.6073 (0.636 sec/step)\n",
            "I0915 14:02:43.032754 139848901543808 learning.py:507] global step 1590: loss = 2.6073 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1591: loss = 2.5889 (0.644 sec/step)\n",
            "I0915 14:02:43.679131 139848901543808 learning.py:507] global step 1591: loss = 2.5889 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1592: loss = 1.7139 (0.648 sec/step)\n",
            "I0915 14:02:44.329159 139848901543808 learning.py:507] global step 1592: loss = 1.7139 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1593: loss = 2.3966 (0.624 sec/step)\n",
            "I0915 14:02:44.955450 139848901543808 learning.py:507] global step 1593: loss = 2.3966 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1594: loss = 3.1623 (0.624 sec/step)\n",
            "I0915 14:02:45.582005 139848901543808 learning.py:507] global step 1594: loss = 3.1623 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1595: loss = 2.0613 (0.627 sec/step)\n",
            "I0915 14:02:46.210409 139848901543808 learning.py:507] global step 1595: loss = 2.0613 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1596: loss = 1.1480 (0.630 sec/step)\n",
            "I0915 14:02:46.842827 139848901543808 learning.py:507] global step 1596: loss = 1.1480 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1597: loss = 3.1859 (0.652 sec/step)\n",
            "I0915 14:02:47.501394 139848901543808 learning.py:507] global step 1597: loss = 3.1859 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1598: loss = 1.8830 (0.634 sec/step)\n",
            "I0915 14:02:48.137586 139848901543808 learning.py:507] global step 1598: loss = 1.8830 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1599: loss = 2.6693 (0.657 sec/step)\n",
            "I0915 14:02:48.796465 139848901543808 learning.py:507] global step 1599: loss = 2.6693 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1600: loss = 2.1998 (0.663 sec/step)\n",
            "I0915 14:02:49.461832 139848901543808 learning.py:507] global step 1600: loss = 2.1998 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1601: loss = 2.4411 (0.652 sec/step)\n",
            "I0915 14:02:50.115942 139848901543808 learning.py:507] global step 1601: loss = 2.4411 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1602: loss = 2.3186 (0.650 sec/step)\n",
            "I0915 14:02:50.767325 139848901543808 learning.py:507] global step 1602: loss = 2.3186 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1603: loss = 4.2549 (0.638 sec/step)\n",
            "I0915 14:02:51.407044 139848901543808 learning.py:507] global step 1603: loss = 4.2549 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1604: loss = 2.1788 (0.629 sec/step)\n",
            "I0915 14:02:52.038541 139848901543808 learning.py:507] global step 1604: loss = 2.1788 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1605: loss = 1.8467 (0.637 sec/step)\n",
            "I0915 14:02:52.677606 139848901543808 learning.py:507] global step 1605: loss = 1.8467 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1606: loss = 1.9565 (0.636 sec/step)\n",
            "I0915 14:02:53.315881 139848901543808 learning.py:507] global step 1606: loss = 1.9565 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1607: loss = 1.8257 (0.650 sec/step)\n",
            "I0915 14:02:53.967838 139848901543808 learning.py:507] global step 1607: loss = 1.8257 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1608: loss = 2.3880 (0.657 sec/step)\n",
            "I0915 14:02:54.626867 139848901543808 learning.py:507] global step 1608: loss = 2.3880 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1609: loss = 1.7257 (0.663 sec/step)\n",
            "I0915 14:02:55.292208 139848901543808 learning.py:507] global step 1609: loss = 1.7257 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1610: loss = 3.0513 (0.638 sec/step)\n",
            "I0915 14:02:55.933464 139848901543808 learning.py:507] global step 1610: loss = 3.0513 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1611: loss = 2.2366 (0.645 sec/step)\n",
            "I0915 14:02:56.580729 139848901543808 learning.py:507] global step 1611: loss = 2.2366 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1612: loss = 2.1507 (0.660 sec/step)\n",
            "I0915 14:02:57.242864 139848901543808 learning.py:507] global step 1612: loss = 2.1507 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1613: loss = 1.4276 (0.638 sec/step)\n",
            "I0915 14:02:57.882959 139848901543808 learning.py:507] global step 1613: loss = 1.4276 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1614: loss = 1.9322 (0.648 sec/step)\n",
            "I0915 14:02:58.532482 139848901543808 learning.py:507] global step 1614: loss = 1.9322 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1615: loss = 1.4333 (0.652 sec/step)\n",
            "I0915 14:02:59.186229 139848901543808 learning.py:507] global step 1615: loss = 1.4333 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1616: loss = 2.2323 (0.630 sec/step)\n",
            "I0915 14:02:59.818374 139848901543808 learning.py:507] global step 1616: loss = 2.2323 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1617: loss = 2.5733 (0.652 sec/step)\n",
            "I0915 14:03:00.472745 139848901543808 learning.py:507] global step 1617: loss = 2.5733 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1618: loss = 2.1809 (0.643 sec/step)\n",
            "I0915 14:03:01.117442 139848901543808 learning.py:507] global step 1618: loss = 2.1809 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1619: loss = 1.7449 (0.639 sec/step)\n",
            "I0915 14:03:01.757814 139848901543808 learning.py:507] global step 1619: loss = 1.7449 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1620: loss = 2.6921 (0.634 sec/step)\n",
            "I0915 14:03:02.394426 139848901543808 learning.py:507] global step 1620: loss = 2.6921 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1621: loss = 1.7751 (0.629 sec/step)\n",
            "I0915 14:03:03.025310 139848901543808 learning.py:507] global step 1621: loss = 1.7751 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1622: loss = 2.4665 (0.631 sec/step)\n",
            "I0915 14:03:03.658184 139848901543808 learning.py:507] global step 1622: loss = 2.4665 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1623: loss = 2.7522 (0.643 sec/step)\n",
            "I0915 14:03:04.302907 139848901543808 learning.py:507] global step 1623: loss = 2.7522 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1624: loss = 1.6937 (0.631 sec/step)\n",
            "I0915 14:03:04.936299 139848901543808 learning.py:507] global step 1624: loss = 1.6937 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1625: loss = 2.2580 (0.642 sec/step)\n",
            "I0915 14:03:05.580038 139848901543808 learning.py:507] global step 1625: loss = 2.2580 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1626: loss = 2.0629 (0.633 sec/step)\n",
            "I0915 14:03:06.215479 139848901543808 learning.py:507] global step 1626: loss = 2.0629 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1627: loss = 1.9681 (0.652 sec/step)\n",
            "I0915 14:03:06.869389 139848901543808 learning.py:507] global step 1627: loss = 1.9681 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1628: loss = 1.6625 (0.644 sec/step)\n",
            "I0915 14:03:07.515605 139848901543808 learning.py:507] global step 1628: loss = 1.6625 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1629: loss = 2.3207 (0.633 sec/step)\n",
            "I0915 14:03:08.150243 139848901543808 learning.py:507] global step 1629: loss = 2.3207 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1630: loss = 4.2803 (0.663 sec/step)\n",
            "I0915 14:03:08.815586 139848901543808 learning.py:507] global step 1630: loss = 4.2803 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1631: loss = 1.7004 (0.692 sec/step)\n",
            "I0915 14:03:09.509643 139848901543808 learning.py:507] global step 1631: loss = 1.7004 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1632: loss = 1.8985 (0.660 sec/step)\n",
            "I0915 14:03:10.171501 139848901543808 learning.py:507] global step 1632: loss = 1.8985 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1633: loss = 1.4582 (0.634 sec/step)\n",
            "I0915 14:03:10.807433 139848901543808 learning.py:507] global step 1633: loss = 1.4582 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1634: loss = 2.4736 (0.638 sec/step)\n",
            "I0915 14:03:11.447492 139848901543808 learning.py:507] global step 1634: loss = 2.4736 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1635: loss = 3.2909 (0.642 sec/step)\n",
            "I0915 14:03:12.092534 139848901543808 learning.py:507] global step 1635: loss = 3.2909 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1636: loss = 2.2630 (0.631 sec/step)\n",
            "I0915 14:03:12.725836 139848901543808 learning.py:507] global step 1636: loss = 2.2630 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1637: loss = 2.0801 (0.653 sec/step)\n",
            "I0915 14:03:13.381022 139848901543808 learning.py:507] global step 1637: loss = 2.0801 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1638: loss = 1.9578 (0.627 sec/step)\n",
            "I0915 14:03:14.009888 139848901543808 learning.py:507] global step 1638: loss = 1.9578 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1639: loss = 2.2182 (0.630 sec/step)\n",
            "I0915 14:03:14.642048 139848901543808 learning.py:507] global step 1639: loss = 2.2182 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1640: loss = 2.6311 (0.619 sec/step)\n",
            "I0915 14:03:15.263581 139848901543808 learning.py:507] global step 1640: loss = 2.6311 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1641: loss = 2.6490 (0.642 sec/step)\n",
            "I0915 14:03:15.907566 139848901543808 learning.py:507] global step 1641: loss = 2.6490 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1642: loss = 2.1966 (0.639 sec/step)\n",
            "I0915 14:03:16.548694 139848901543808 learning.py:507] global step 1642: loss = 2.1966 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1643: loss = 2.0061 (0.660 sec/step)\n",
            "I0915 14:03:17.210638 139848901543808 learning.py:507] global step 1643: loss = 2.0061 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1644: loss = 1.9521 (0.656 sec/step)\n",
            "I0915 14:03:17.869685 139848901543808 learning.py:507] global step 1644: loss = 1.9521 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1645: loss = 2.7389 (0.638 sec/step)\n",
            "I0915 14:03:18.509916 139848901543808 learning.py:507] global step 1645: loss = 2.7389 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1646: loss = 2.2285 (0.632 sec/step)\n",
            "I0915 14:03:19.144286 139848901543808 learning.py:507] global step 1646: loss = 2.2285 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1647: loss = 1.8534 (0.653 sec/step)\n",
            "I0915 14:03:19.799211 139848901543808 learning.py:507] global step 1647: loss = 1.8534 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1648: loss = 1.6305 (0.649 sec/step)\n",
            "I0915 14:03:20.452028 139848901543808 learning.py:507] global step 1648: loss = 1.6305 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1649: loss = 2.8143 (0.633 sec/step)\n",
            "I0915 14:03:21.087547 139848901543808 learning.py:507] global step 1649: loss = 2.8143 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1650: loss = 1.9831 (0.640 sec/step)\n",
            "I0915 14:03:21.728854 139848901543808 learning.py:507] global step 1650: loss = 1.9831 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1651: loss = 2.8780 (0.619 sec/step)\n",
            "I0915 14:03:22.349806 139848901543808 learning.py:507] global step 1651: loss = 2.8780 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1652: loss = 2.7744 (0.635 sec/step)\n",
            "I0915 14:03:22.987015 139848901543808 learning.py:507] global step 1652: loss = 2.7744 (0.635 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1652.\n",
            "I0915 14:03:23.940223 139845834331904 supervisor.py:1050] Recording summary at step 1652.\n",
            "INFO:tensorflow:global step 1653: loss = 2.5775 (1.154 sec/step)\n",
            "I0915 14:03:24.143407 139848901543808 learning.py:507] global step 1653: loss = 2.5775 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 1654: loss = 2.6175 (0.648 sec/step)\n",
            "I0915 14:03:24.793857 139848901543808 learning.py:507] global step 1654: loss = 2.6175 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1655: loss = 3.0376 (0.626 sec/step)\n",
            "I0915 14:03:25.421777 139848901543808 learning.py:507] global step 1655: loss = 3.0376 (0.626 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 1.54201\n",
            "I0915 14:03:25.845926 139845842724608 supervisor.py:1099] global_step/sec: 1.54201\n",
            "INFO:tensorflow:global step 1656: loss = 1.6188 (0.642 sec/step)\n",
            "I0915 14:03:26.065756 139848901543808 learning.py:507] global step 1656: loss = 1.6188 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1657: loss = 2.3576 (0.633 sec/step)\n",
            "I0915 14:03:26.700485 139848901543808 learning.py:507] global step 1657: loss = 2.3576 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1658: loss = 1.9786 (0.644 sec/step)\n",
            "I0915 14:03:27.346486 139848901543808 learning.py:507] global step 1658: loss = 1.9786 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1659: loss = 1.8718 (0.632 sec/step)\n",
            "I0915 14:03:27.980519 139848901543808 learning.py:507] global step 1659: loss = 1.8718 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1660: loss = 2.2703 (0.624 sec/step)\n",
            "I0915 14:03:28.606154 139848901543808 learning.py:507] global step 1660: loss = 2.2703 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1661: loss = 2.6292 (0.644 sec/step)\n",
            "I0915 14:03:29.252542 139848901543808 learning.py:507] global step 1661: loss = 2.6292 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1662: loss = 1.7939 (0.645 sec/step)\n",
            "I0915 14:03:29.899751 139848901543808 learning.py:507] global step 1662: loss = 1.7939 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1663: loss = 2.4744 (0.631 sec/step)\n",
            "I0915 14:03:30.532801 139848901543808 learning.py:507] global step 1663: loss = 2.4744 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1664: loss = 2.7077 (0.642 sec/step)\n",
            "I0915 14:03:31.176733 139848901543808 learning.py:507] global step 1664: loss = 2.7077 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1665: loss = 1.9029 (0.627 sec/step)\n",
            "I0915 14:03:31.805333 139848901543808 learning.py:507] global step 1665: loss = 1.9029 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1666: loss = 3.1353 (0.662 sec/step)\n",
            "I0915 14:03:32.468529 139848901543808 learning.py:507] global step 1666: loss = 3.1353 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1667: loss = 3.0458 (0.622 sec/step)\n",
            "I0915 14:03:33.091888 139848901543808 learning.py:507] global step 1667: loss = 3.0458 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1668: loss = 1.9453 (0.624 sec/step)\n",
            "I0915 14:03:33.717988 139848901543808 learning.py:507] global step 1668: loss = 1.9453 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1669: loss = 2.6065 (0.669 sec/step)\n",
            "I0915 14:03:34.388701 139848901543808 learning.py:507] global step 1669: loss = 2.6065 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1670: loss = 2.7572 (0.643 sec/step)\n",
            "I0915 14:03:35.033275 139848901543808 learning.py:507] global step 1670: loss = 2.7572 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1671: loss = 1.7712 (0.650 sec/step)\n",
            "I0915 14:03:35.684636 139848901543808 learning.py:507] global step 1671: loss = 1.7712 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1672: loss = 2.8826 (0.634 sec/step)\n",
            "I0915 14:03:36.320513 139848901543808 learning.py:507] global step 1672: loss = 2.8826 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1673: loss = 2.6789 (0.633 sec/step)\n",
            "I0915 14:03:36.955698 139848901543808 learning.py:507] global step 1673: loss = 2.6789 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1674: loss = 1.9188 (0.648 sec/step)\n",
            "I0915 14:03:37.605468 139848901543808 learning.py:507] global step 1674: loss = 1.9188 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1675: loss = 2.3155 (0.657 sec/step)\n",
            "I0915 14:03:38.264160 139848901543808 learning.py:507] global step 1675: loss = 2.3155 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1676: loss = 1.8946 (0.630 sec/step)\n",
            "I0915 14:03:38.895948 139848901543808 learning.py:507] global step 1676: loss = 1.8946 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1677: loss = 2.4157 (0.633 sec/step)\n",
            "I0915 14:03:39.530687 139848901543808 learning.py:507] global step 1677: loss = 2.4157 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1678: loss = 2.9092 (0.612 sec/step)\n",
            "I0915 14:03:40.145306 139848901543808 learning.py:507] global step 1678: loss = 2.9092 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1679: loss = 1.7612 (0.641 sec/step)\n",
            "I0915 14:03:40.788252 139848901543808 learning.py:507] global step 1679: loss = 1.7612 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1680: loss = 1.4094 (0.643 sec/step)\n",
            "I0915 14:03:41.433164 139848901543808 learning.py:507] global step 1680: loss = 1.4094 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1681: loss = 1.9310 (0.615 sec/step)\n",
            "I0915 14:03:42.051461 139848901543808 learning.py:507] global step 1681: loss = 1.9310 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1682: loss = 2.9702 (0.633 sec/step)\n",
            "I0915 14:03:42.686562 139848901543808 learning.py:507] global step 1682: loss = 2.9702 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1683: loss = 2.1862 (0.619 sec/step)\n",
            "I0915 14:03:43.307749 139848901543808 learning.py:507] global step 1683: loss = 2.1862 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1684: loss = 1.6338 (0.633 sec/step)\n",
            "I0915 14:03:43.942583 139848901543808 learning.py:507] global step 1684: loss = 1.6338 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1685: loss = 2.0460 (0.639 sec/step)\n",
            "I0915 14:03:44.582819 139848901543808 learning.py:507] global step 1685: loss = 2.0460 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1686: loss = 1.8603 (0.611 sec/step)\n",
            "I0915 14:03:45.195842 139848901543808 learning.py:507] global step 1686: loss = 1.8603 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1687: loss = 2.2501 (0.633 sec/step)\n",
            "I0915 14:03:45.831506 139848901543808 learning.py:507] global step 1687: loss = 2.2501 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1688: loss = 2.3699 (0.670 sec/step)\n",
            "I0915 14:03:46.503592 139848901543808 learning.py:507] global step 1688: loss = 2.3699 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1689: loss = 2.0870 (0.639 sec/step)\n",
            "I0915 14:03:47.144234 139848901543808 learning.py:507] global step 1689: loss = 2.0870 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1690: loss = 2.9443 (0.626 sec/step)\n",
            "I0915 14:03:47.772018 139848901543808 learning.py:507] global step 1690: loss = 2.9443 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1691: loss = 1.8278 (0.623 sec/step)\n",
            "I0915 14:03:48.397760 139848901543808 learning.py:507] global step 1691: loss = 1.8278 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1692: loss = 2.8929 (0.625 sec/step)\n",
            "I0915 14:03:49.024755 139848901543808 learning.py:507] global step 1692: loss = 2.8929 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1693: loss = 2.1306 (0.630 sec/step)\n",
            "I0915 14:03:49.656134 139848901543808 learning.py:507] global step 1693: loss = 2.1306 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1694: loss = 2.3773 (0.614 sec/step)\n",
            "I0915 14:03:50.272453 139848901543808 learning.py:507] global step 1694: loss = 2.3773 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1695: loss = 2.3155 (0.651 sec/step)\n",
            "I0915 14:03:50.924843 139848901543808 learning.py:507] global step 1695: loss = 2.3155 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1696: loss = 1.3236 (0.636 sec/step)\n",
            "I0915 14:03:51.562856 139848901543808 learning.py:507] global step 1696: loss = 1.3236 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1697: loss = 1.7924 (0.628 sec/step)\n",
            "I0915 14:03:52.192268 139848901543808 learning.py:507] global step 1697: loss = 1.7924 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1698: loss = 2.5045 (0.640 sec/step)\n",
            "I0915 14:03:52.833789 139848901543808 learning.py:507] global step 1698: loss = 2.5045 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1699: loss = 1.6963 (0.616 sec/step)\n",
            "I0915 14:03:53.451637 139848901543808 learning.py:507] global step 1699: loss = 1.6963 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1700: loss = 3.5964 (0.631 sec/step)\n",
            "I0915 14:03:54.084482 139848901543808 learning.py:507] global step 1700: loss = 3.5964 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1701: loss = 2.3433 (0.652 sec/step)\n",
            "I0915 14:03:54.738019 139848901543808 learning.py:507] global step 1701: loss = 2.3433 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1702: loss = 1.9875 (0.619 sec/step)\n",
            "I0915 14:03:55.358659 139848901543808 learning.py:507] global step 1702: loss = 1.9875 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1703: loss = 1.8918 (0.651 sec/step)\n",
            "I0915 14:03:56.011580 139848901543808 learning.py:507] global step 1703: loss = 1.8918 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1704: loss = 1.8711 (0.641 sec/step)\n",
            "I0915 14:03:56.656629 139848901543808 learning.py:507] global step 1704: loss = 1.8711 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1705: loss = 1.7650 (0.621 sec/step)\n",
            "I0915 14:03:57.279468 139848901543808 learning.py:507] global step 1705: loss = 1.7650 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1706: loss = 1.5225 (0.637 sec/step)\n",
            "I0915 14:03:57.918266 139848901543808 learning.py:507] global step 1706: loss = 1.5225 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1707: loss = 2.9300 (0.629 sec/step)\n",
            "I0915 14:03:58.548983 139848901543808 learning.py:507] global step 1707: loss = 2.9300 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1708: loss = 2.0066 (0.626 sec/step)\n",
            "I0915 14:03:59.177118 139848901543808 learning.py:507] global step 1708: loss = 2.0066 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1709: loss = 2.5923 (0.634 sec/step)\n",
            "I0915 14:03:59.812695 139848901543808 learning.py:507] global step 1709: loss = 2.5923 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1710: loss = 1.5677 (0.623 sec/step)\n",
            "I0915 14:04:00.437370 139848901543808 learning.py:507] global step 1710: loss = 1.5677 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1711: loss = 2.2677 (0.648 sec/step)\n",
            "I0915 14:04:01.087445 139848901543808 learning.py:507] global step 1711: loss = 2.2677 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1712: loss = 2.7405 (0.631 sec/step)\n",
            "I0915 14:04:01.720496 139848901543808 learning.py:507] global step 1712: loss = 2.7405 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1713: loss = 3.2493 (0.626 sec/step)\n",
            "I0915 14:04:02.348629 139848901543808 learning.py:507] global step 1713: loss = 3.2493 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1714: loss = 1.6780 (0.659 sec/step)\n",
            "I0915 14:04:03.009113 139848901543808 learning.py:507] global step 1714: loss = 1.6780 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1715: loss = 1.7527 (0.648 sec/step)\n",
            "I0915 14:04:03.661458 139848901543808 learning.py:507] global step 1715: loss = 1.7527 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1716: loss = 1.6874 (0.637 sec/step)\n",
            "I0915 14:04:04.301486 139848901543808 learning.py:507] global step 1716: loss = 1.6874 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1717: loss = 3.3344 (0.617 sec/step)\n",
            "I0915 14:04:04.921092 139848901543808 learning.py:507] global step 1717: loss = 3.3344 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1718: loss = 2.4706 (0.642 sec/step)\n",
            "I0915 14:04:05.564754 139848901543808 learning.py:507] global step 1718: loss = 2.4706 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1719: loss = 2.1190 (0.630 sec/step)\n",
            "I0915 14:04:06.197059 139848901543808 learning.py:507] global step 1719: loss = 2.1190 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1720: loss = 2.1630 (0.641 sec/step)\n",
            "I0915 14:04:06.840415 139848901543808 learning.py:507] global step 1720: loss = 2.1630 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1721: loss = 2.1984 (0.630 sec/step)\n",
            "I0915 14:04:07.472403 139848901543808 learning.py:507] global step 1721: loss = 2.1984 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1722: loss = 1.8237 (0.627 sec/step)\n",
            "I0915 14:04:08.102096 139848901543808 learning.py:507] global step 1722: loss = 1.8237 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1723: loss = 2.4269 (0.631 sec/step)\n",
            "I0915 14:04:08.735374 139848901543808 learning.py:507] global step 1723: loss = 2.4269 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1724: loss = 2.3240 (0.628 sec/step)\n",
            "I0915 14:04:09.365550 139848901543808 learning.py:507] global step 1724: loss = 2.3240 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1725: loss = 2.1747 (0.626 sec/step)\n",
            "I0915 14:04:09.994265 139848901543808 learning.py:507] global step 1725: loss = 2.1747 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1726: loss = 2.7869 (0.606 sec/step)\n",
            "I0915 14:04:10.601983 139848901543808 learning.py:507] global step 1726: loss = 2.7869 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 1727: loss = 2.3437 (0.648 sec/step)\n",
            "I0915 14:04:11.252987 139848901543808 learning.py:507] global step 1727: loss = 2.3437 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1728: loss = 1.8268 (0.644 sec/step)\n",
            "I0915 14:04:11.899407 139848901543808 learning.py:507] global step 1728: loss = 1.8268 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1729: loss = 1.7397 (0.643 sec/step)\n",
            "I0915 14:04:12.544863 139848901543808 learning.py:507] global step 1729: loss = 1.7397 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1730: loss = 2.1931 (0.632 sec/step)\n",
            "I0915 14:04:13.178863 139848901543808 learning.py:507] global step 1730: loss = 2.1931 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1731: loss = 2.0283 (0.625 sec/step)\n",
            "I0915 14:04:13.805710 139848901543808 learning.py:507] global step 1731: loss = 2.0283 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1732: loss = 1.7608 (0.633 sec/step)\n",
            "I0915 14:04:14.441095 139848901543808 learning.py:507] global step 1732: loss = 1.7608 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1733: loss = 2.6590 (0.634 sec/step)\n",
            "I0915 14:04:15.076923 139848901543808 learning.py:507] global step 1733: loss = 2.6590 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1734: loss = 2.2539 (0.597 sec/step)\n",
            "I0915 14:04:15.675884 139848901543808 learning.py:507] global step 1734: loss = 2.2539 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1735: loss = 2.0097 (0.652 sec/step)\n",
            "I0915 14:04:16.329728 139848901543808 learning.py:507] global step 1735: loss = 2.0097 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1736: loss = 2.1469 (0.615 sec/step)\n",
            "I0915 14:04:16.946901 139848901543808 learning.py:507] global step 1736: loss = 2.1469 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1737: loss = 2.6338 (0.631 sec/step)\n",
            "I0915 14:04:17.579519 139848901543808 learning.py:507] global step 1737: loss = 2.6338 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1738: loss = 2.9934 (0.619 sec/step)\n",
            "I0915 14:04:18.200672 139848901543808 learning.py:507] global step 1738: loss = 2.9934 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1739: loss = 2.4044 (0.619 sec/step)\n",
            "I0915 14:04:18.821609 139848901543808 learning.py:507] global step 1739: loss = 2.4044 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1740: loss = 2.0561 (0.632 sec/step)\n",
            "I0915 14:04:19.455873 139848901543808 learning.py:507] global step 1740: loss = 2.0561 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1741: loss = 1.8454 (0.653 sec/step)\n",
            "I0915 14:04:20.111392 139848901543808 learning.py:507] global step 1741: loss = 1.8454 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1742: loss = 2.0548 (0.614 sec/step)\n",
            "I0915 14:04:20.727876 139848901543808 learning.py:507] global step 1742: loss = 2.0548 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1743: loss = 2.0896 (0.660 sec/step)\n",
            "I0915 14:04:21.390431 139848901543808 learning.py:507] global step 1743: loss = 2.0896 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1744: loss = 1.7210 (0.646 sec/step)\n",
            "I0915 14:04:22.039078 139848901543808 learning.py:507] global step 1744: loss = 1.7210 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1745: loss = 2.7774 (0.643 sec/step)\n",
            "I0915 14:04:22.684363 139848901543808 learning.py:507] global step 1745: loss = 2.7774 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1746: loss = 2.6157 (0.641 sec/step)\n",
            "I0915 14:04:23.326813 139848901543808 learning.py:507] global step 1746: loss = 2.6157 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1747: loss = 2.6542 (0.654 sec/step)\n",
            "I0915 14:04:23.982809 139848901543808 learning.py:507] global step 1747: loss = 2.6542 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1748: loss = 1.9256 (0.628 sec/step)\n",
            "I0915 14:04:24.612853 139848901543808 learning.py:507] global step 1748: loss = 1.9256 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1749: loss = 2.9686 (0.633 sec/step)\n",
            "I0915 14:04:25.247324 139848901543808 learning.py:507] global step 1749: loss = 2.9686 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1750: loss = 1.5100 (0.645 sec/step)\n",
            "I0915 14:04:25.894562 139848901543808 learning.py:507] global step 1750: loss = 1.5100 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1751: loss = 2.4125 (0.633 sec/step)\n",
            "I0915 14:04:26.529958 139848901543808 learning.py:507] global step 1751: loss = 2.4125 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1752: loss = 1.8706 (0.634 sec/step)\n",
            "I0915 14:04:27.166338 139848901543808 learning.py:507] global step 1752: loss = 1.8706 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1753: loss = 3.1251 (0.625 sec/step)\n",
            "I0915 14:04:27.792868 139848901543808 learning.py:507] global step 1753: loss = 3.1251 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1754: loss = 1.3034 (0.624 sec/step)\n",
            "I0915 14:04:28.419240 139848901543808 learning.py:507] global step 1754: loss = 1.3034 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1755: loss = 2.7674 (0.611 sec/step)\n",
            "I0915 14:04:29.032421 139848901543808 learning.py:507] global step 1755: loss = 2.7674 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1756: loss = 1.6431 (0.640 sec/step)\n",
            "I0915 14:04:29.674795 139848901543808 learning.py:507] global step 1756: loss = 1.6431 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1757: loss = 2.0975 (0.637 sec/step)\n",
            "I0915 14:04:30.313254 139848901543808 learning.py:507] global step 1757: loss = 2.0975 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1758: loss = 1.8028 (0.641 sec/step)\n",
            "I0915 14:04:30.955883 139848901543808 learning.py:507] global step 1758: loss = 1.8028 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1759: loss = 2.6925 (0.639 sec/step)\n",
            "I0915 14:04:31.597032 139848901543808 learning.py:507] global step 1759: loss = 2.6925 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1760: loss = 2.6728 (0.663 sec/step)\n",
            "I0915 14:04:32.261873 139848901543808 learning.py:507] global step 1760: loss = 2.6728 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1761: loss = 2.5137 (0.644 sec/step)\n",
            "I0915 14:04:32.908236 139848901543808 learning.py:507] global step 1761: loss = 2.5137 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1762: loss = 2.0377 (0.618 sec/step)\n",
            "I0915 14:04:33.528200 139848901543808 learning.py:507] global step 1762: loss = 2.0377 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1763: loss = 1.7629 (0.621 sec/step)\n",
            "I0915 14:04:34.151314 139848901543808 learning.py:507] global step 1763: loss = 1.7629 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1764: loss = 1.9045 (0.633 sec/step)\n",
            "I0915 14:04:34.786901 139848901543808 learning.py:507] global step 1764: loss = 1.9045 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1765: loss = 1.3931 (0.632 sec/step)\n",
            "I0915 14:04:35.420675 139848901543808 learning.py:507] global step 1765: loss = 1.3931 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1766: loss = 2.9746 (0.640 sec/step)\n",
            "I0915 14:04:36.062055 139848901543808 learning.py:507] global step 1766: loss = 2.9746 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1767: loss = 1.9638 (0.606 sec/step)\n",
            "I0915 14:04:36.669604 139848901543808 learning.py:507] global step 1767: loss = 1.9638 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 1768: loss = 2.6486 (0.614 sec/step)\n",
            "I0915 14:04:37.285733 139848901543808 learning.py:507] global step 1768: loss = 2.6486 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1769: loss = 2.3449 (0.649 sec/step)\n",
            "I0915 14:04:37.937098 139848901543808 learning.py:507] global step 1769: loss = 2.3449 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1770: loss = 2.0710 (0.632 sec/step)\n",
            "I0915 14:04:38.571144 139848901543808 learning.py:507] global step 1770: loss = 2.0710 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1771: loss = 2.3004 (0.625 sec/step)\n",
            "I0915 14:04:39.197976 139848901543808 learning.py:507] global step 1771: loss = 2.3004 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1772: loss = 2.2937 (0.636 sec/step)\n",
            "I0915 14:04:39.835828 139848901543808 learning.py:507] global step 1772: loss = 2.2937 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1773: loss = 2.0614 (0.619 sec/step)\n",
            "I0915 14:04:40.456918 139848901543808 learning.py:507] global step 1773: loss = 2.0614 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1774: loss = 2.3844 (0.612 sec/step)\n",
            "I0915 14:04:41.070653 139848901543808 learning.py:507] global step 1774: loss = 2.3844 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1775: loss = 2.2722 (0.658 sec/step)\n",
            "I0915 14:04:41.731594 139848901543808 learning.py:507] global step 1775: loss = 2.2722 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1776: loss = 1.6431 (0.645 sec/step)\n",
            "I0915 14:04:42.378901 139848901543808 learning.py:507] global step 1776: loss = 1.6431 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1777: loss = 1.7701 (0.646 sec/step)\n",
            "I0915 14:04:43.026374 139848901543808 learning.py:507] global step 1777: loss = 1.7701 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1778: loss = 2.3435 (0.652 sec/step)\n",
            "I0915 14:04:43.680755 139848901543808 learning.py:507] global step 1778: loss = 2.3435 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1779: loss = 1.7461 (0.638 sec/step)\n",
            "I0915 14:04:44.321287 139848901543808 learning.py:507] global step 1779: loss = 1.7461 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1780: loss = 2.7984 (0.631 sec/step)\n",
            "I0915 14:04:44.953794 139848901543808 learning.py:507] global step 1780: loss = 2.7984 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1781: loss = 1.8450 (0.609 sec/step)\n",
            "I0915 14:04:45.564757 139848901543808 learning.py:507] global step 1781: loss = 1.8450 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1782: loss = 2.0323 (0.613 sec/step)\n",
            "I0915 14:04:46.179668 139848901543808 learning.py:507] global step 1782: loss = 2.0323 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 1783: loss = 1.5927 (0.640 sec/step)\n",
            "I0915 14:04:46.821761 139848901543808 learning.py:507] global step 1783: loss = 1.5927 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1784: loss = 1.5264 (0.618 sec/step)\n",
            "I0915 14:04:47.441108 139848901543808 learning.py:507] global step 1784: loss = 1.5264 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1785: loss = 1.9440 (0.631 sec/step)\n",
            "I0915 14:04:48.073983 139848901543808 learning.py:507] global step 1785: loss = 1.9440 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1786: loss = 2.5225 (0.633 sec/step)\n",
            "I0915 14:04:48.709442 139848901543808 learning.py:507] global step 1786: loss = 2.5225 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1787: loss = 2.5401 (0.630 sec/step)\n",
            "I0915 14:04:49.341954 139848901543808 learning.py:507] global step 1787: loss = 2.5401 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1788: loss = 2.0297 (0.636 sec/step)\n",
            "I0915 14:04:49.980317 139848901543808 learning.py:507] global step 1788: loss = 2.0297 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1789: loss = 1.5103 (0.640 sec/step)\n",
            "I0915 14:04:50.622860 139848901543808 learning.py:507] global step 1789: loss = 1.5103 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1790: loss = 2.6142 (0.645 sec/step)\n",
            "I0915 14:04:51.269938 139848901543808 learning.py:507] global step 1790: loss = 2.6142 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1791: loss = 1.9375 (0.616 sec/step)\n",
            "I0915 14:04:51.888089 139848901543808 learning.py:507] global step 1791: loss = 1.9375 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1792: loss = 2.4262 (0.636 sec/step)\n",
            "I0915 14:04:52.526263 139848901543808 learning.py:507] global step 1792: loss = 2.4262 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1793: loss = 2.4253 (0.632 sec/step)\n",
            "I0915 14:04:53.160508 139848901543808 learning.py:507] global step 1793: loss = 2.4253 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1794: loss = 1.8040 (0.621 sec/step)\n",
            "I0915 14:04:53.783196 139848901543808 learning.py:507] global step 1794: loss = 1.8040 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1795: loss = 1.6971 (0.662 sec/step)\n",
            "I0915 14:04:54.447506 139848901543808 learning.py:507] global step 1795: loss = 1.6971 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1796: loss = 1.6646 (0.631 sec/step)\n",
            "I0915 14:04:55.080416 139848901543808 learning.py:507] global step 1796: loss = 1.6646 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1797: loss = 4.2424 (0.617 sec/step)\n",
            "I0915 14:04:55.700043 139848901543808 learning.py:507] global step 1797: loss = 4.2424 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1798: loss = 1.9480 (0.637 sec/step)\n",
            "I0915 14:04:56.339382 139848901543808 learning.py:507] global step 1798: loss = 1.9480 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1799: loss = 1.6753 (0.615 sec/step)\n",
            "I0915 14:04:56.956453 139848901543808 learning.py:507] global step 1799: loss = 1.6753 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1800: loss = 2.4504 (0.621 sec/step)\n",
            "I0915 14:04:57.579134 139848901543808 learning.py:507] global step 1800: loss = 2.4504 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1801: loss = 2.1262 (0.617 sec/step)\n",
            "I0915 14:04:58.197801 139848901543808 learning.py:507] global step 1801: loss = 2.1262 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1802: loss = 1.3466 (0.634 sec/step)\n",
            "I0915 14:04:58.833855 139848901543808 learning.py:507] global step 1802: loss = 1.3466 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1803: loss = 1.6855 (0.622 sec/step)\n",
            "I0915 14:04:59.458060 139848901543808 learning.py:507] global step 1803: loss = 1.6855 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1804: loss = 2.2105 (0.627 sec/step)\n",
            "I0915 14:05:00.087091 139848901543808 learning.py:507] global step 1804: loss = 2.2105 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1805: loss = 2.7045 (0.633 sec/step)\n",
            "I0915 14:05:00.722253 139848901543808 learning.py:507] global step 1805: loss = 2.7045 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1806: loss = 2.1436 (0.610 sec/step)\n",
            "I0915 14:05:01.334134 139848901543808 learning.py:507] global step 1806: loss = 2.1436 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1807: loss = 2.4911 (0.626 sec/step)\n",
            "I0915 14:05:01.962566 139848901543808 learning.py:507] global step 1807: loss = 2.4911 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1808: loss = 2.9201 (0.627 sec/step)\n",
            "I0915 14:05:02.591701 139848901543808 learning.py:507] global step 1808: loss = 2.9201 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1809: loss = 1.7094 (0.612 sec/step)\n",
            "I0915 14:05:03.205913 139848901543808 learning.py:507] global step 1809: loss = 1.7094 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1810: loss = 2.1800 (0.612 sec/step)\n",
            "I0915 14:05:03.820678 139848901543808 learning.py:507] global step 1810: loss = 2.1800 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1811: loss = 2.3127 (0.614 sec/step)\n",
            "I0915 14:05:04.436452 139848901543808 learning.py:507] global step 1811: loss = 2.3127 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1812: loss = 2.8912 (0.641 sec/step)\n",
            "I0915 14:05:05.078718 139848901543808 learning.py:507] global step 1812: loss = 2.8912 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1813: loss = 2.1533 (0.627 sec/step)\n",
            "I0915 14:05:05.707770 139848901543808 learning.py:507] global step 1813: loss = 2.1533 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1814: loss = 2.1889 (0.616 sec/step)\n",
            "I0915 14:05:06.325482 139848901543808 learning.py:507] global step 1814: loss = 2.1889 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1815: loss = 3.4527 (0.631 sec/step)\n",
            "I0915 14:05:06.958496 139848901543808 learning.py:507] global step 1815: loss = 3.4527 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1816: loss = 2.6170 (0.610 sec/step)\n",
            "I0915 14:05:07.570329 139848901543808 learning.py:507] global step 1816: loss = 2.6170 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1817: loss = 2.3493 (0.596 sec/step)\n",
            "I0915 14:05:08.168021 139848901543808 learning.py:507] global step 1817: loss = 2.3493 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1818: loss = 1.4156 (0.630 sec/step)\n",
            "I0915 14:05:08.799640 139848901543808 learning.py:507] global step 1818: loss = 1.4156 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1819: loss = 2.0106 (0.621 sec/step)\n",
            "I0915 14:05:09.422741 139848901543808 learning.py:507] global step 1819: loss = 2.0106 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1820: loss = 1.5099 (0.611 sec/step)\n",
            "I0915 14:05:10.035452 139848901543808 learning.py:507] global step 1820: loss = 1.5099 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1821: loss = 1.3980 (0.620 sec/step)\n",
            "I0915 14:05:10.657835 139848901543808 learning.py:507] global step 1821: loss = 1.3980 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1822: loss = 1.7204 (0.642 sec/step)\n",
            "I0915 14:05:11.301245 139848901543808 learning.py:507] global step 1822: loss = 1.7204 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1823: loss = 2.0248 (0.651 sec/step)\n",
            "I0915 14:05:11.954106 139848901543808 learning.py:507] global step 1823: loss = 2.0248 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1824: loss = 2.9445 (0.614 sec/step)\n",
            "I0915 14:05:12.570242 139848901543808 learning.py:507] global step 1824: loss = 2.9445 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1825: loss = 1.7012 (0.653 sec/step)\n",
            "I0915 14:05:13.225418 139848901543808 learning.py:507] global step 1825: loss = 1.7012 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1826: loss = 2.5463 (0.618 sec/step)\n",
            "I0915 14:05:13.845371 139848901543808 learning.py:507] global step 1826: loss = 2.5463 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1827: loss = 2.4498 (0.600 sec/step)\n",
            "I0915 14:05:14.447196 139848901543808 learning.py:507] global step 1827: loss = 2.4498 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 1828: loss = 2.0520 (0.605 sec/step)\n",
            "I0915 14:05:15.054035 139848901543808 learning.py:507] global step 1828: loss = 2.0520 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 1829: loss = 2.0724 (0.619 sec/step)\n",
            "I0915 14:05:15.675275 139848901543808 learning.py:507] global step 1829: loss = 2.0724 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1830: loss = 1.9865 (0.603 sec/step)\n",
            "I0915 14:05:16.280660 139848901543808 learning.py:507] global step 1830: loss = 1.9865 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1831: loss = 1.7277 (0.634 sec/step)\n",
            "I0915 14:05:16.916574 139848901543808 learning.py:507] global step 1831: loss = 1.7277 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1832: loss = 2.0983 (0.598 sec/step)\n",
            "I0915 14:05:17.516759 139848901543808 learning.py:507] global step 1832: loss = 2.0983 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1833: loss = 1.8099 (0.621 sec/step)\n",
            "I0915 14:05:18.140152 139848901543808 learning.py:507] global step 1833: loss = 1.8099 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1834: loss = 2.2512 (0.596 sec/step)\n",
            "I0915 14:05:18.738287 139848901543808 learning.py:507] global step 1834: loss = 2.2512 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1835: loss = 2.3289 (0.629 sec/step)\n",
            "I0915 14:05:19.369643 139848901543808 learning.py:507] global step 1835: loss = 2.3289 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1836: loss = 2.3101 (0.610 sec/step)\n",
            "I0915 14:05:19.981731 139848901543808 learning.py:507] global step 1836: loss = 2.3101 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1837: loss = 1.8671 (0.614 sec/step)\n",
            "I0915 14:05:20.597872 139848901543808 learning.py:507] global step 1837: loss = 1.8671 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1838: loss = 1.5034 (0.656 sec/step)\n",
            "I0915 14:05:21.255842 139848901543808 learning.py:507] global step 1838: loss = 1.5034 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1839: loss = 2.3225 (0.602 sec/step)\n",
            "I0915 14:05:21.859399 139848901543808 learning.py:507] global step 1839: loss = 2.3225 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1840: loss = 3.0408 (0.619 sec/step)\n",
            "I0915 14:05:22.480381 139848901543808 learning.py:507] global step 1840: loss = 3.0408 (0.619 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "I0915 14:05:22.837676 139845851117312 supervisor.py:1117] Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "INFO:tensorflow:global step 1841: loss = 2.6056 (0.738 sec/step)\n",
            "I0915 14:05:23.260547 139848901543808 learning.py:507] global step 1841: loss = 2.6056 (0.738 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1841.\n",
            "I0915 14:05:24.469491 139845834331904 supervisor.py:1050] Recording summary at step 1841.\n",
            "INFO:tensorflow:global step 1842: loss = 2.5288 (1.385 sec/step)\n",
            "I0915 14:05:24.660374 139848901543808 learning.py:507] global step 1842: loss = 2.5288 (1.385 sec/step)\n",
            "INFO:tensorflow:global step 1843: loss = 3.2359 (1.030 sec/step)\n",
            "I0915 14:05:25.703665 139848901543808 learning.py:507] global step 1843: loss = 3.2359 (1.030 sec/step)\n",
            "INFO:tensorflow:global step 1844: loss = 2.1659 (1.017 sec/step)\n",
            "I0915 14:05:26.736996 139848901543808 learning.py:507] global step 1844: loss = 2.1659 (1.017 sec/step)\n",
            "INFO:tensorflow:global step 1845: loss = 2.4647 (0.826 sec/step)\n",
            "I0915 14:05:27.594460 139848901543808 learning.py:507] global step 1845: loss = 2.4647 (0.826 sec/step)\n",
            "INFO:tensorflow:global step 1846: loss = 1.7919 (0.664 sec/step)\n",
            "I0915 14:05:28.261287 139848901543808 learning.py:507] global step 1846: loss = 1.7919 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1847: loss = 2.8221 (0.631 sec/step)\n",
            "I0915 14:05:28.894292 139848901543808 learning.py:507] global step 1847: loss = 2.8221 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1848: loss = 2.0262 (0.654 sec/step)\n",
            "I0915 14:05:29.550820 139848901543808 learning.py:507] global step 1848: loss = 2.0262 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1849: loss = 2.4592 (0.642 sec/step)\n",
            "I0915 14:05:30.195054 139848901543808 learning.py:507] global step 1849: loss = 2.4592 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1850: loss = 1.6479 (0.669 sec/step)\n",
            "I0915 14:05:30.865784 139848901543808 learning.py:507] global step 1850: loss = 1.6479 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1851: loss = 2.2858 (0.632 sec/step)\n",
            "I0915 14:05:31.499926 139848901543808 learning.py:507] global step 1851: loss = 2.2858 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1852: loss = 2.3412 (0.656 sec/step)\n",
            "I0915 14:05:32.157773 139848901543808 learning.py:507] global step 1852: loss = 2.3412 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1853: loss = 2.0607 (0.639 sec/step)\n",
            "I0915 14:05:32.798526 139848901543808 learning.py:507] global step 1853: loss = 2.0607 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1854: loss = 2.3779 (0.640 sec/step)\n",
            "I0915 14:05:33.440383 139848901543808 learning.py:507] global step 1854: loss = 2.3779 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1855: loss = 2.2490 (0.626 sec/step)\n",
            "I0915 14:05:34.068327 139848901543808 learning.py:507] global step 1855: loss = 2.2490 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1856: loss = 0.9472 (0.691 sec/step)\n",
            "I0915 14:05:34.760800 139848901543808 learning.py:507] global step 1856: loss = 0.9472 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1857: loss = 1.9466 (0.646 sec/step)\n",
            "I0915 14:05:35.408495 139848901543808 learning.py:507] global step 1857: loss = 1.9466 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1858: loss = 1.7974 (0.631 sec/step)\n",
            "I0915 14:05:36.041694 139848901543808 learning.py:507] global step 1858: loss = 1.7974 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1859: loss = 2.8616 (0.625 sec/step)\n",
            "I0915 14:05:36.669027 139848901543808 learning.py:507] global step 1859: loss = 2.8616 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1860: loss = 3.4010 (0.643 sec/step)\n",
            "I0915 14:05:37.314443 139848901543808 learning.py:507] global step 1860: loss = 3.4010 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1861: loss = 2.1717 (0.641 sec/step)\n",
            "I0915 14:05:37.957531 139848901543808 learning.py:507] global step 1861: loss = 2.1717 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1862: loss = 2.0628 (0.663 sec/step)\n",
            "I0915 14:05:38.622317 139848901543808 learning.py:507] global step 1862: loss = 2.0628 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1863: loss = 3.6924 (0.637 sec/step)\n",
            "I0915 14:05:39.260725 139848901543808 learning.py:507] global step 1863: loss = 3.6924 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1864: loss = 2.5792 (0.632 sec/step)\n",
            "I0915 14:05:39.895369 139848901543808 learning.py:507] global step 1864: loss = 2.5792 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1865: loss = 1.3358 (0.642 sec/step)\n",
            "I0915 14:05:40.539462 139848901543808 learning.py:507] global step 1865: loss = 1.3358 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1866: loss = 1.9356 (0.633 sec/step)\n",
            "I0915 14:05:41.174900 139848901543808 learning.py:507] global step 1866: loss = 1.9356 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1867: loss = 1.6803 (0.636 sec/step)\n",
            "I0915 14:05:41.812855 139848901543808 learning.py:507] global step 1867: loss = 1.6803 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1868: loss = 2.0094 (0.650 sec/step)\n",
            "I0915 14:05:42.465607 139848901543808 learning.py:507] global step 1868: loss = 2.0094 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1869: loss = 1.7624 (0.617 sec/step)\n",
            "I0915 14:05:43.084680 139848901543808 learning.py:507] global step 1869: loss = 1.7624 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1870: loss = 3.1299 (0.638 sec/step)\n",
            "I0915 14:05:43.724854 139848901543808 learning.py:507] global step 1870: loss = 3.1299 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1871: loss = 2.1752 (0.667 sec/step)\n",
            "I0915 14:05:44.394078 139848901543808 learning.py:507] global step 1871: loss = 2.1752 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1872: loss = 3.0748 (0.652 sec/step)\n",
            "I0915 14:05:45.049312 139848901543808 learning.py:507] global step 1872: loss = 3.0748 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1873: loss = 1.5516 (0.644 sec/step)\n",
            "I0915 14:05:45.695806 139848901543808 learning.py:507] global step 1873: loss = 1.5516 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1874: loss = 1.5012 (0.636 sec/step)\n",
            "I0915 14:05:46.333940 139848901543808 learning.py:507] global step 1874: loss = 1.5012 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1875: loss = 2.0794 (0.644 sec/step)\n",
            "I0915 14:05:46.980278 139848901543808 learning.py:507] global step 1875: loss = 2.0794 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1876: loss = 1.6273 (0.614 sec/step)\n",
            "I0915 14:05:47.598562 139848901543808 learning.py:507] global step 1876: loss = 1.6273 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1877: loss = 2.0782 (0.623 sec/step)\n",
            "I0915 14:05:48.224084 139848901543808 learning.py:507] global step 1877: loss = 2.0782 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1878: loss = 2.7689 (0.653 sec/step)\n",
            "I0915 14:05:48.879269 139848901543808 learning.py:507] global step 1878: loss = 2.7689 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1879: loss = 1.8480 (0.625 sec/step)\n",
            "I0915 14:05:49.505758 139848901543808 learning.py:507] global step 1879: loss = 1.8480 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1880: loss = 1.4806 (0.648 sec/step)\n",
            "I0915 14:05:50.155990 139848901543808 learning.py:507] global step 1880: loss = 1.4806 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1881: loss = 1.6833 (0.627 sec/step)\n",
            "I0915 14:05:50.785328 139848901543808 learning.py:507] global step 1881: loss = 1.6833 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1882: loss = 2.8606 (0.658 sec/step)\n",
            "I0915 14:05:51.444896 139848901543808 learning.py:507] global step 1882: loss = 2.8606 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1883: loss = 1.7927 (0.636 sec/step)\n",
            "I0915 14:05:52.082313 139848901543808 learning.py:507] global step 1883: loss = 1.7927 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1884: loss = 1.6444 (0.630 sec/step)\n",
            "I0915 14:05:52.714608 139848901543808 learning.py:507] global step 1884: loss = 1.6444 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1885: loss = 1.4114 (0.627 sec/step)\n",
            "I0915 14:05:53.344129 139848901543808 learning.py:507] global step 1885: loss = 1.4114 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1886: loss = 2.8412 (0.659 sec/step)\n",
            "I0915 14:05:54.005292 139848901543808 learning.py:507] global step 1886: loss = 2.8412 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1887: loss = 1.4193 (0.647 sec/step)\n",
            "I0915 14:05:54.654399 139848901543808 learning.py:507] global step 1887: loss = 1.4193 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1888: loss = 1.2365 (0.656 sec/step)\n",
            "I0915 14:05:55.312011 139848901543808 learning.py:507] global step 1888: loss = 1.2365 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1889: loss = 2.2424 (0.626 sec/step)\n",
            "I0915 14:05:55.940122 139848901543808 learning.py:507] global step 1889: loss = 2.2424 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1890: loss = 2.0365 (0.657 sec/step)\n",
            "I0915 14:05:56.599102 139848901543808 learning.py:507] global step 1890: loss = 2.0365 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1891: loss = 1.8175 (0.639 sec/step)\n",
            "I0915 14:05:57.239937 139848901543808 learning.py:507] global step 1891: loss = 1.8175 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1892: loss = 2.8009 (0.654 sec/step)\n",
            "I0915 14:05:57.896462 139848901543808 learning.py:507] global step 1892: loss = 2.8009 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1893: loss = 2.0662 (0.636 sec/step)\n",
            "I0915 14:05:58.534469 139848901543808 learning.py:507] global step 1893: loss = 2.0662 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1894: loss = 2.3136 (0.637 sec/step)\n",
            "I0915 14:05:59.173907 139848901543808 learning.py:507] global step 1894: loss = 2.3136 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1895: loss = 1.8817 (0.636 sec/step)\n",
            "I0915 14:05:59.811601 139848901543808 learning.py:507] global step 1895: loss = 1.8817 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1896: loss = 2.0099 (0.634 sec/step)\n",
            "I0915 14:06:00.448077 139848901543808 learning.py:507] global step 1896: loss = 2.0099 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1897: loss = 1.9000 (0.633 sec/step)\n",
            "I0915 14:06:01.082879 139848901543808 learning.py:507] global step 1897: loss = 1.9000 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1898: loss = 1.8964 (0.632 sec/step)\n",
            "I0915 14:06:01.716548 139848901543808 learning.py:507] global step 1898: loss = 1.8964 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1899: loss = 2.1056 (0.631 sec/step)\n",
            "I0915 14:06:02.349163 139848901543808 learning.py:507] global step 1899: loss = 2.1056 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1900: loss = 2.1311 (0.645 sec/step)\n",
            "I0915 14:06:02.995951 139848901543808 learning.py:507] global step 1900: loss = 2.1311 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1901: loss = 1.4580 (0.620 sec/step)\n",
            "I0915 14:06:03.617936 139848901543808 learning.py:507] global step 1901: loss = 1.4580 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1902: loss = 1.7777 (0.658 sec/step)\n",
            "I0915 14:06:04.277696 139848901543808 learning.py:507] global step 1902: loss = 1.7777 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1903: loss = 1.9996 (0.636 sec/step)\n",
            "I0915 14:06:04.915695 139848901543808 learning.py:507] global step 1903: loss = 1.9996 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1904: loss = 1.5914 (0.651 sec/step)\n",
            "I0915 14:06:05.568654 139848901543808 learning.py:507] global step 1904: loss = 1.5914 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1905: loss = 1.3187 (0.615 sec/step)\n",
            "I0915 14:06:06.185424 139848901543808 learning.py:507] global step 1905: loss = 1.3187 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1906: loss = 2.3884 (0.653 sec/step)\n",
            "I0915 14:06:06.840296 139848901543808 learning.py:507] global step 1906: loss = 2.3884 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1907: loss = 1.5027 (0.656 sec/step)\n",
            "I0915 14:06:07.498723 139848901543808 learning.py:507] global step 1907: loss = 1.5027 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1908: loss = 1.1694 (0.641 sec/step)\n",
            "I0915 14:06:08.142051 139848901543808 learning.py:507] global step 1908: loss = 1.1694 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1909: loss = 2.1567 (0.628 sec/step)\n",
            "I0915 14:06:08.772179 139848901543808 learning.py:507] global step 1909: loss = 2.1567 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1910: loss = 2.5486 (0.619 sec/step)\n",
            "I0915 14:06:09.392656 139848901543808 learning.py:507] global step 1910: loss = 2.5486 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1911: loss = 3.0210 (0.626 sec/step)\n",
            "I0915 14:06:10.021120 139848901543808 learning.py:507] global step 1911: loss = 3.0210 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1912: loss = 2.3001 (0.645 sec/step)\n",
            "I0915 14:06:10.667894 139848901543808 learning.py:507] global step 1912: loss = 2.3001 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1913: loss = 1.7638 (0.632 sec/step)\n",
            "I0915 14:06:11.301728 139848901543808 learning.py:507] global step 1913: loss = 1.7638 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1914: loss = 1.8485 (0.639 sec/step)\n",
            "I0915 14:06:11.942556 139848901543808 learning.py:507] global step 1914: loss = 1.8485 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1915: loss = 2.1796 (0.619 sec/step)\n",
            "I0915 14:06:12.563610 139848901543808 learning.py:507] global step 1915: loss = 2.1796 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1916: loss = 2.4479 (0.654 sec/step)\n",
            "I0915 14:06:13.219731 139848901543808 learning.py:507] global step 1916: loss = 2.4479 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1917: loss = 1.0881 (0.635 sec/step)\n",
            "I0915 14:06:13.856649 139848901543808 learning.py:507] global step 1917: loss = 1.0881 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1918: loss = 3.0245 (0.624 sec/step)\n",
            "I0915 14:06:14.482150 139848901543808 learning.py:507] global step 1918: loss = 3.0245 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1919: loss = 2.7694 (0.627 sec/step)\n",
            "I0915 14:06:15.110517 139848901543808 learning.py:507] global step 1919: loss = 2.7694 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1920: loss = 2.0430 (0.623 sec/step)\n",
            "I0915 14:06:15.735456 139848901543808 learning.py:507] global step 1920: loss = 2.0430 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1921: loss = 2.3263 (0.632 sec/step)\n",
            "I0915 14:06:16.369233 139848901543808 learning.py:507] global step 1921: loss = 2.3263 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1922: loss = 2.0055 (0.653 sec/step)\n",
            "I0915 14:06:17.024752 139848901543808 learning.py:507] global step 1922: loss = 2.0055 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1923: loss = 2.7772 (0.647 sec/step)\n",
            "I0915 14:06:17.674450 139848901543808 learning.py:507] global step 1923: loss = 2.7772 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1924: loss = 1.7420 (0.636 sec/step)\n",
            "I0915 14:06:18.312723 139848901543808 learning.py:507] global step 1924: loss = 1.7420 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1925: loss = 2.1954 (0.638 sec/step)\n",
            "I0915 14:06:18.952602 139848901543808 learning.py:507] global step 1925: loss = 2.1954 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1926: loss = 2.1583 (0.646 sec/step)\n",
            "I0915 14:06:19.600848 139848901543808 learning.py:507] global step 1926: loss = 2.1583 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1927: loss = 2.2649 (0.639 sec/step)\n",
            "I0915 14:06:20.241763 139848901543808 learning.py:507] global step 1927: loss = 2.2649 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1928: loss = 3.3225 (0.664 sec/step)\n",
            "I0915 14:06:20.907255 139848901543808 learning.py:507] global step 1928: loss = 3.3225 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1929: loss = 1.8094 (0.637 sec/step)\n",
            "I0915 14:06:21.546501 139848901543808 learning.py:507] global step 1929: loss = 1.8094 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1930: loss = 2.1873 (0.621 sec/step)\n",
            "I0915 14:06:22.169610 139848901543808 learning.py:507] global step 1930: loss = 2.1873 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1931: loss = 1.9455 (0.629 sec/step)\n",
            "I0915 14:06:22.800801 139848901543808 learning.py:507] global step 1931: loss = 1.9455 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1932: loss = 2.1233 (0.632 sec/step)\n",
            "I0915 14:06:23.435300 139848901543808 learning.py:507] global step 1932: loss = 2.1233 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1933: loss = 1.7830 (0.638 sec/step)\n",
            "I0915 14:06:24.075870 139848901543808 learning.py:507] global step 1933: loss = 1.7830 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1934: loss = 3.5239 (0.639 sec/step)\n",
            "I0915 14:06:24.716669 139848901543808 learning.py:507] global step 1934: loss = 3.5239 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1935: loss = 2.0674 (0.654 sec/step)\n",
            "I0915 14:06:25.372618 139848901543808 learning.py:507] global step 1935: loss = 2.0674 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1936: loss = 2.2120 (0.617 sec/step)\n",
            "I0915 14:06:25.991528 139848901543808 learning.py:507] global step 1936: loss = 2.2120 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1937: loss = 2.3951 (0.645 sec/step)\n",
            "I0915 14:06:26.638451 139848901543808 learning.py:507] global step 1937: loss = 2.3951 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1938: loss = 1.4565 (0.624 sec/step)\n",
            "I0915 14:06:27.263791 139848901543808 learning.py:507] global step 1938: loss = 1.4565 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1939: loss = 2.2569 (0.640 sec/step)\n",
            "I0915 14:06:27.905393 139848901543808 learning.py:507] global step 1939: loss = 2.2569 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1940: loss = 3.1458 (0.657 sec/step)\n",
            "I0915 14:06:28.564481 139848901543808 learning.py:507] global step 1940: loss = 3.1458 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1941: loss = 2.0647 (0.639 sec/step)\n",
            "I0915 14:06:29.205469 139848901543808 learning.py:507] global step 1941: loss = 2.0647 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1942: loss = 3.0702 (0.635 sec/step)\n",
            "I0915 14:06:29.842540 139848901543808 learning.py:507] global step 1942: loss = 3.0702 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1943: loss = 3.2352 (0.619 sec/step)\n",
            "I0915 14:06:30.463894 139848901543808 learning.py:507] global step 1943: loss = 3.2352 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1944: loss = 1.6383 (0.666 sec/step)\n",
            "I0915 14:06:31.131721 139848901543808 learning.py:507] global step 1944: loss = 1.6383 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1945: loss = 1.5830 (0.643 sec/step)\n",
            "I0915 14:06:31.776515 139848901543808 learning.py:507] global step 1945: loss = 1.5830 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1946: loss = 2.1546 (0.645 sec/step)\n",
            "I0915 14:06:32.423713 139848901543808 learning.py:507] global step 1946: loss = 2.1546 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1947: loss = 2.2612 (0.650 sec/step)\n",
            "I0915 14:06:33.075274 139848901543808 learning.py:507] global step 1947: loss = 2.2612 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1948: loss = 2.4313 (0.625 sec/step)\n",
            "I0915 14:06:33.702447 139848901543808 learning.py:507] global step 1948: loss = 2.4313 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1949: loss = 2.3584 (0.635 sec/step)\n",
            "I0915 14:06:34.339533 139848901543808 learning.py:507] global step 1949: loss = 2.3584 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1950: loss = 2.1133 (0.619 sec/step)\n",
            "I0915 14:06:34.961062 139848901543808 learning.py:507] global step 1950: loss = 2.1133 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1951: loss = 1.4038 (0.622 sec/step)\n",
            "I0915 14:06:35.585453 139848901543808 learning.py:507] global step 1951: loss = 1.4038 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1952: loss = 1.9327 (0.617 sec/step)\n",
            "I0915 14:06:36.204101 139848901543808 learning.py:507] global step 1952: loss = 1.9327 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1953: loss = 2.3833 (0.598 sec/step)\n",
            "I0915 14:06:36.804485 139848901543808 learning.py:507] global step 1953: loss = 2.3833 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1954: loss = 2.4864 (0.647 sec/step)\n",
            "I0915 14:06:37.453880 139848901543808 learning.py:507] global step 1954: loss = 2.4864 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1955: loss = 3.0030 (0.619 sec/step)\n",
            "I0915 14:06:38.075789 139848901543808 learning.py:507] global step 1955: loss = 3.0030 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1956: loss = 1.6705 (0.628 sec/step)\n",
            "I0915 14:06:38.705341 139848901543808 learning.py:507] global step 1956: loss = 1.6705 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1957: loss = 1.8128 (0.650 sec/step)\n",
            "I0915 14:06:39.357496 139848901543808 learning.py:507] global step 1957: loss = 1.8128 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1958: loss = 2.9503 (0.638 sec/step)\n",
            "I0915 14:06:39.997065 139848901543808 learning.py:507] global step 1958: loss = 2.9503 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1959: loss = 2.9961 (0.648 sec/step)\n",
            "I0915 14:06:40.647154 139848901543808 learning.py:507] global step 1959: loss = 2.9961 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1960: loss = 1.9510 (0.633 sec/step)\n",
            "I0915 14:06:41.282326 139848901543808 learning.py:507] global step 1960: loss = 1.9510 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1961: loss = 2.9276 (0.632 sec/step)\n",
            "I0915 14:06:41.915817 139848901543808 learning.py:507] global step 1961: loss = 2.9276 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1962: loss = 1.9581 (0.630 sec/step)\n",
            "I0915 14:06:42.548083 139848901543808 learning.py:507] global step 1962: loss = 1.9581 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1963: loss = 2.1620 (0.629 sec/step)\n",
            "I0915 14:06:43.179865 139848901543808 learning.py:507] global step 1963: loss = 2.1620 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1964: loss = 2.3298 (0.644 sec/step)\n",
            "I0915 14:06:43.825797 139848901543808 learning.py:507] global step 1964: loss = 2.3298 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1965: loss = 2.4739 (0.682 sec/step)\n",
            "I0915 14:06:44.510066 139848901543808 learning.py:507] global step 1965: loss = 2.4739 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1966: loss = 1.9042 (0.644 sec/step)\n",
            "I0915 14:06:45.155952 139848901543808 learning.py:507] global step 1966: loss = 1.9042 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1967: loss = 2.2859 (0.631 sec/step)\n",
            "I0915 14:06:45.788657 139848901543808 learning.py:507] global step 1967: loss = 2.2859 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1968: loss = 1.7876 (0.649 sec/step)\n",
            "I0915 14:06:46.439866 139848901543808 learning.py:507] global step 1968: loss = 1.7876 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1969: loss = 1.4214 (0.624 sec/step)\n",
            "I0915 14:06:47.066808 139848901543808 learning.py:507] global step 1969: loss = 1.4214 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1970: loss = 1.9967 (0.632 sec/step)\n",
            "I0915 14:06:47.700872 139848901543808 learning.py:507] global step 1970: loss = 1.9967 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1971: loss = 2.2022 (0.656 sec/step)\n",
            "I0915 14:06:48.358863 139848901543808 learning.py:507] global step 1971: loss = 2.2022 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1972: loss = 2.6562 (0.615 sec/step)\n",
            "I0915 14:06:48.975670 139848901543808 learning.py:507] global step 1972: loss = 2.6562 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1973: loss = 1.9374 (0.652 sec/step)\n",
            "I0915 14:06:49.630119 139848901543808 learning.py:507] global step 1973: loss = 1.9374 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1974: loss = 0.9025 (0.631 sec/step)\n",
            "I0915 14:06:50.262626 139848901543808 learning.py:507] global step 1974: loss = 0.9025 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1975: loss = 2.9665 (0.656 sec/step)\n",
            "I0915 14:06:50.920225 139848901543808 learning.py:507] global step 1975: loss = 2.9665 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1976: loss = 2.7387 (0.625 sec/step)\n",
            "I0915 14:06:51.547249 139848901543808 learning.py:507] global step 1976: loss = 2.7387 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1977: loss = 2.5864 (0.637 sec/step)\n",
            "I0915 14:06:52.186586 139848901543808 learning.py:507] global step 1977: loss = 2.5864 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1978: loss = 2.7020 (0.633 sec/step)\n",
            "I0915 14:06:52.821781 139848901543808 learning.py:507] global step 1978: loss = 2.7020 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1979: loss = 1.9980 (0.615 sec/step)\n",
            "I0915 14:06:53.438517 139848901543808 learning.py:507] global step 1979: loss = 1.9980 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1980: loss = 2.8567 (0.621 sec/step)\n",
            "I0915 14:06:54.062136 139848901543808 learning.py:507] global step 1980: loss = 2.8567 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1981: loss = 1.9859 (0.639 sec/step)\n",
            "I0915 14:06:54.703528 139848901543808 learning.py:507] global step 1981: loss = 1.9859 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1982: loss = 2.3889 (0.625 sec/step)\n",
            "I0915 14:06:55.330638 139848901543808 learning.py:507] global step 1982: loss = 2.3889 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1983: loss = 1.5406 (0.637 sec/step)\n",
            "I0915 14:06:55.970271 139848901543808 learning.py:507] global step 1983: loss = 1.5406 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1984: loss = 1.5445 (0.635 sec/step)\n",
            "I0915 14:06:56.606892 139848901543808 learning.py:507] global step 1984: loss = 1.5445 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1985: loss = 2.1237 (0.641 sec/step)\n",
            "I0915 14:06:57.249705 139848901543808 learning.py:507] global step 1985: loss = 2.1237 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1986: loss = 2.1289 (0.638 sec/step)\n",
            "I0915 14:06:57.890398 139848901543808 learning.py:507] global step 1986: loss = 2.1289 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1987: loss = 1.1162 (0.658 sec/step)\n",
            "I0915 14:06:58.551090 139848901543808 learning.py:507] global step 1987: loss = 1.1162 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1988: loss = 1.6999 (0.645 sec/step)\n",
            "I0915 14:06:59.200380 139848901543808 learning.py:507] global step 1988: loss = 1.6999 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1989: loss = 1.9814 (0.651 sec/step)\n",
            "I0915 14:06:59.853607 139848901543808 learning.py:507] global step 1989: loss = 1.9814 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1990: loss = 2.0012 (0.625 sec/step)\n",
            "I0915 14:07:00.480502 139848901543808 learning.py:507] global step 1990: loss = 2.0012 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1991: loss = 1.9740 (0.640 sec/step)\n",
            "I0915 14:07:01.122249 139848901543808 learning.py:507] global step 1991: loss = 1.9740 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1992: loss = 1.7758 (0.614 sec/step)\n",
            "I0915 14:07:01.738117 139848901543808 learning.py:507] global step 1992: loss = 1.7758 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1993: loss = 2.1510 (0.631 sec/step)\n",
            "I0915 14:07:02.371629 139848901543808 learning.py:507] global step 1993: loss = 2.1510 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1994: loss = 1.8056 (0.647 sec/step)\n",
            "I0915 14:07:03.020399 139848901543808 learning.py:507] global step 1994: loss = 1.8056 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1995: loss = 2.0735 (0.642 sec/step)\n",
            "I0915 14:07:03.664055 139848901543808 learning.py:507] global step 1995: loss = 2.0735 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1996: loss = 1.9357 (0.640 sec/step)\n",
            "I0915 14:07:04.305531 139848901543808 learning.py:507] global step 1996: loss = 1.9357 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1997: loss = 1.9326 (0.632 sec/step)\n",
            "I0915 14:07:04.939201 139848901543808 learning.py:507] global step 1997: loss = 1.9326 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1998: loss = 2.4525 (0.643 sec/step)\n",
            "I0915 14:07:05.583893 139848901543808 learning.py:507] global step 1998: loss = 2.4525 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1999: loss = 1.3840 (0.649 sec/step)\n",
            "I0915 14:07:06.234902 139848901543808 learning.py:507] global step 1999: loss = 1.3840 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2000: loss = 2.6591 (0.636 sec/step)\n",
            "I0915 14:07:06.872478 139848901543808 learning.py:507] global step 2000: loss = 2.6591 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2001: loss = 1.7993 (0.625 sec/step)\n",
            "I0915 14:07:07.499096 139848901543808 learning.py:507] global step 2001: loss = 1.7993 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2002: loss = 1.9469 (0.640 sec/step)\n",
            "I0915 14:07:08.140814 139848901543808 learning.py:507] global step 2002: loss = 1.9469 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2003: loss = 2.7398 (0.632 sec/step)\n",
            "I0915 14:07:08.775480 139848901543808 learning.py:507] global step 2003: loss = 2.7398 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2004: loss = 2.1090 (0.647 sec/step)\n",
            "I0915 14:07:09.424546 139848901543808 learning.py:507] global step 2004: loss = 2.1090 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2005: loss = 2.0537 (0.653 sec/step)\n",
            "I0915 14:07:10.079877 139848901543808 learning.py:507] global step 2005: loss = 2.0537 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2006: loss = 1.9702 (0.629 sec/step)\n",
            "I0915 14:07:10.710546 139848901543808 learning.py:507] global step 2006: loss = 1.9702 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2007: loss = 2.8177 (0.612 sec/step)\n",
            "I0915 14:07:11.325175 139848901543808 learning.py:507] global step 2007: loss = 2.8177 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2008: loss = 2.3002 (0.632 sec/step)\n",
            "I0915 14:07:11.959275 139848901543808 learning.py:507] global step 2008: loss = 2.3002 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2009: loss = 2.0716 (0.628 sec/step)\n",
            "I0915 14:07:12.589708 139848901543808 learning.py:507] global step 2009: loss = 2.0716 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2010: loss = 2.9085 (0.626 sec/step)\n",
            "I0915 14:07:13.217500 139848901543808 learning.py:507] global step 2010: loss = 2.9085 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2011: loss = 2.3911 (0.608 sec/step)\n",
            "I0915 14:07:13.827737 139848901543808 learning.py:507] global step 2011: loss = 2.3911 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2012: loss = 1.3168 (0.651 sec/step)\n",
            "I0915 14:07:14.480656 139848901543808 learning.py:507] global step 2012: loss = 1.3168 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2013: loss = 1.7031 (0.622 sec/step)\n",
            "I0915 14:07:15.104796 139848901543808 learning.py:507] global step 2013: loss = 1.7031 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2014: loss = 2.1156 (0.641 sec/step)\n",
            "I0915 14:07:15.747602 139848901543808 learning.py:507] global step 2014: loss = 2.1156 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2015: loss = 1.9709 (0.661 sec/step)\n",
            "I0915 14:07:16.411029 139848901543808 learning.py:507] global step 2015: loss = 1.9709 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2016: loss = 2.6861 (0.642 sec/step)\n",
            "I0915 14:07:17.054928 139848901543808 learning.py:507] global step 2016: loss = 2.6861 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2017: loss = 3.0768 (0.632 sec/step)\n",
            "I0915 14:07:17.688376 139848901543808 learning.py:507] global step 2017: loss = 3.0768 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2018: loss = 1.4915 (0.649 sec/step)\n",
            "I0915 14:07:18.339076 139848901543808 learning.py:507] global step 2018: loss = 1.4915 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2019: loss = 3.1595 (0.643 sec/step)\n",
            "I0915 14:07:18.984210 139848901543808 learning.py:507] global step 2019: loss = 3.1595 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2020: loss = 1.2581 (0.640 sec/step)\n",
            "I0915 14:07:19.626575 139848901543808 learning.py:507] global step 2020: loss = 1.2581 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2021: loss = 1.8708 (0.623 sec/step)\n",
            "I0915 14:07:20.251218 139848901543808 learning.py:507] global step 2021: loss = 1.8708 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2022: loss = 2.4604 (0.644 sec/step)\n",
            "I0915 14:07:20.897093 139848901543808 learning.py:507] global step 2022: loss = 2.4604 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2023: loss = 2.5508 (0.648 sec/step)\n",
            "I0915 14:07:21.546799 139848901543808 learning.py:507] global step 2023: loss = 2.5508 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2024: loss = 1.6923 (0.656 sec/step)\n",
            "I0915 14:07:22.204513 139848901543808 learning.py:507] global step 2024: loss = 1.6923 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2025: loss = 1.4230 (0.638 sec/step)\n",
            "I0915 14:07:22.847605 139848901543808 learning.py:507] global step 2025: loss = 1.4230 (0.638 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2026.\n",
            "I0915 14:07:23.932980 139845834331904 supervisor.py:1050] Recording summary at step 2026.\n",
            "INFO:tensorflow:global step 2026: loss = 1.6170 (1.083 sec/step)\n",
            "I0915 14:07:23.941247 139848901543808 learning.py:507] global step 2026: loss = 1.6170 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 2027: loss = 2.3428 (0.652 sec/step)\n",
            "I0915 14:07:24.599508 139848901543808 learning.py:507] global step 2027: loss = 2.3428 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2028: loss = 1.9573 (0.629 sec/step)\n",
            "I0915 14:07:25.230841 139848901543808 learning.py:507] global step 2028: loss = 1.9573 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2029: loss = 1.8827 (0.655 sec/step)\n",
            "I0915 14:07:25.887808 139848901543808 learning.py:507] global step 2029: loss = 1.8827 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2030: loss = 3.0645 (0.667 sec/step)\n",
            "I0915 14:07:26.556748 139848901543808 learning.py:507] global step 2030: loss = 3.0645 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 2031: loss = 2.2487 (0.627 sec/step)\n",
            "I0915 14:07:27.186056 139848901543808 learning.py:507] global step 2031: loss = 2.2487 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2032: loss = 1.9960 (0.655 sec/step)\n",
            "I0915 14:07:27.842917 139848901543808 learning.py:507] global step 2032: loss = 1.9960 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2033: loss = 2.6819 (0.652 sec/step)\n",
            "I0915 14:07:28.497458 139848901543808 learning.py:507] global step 2033: loss = 2.6819 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2034: loss = 1.8545 (0.634 sec/step)\n",
            "I0915 14:07:29.133433 139848901543808 learning.py:507] global step 2034: loss = 1.8545 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2035: loss = 1.8837 (0.635 sec/step)\n",
            "I0915 14:07:29.770818 139848901543808 learning.py:507] global step 2035: loss = 1.8837 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2036: loss = 1.5871 (0.633 sec/step)\n",
            "I0915 14:07:30.406322 139848901543808 learning.py:507] global step 2036: loss = 1.5871 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2037: loss = 1.7627 (0.631 sec/step)\n",
            "I0915 14:07:31.039518 139848901543808 learning.py:507] global step 2037: loss = 1.7627 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2038: loss = 2.2669 (0.656 sec/step)\n",
            "I0915 14:07:31.697017 139848901543808 learning.py:507] global step 2038: loss = 2.2669 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2039: loss = 2.1107 (0.653 sec/step)\n",
            "I0915 14:07:32.352992 139848901543808 learning.py:507] global step 2039: loss = 2.1107 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2040: loss = 1.7897 (0.631 sec/step)\n",
            "I0915 14:07:32.985470 139848901543808 learning.py:507] global step 2040: loss = 1.7897 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2041: loss = 2.1269 (0.625 sec/step)\n",
            "I0915 14:07:33.611875 139848901543808 learning.py:507] global step 2041: loss = 2.1269 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2042: loss = 1.2443 (0.631 sec/step)\n",
            "I0915 14:07:34.245187 139848901543808 learning.py:507] global step 2042: loss = 1.2443 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2043: loss = 2.6816 (0.636 sec/step)\n",
            "I0915 14:07:34.883509 139848901543808 learning.py:507] global step 2043: loss = 2.6816 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2044: loss = 2.9438 (0.634 sec/step)\n",
            "I0915 14:07:35.519909 139848901543808 learning.py:507] global step 2044: loss = 2.9438 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2045: loss = 2.5658 (0.636 sec/step)\n",
            "I0915 14:07:36.158141 139848901543808 learning.py:507] global step 2045: loss = 2.5658 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2046: loss = 1.6402 (0.620 sec/step)\n",
            "I0915 14:07:36.780250 139848901543808 learning.py:507] global step 2046: loss = 1.6402 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2047: loss = 1.4943 (0.625 sec/step)\n",
            "I0915 14:07:37.407087 139848901543808 learning.py:507] global step 2047: loss = 1.4943 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2048: loss = 1.8078 (0.636 sec/step)\n",
            "I0915 14:07:38.045543 139848901543808 learning.py:507] global step 2048: loss = 1.8078 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2049: loss = 3.3569 (0.646 sec/step)\n",
            "I0915 14:07:38.693595 139848901543808 learning.py:507] global step 2049: loss = 3.3569 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2050: loss = 1.8276 (0.609 sec/step)\n",
            "I0915 14:07:39.304047 139848901543808 learning.py:507] global step 2050: loss = 1.8276 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2051: loss = 2.1375 (0.626 sec/step)\n",
            "I0915 14:07:39.931772 139848901543808 learning.py:507] global step 2051: loss = 2.1375 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2052: loss = 1.6664 (0.602 sec/step)\n",
            "I0915 14:07:40.535629 139848901543808 learning.py:507] global step 2052: loss = 1.6664 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2053: loss = 2.2079 (0.651 sec/step)\n",
            "I0915 14:07:41.188600 139848901543808 learning.py:507] global step 2053: loss = 2.2079 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2054: loss = 1.6810 (0.633 sec/step)\n",
            "I0915 14:07:41.824035 139848901543808 learning.py:507] global step 2054: loss = 1.6810 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2055: loss = 1.8052 (0.616 sec/step)\n",
            "I0915 14:07:42.441500 139848901543808 learning.py:507] global step 2055: loss = 1.8052 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2056: loss = 2.4172 (0.634 sec/step)\n",
            "I0915 14:07:43.077067 139848901543808 learning.py:507] global step 2056: loss = 2.4172 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2057: loss = 2.2399 (0.630 sec/step)\n",
            "I0915 14:07:43.708667 139848901543808 learning.py:507] global step 2057: loss = 2.2399 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2058: loss = 1.6414 (0.657 sec/step)\n",
            "I0915 14:07:44.367328 139848901543808 learning.py:507] global step 2058: loss = 1.6414 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2059: loss = 2.8462 (0.637 sec/step)\n",
            "I0915 14:07:45.006937 139848901543808 learning.py:507] global step 2059: loss = 2.8462 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2060: loss = 1.9625 (0.635 sec/step)\n",
            "I0915 14:07:45.643740 139848901543808 learning.py:507] global step 2060: loss = 1.9625 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2061: loss = 2.3796 (0.622 sec/step)\n",
            "I0915 14:07:46.267379 139848901543808 learning.py:507] global step 2061: loss = 2.3796 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2062: loss = 1.4015 (0.640 sec/step)\n",
            "I0915 14:07:46.909079 139848901543808 learning.py:507] global step 2062: loss = 1.4015 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2063: loss = 2.6414 (0.640 sec/step)\n",
            "I0915 14:07:47.551215 139848901543808 learning.py:507] global step 2063: loss = 2.6414 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2064: loss = 2.1780 (0.635 sec/step)\n",
            "I0915 14:07:48.187950 139848901543808 learning.py:507] global step 2064: loss = 2.1780 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2065: loss = 2.2800 (0.644 sec/step)\n",
            "I0915 14:07:48.833660 139848901543808 learning.py:507] global step 2065: loss = 2.2800 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2066: loss = 2.0477 (0.639 sec/step)\n",
            "I0915 14:07:49.474563 139848901543808 learning.py:507] global step 2066: loss = 2.0477 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2067: loss = 2.0560 (0.622 sec/step)\n",
            "I0915 14:07:50.098175 139848901543808 learning.py:507] global step 2067: loss = 2.0560 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2068: loss = 2.1225 (0.642 sec/step)\n",
            "I0915 14:07:50.742102 139848901543808 learning.py:507] global step 2068: loss = 2.1225 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2069: loss = 2.2034 (0.631 sec/step)\n",
            "I0915 14:07:51.374963 139848901543808 learning.py:507] global step 2069: loss = 2.2034 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2070: loss = 2.1242 (0.668 sec/step)\n",
            "I0915 14:07:52.045149 139848901543808 learning.py:507] global step 2070: loss = 2.1242 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 2071: loss = 1.9059 (0.636 sec/step)\n",
            "I0915 14:07:52.683229 139848901543808 learning.py:507] global step 2071: loss = 1.9059 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2072: loss = 2.2974 (0.650 sec/step)\n",
            "I0915 14:07:53.335474 139848901543808 learning.py:507] global step 2072: loss = 2.2974 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2073: loss = 2.6022 (0.645 sec/step)\n",
            "I0915 14:07:53.982539 139848901543808 learning.py:507] global step 2073: loss = 2.6022 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2074: loss = 1.9107 (0.627 sec/step)\n",
            "I0915 14:07:54.611365 139848901543808 learning.py:507] global step 2074: loss = 1.9107 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2075: loss = 1.9461 (0.625 sec/step)\n",
            "I0915 14:07:55.238332 139848901543808 learning.py:507] global step 2075: loss = 1.9461 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2076: loss = 1.8396 (0.648 sec/step)\n",
            "I0915 14:07:55.888095 139848901543808 learning.py:507] global step 2076: loss = 1.8396 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2077: loss = 1.9323 (0.656 sec/step)\n",
            "I0915 14:07:56.546113 139848901543808 learning.py:507] global step 2077: loss = 1.9323 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2078: loss = 2.5962 (0.625 sec/step)\n",
            "I0915 14:07:57.173200 139848901543808 learning.py:507] global step 2078: loss = 2.5962 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2079: loss = 1.9201 (0.620 sec/step)\n",
            "I0915 14:07:57.795195 139848901543808 learning.py:507] global step 2079: loss = 1.9201 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2080: loss = 1.4547 (0.635 sec/step)\n",
            "I0915 14:07:58.432601 139848901543808 learning.py:507] global step 2080: loss = 1.4547 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2081: loss = 2.7994 (0.629 sec/step)\n",
            "I0915 14:07:59.064003 139848901543808 learning.py:507] global step 2081: loss = 2.7994 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2082: loss = 2.5772 (0.655 sec/step)\n",
            "I0915 14:07:59.721686 139848901543808 learning.py:507] global step 2082: loss = 2.5772 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2083: loss = 1.7202 (0.618 sec/step)\n",
            "I0915 14:08:00.342040 139848901543808 learning.py:507] global step 2083: loss = 1.7202 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2084: loss = 2.1711 (0.630 sec/step)\n",
            "I0915 14:08:00.973800 139848901543808 learning.py:507] global step 2084: loss = 2.1711 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2085: loss = 1.6038 (0.654 sec/step)\n",
            "I0915 14:08:01.629888 139848901543808 learning.py:507] global step 2085: loss = 1.6038 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2086: loss = 3.1847 (0.638 sec/step)\n",
            "I0915 14:08:02.269810 139848901543808 learning.py:507] global step 2086: loss = 3.1847 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2087: loss = 2.2693 (0.629 sec/step)\n",
            "I0915 14:08:02.900737 139848901543808 learning.py:507] global step 2087: loss = 2.2693 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2088: loss = 1.9810 (0.632 sec/step)\n",
            "I0915 14:08:03.535042 139848901543808 learning.py:507] global step 2088: loss = 1.9810 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2089: loss = 1.7673 (0.618 sec/step)\n",
            "I0915 14:08:04.154708 139848901543808 learning.py:507] global step 2089: loss = 1.7673 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2090: loss = 2.3725 (0.656 sec/step)\n",
            "I0915 14:08:04.812242 139848901543808 learning.py:507] global step 2090: loss = 2.3725 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2091: loss = 1.5855 (0.648 sec/step)\n",
            "I0915 14:08:05.461779 139848901543808 learning.py:507] global step 2091: loss = 1.5855 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2092: loss = 1.7633 (0.614 sec/step)\n",
            "I0915 14:08:06.077273 139848901543808 learning.py:507] global step 2092: loss = 1.7633 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2093: loss = 2.7766 (0.629 sec/step)\n",
            "I0915 14:08:06.707912 139848901543808 learning.py:507] global step 2093: loss = 2.7766 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2094: loss = 2.1228 (0.630 sec/step)\n",
            "I0915 14:08:07.339735 139848901543808 learning.py:507] global step 2094: loss = 2.1228 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2095: loss = 1.5407 (0.645 sec/step)\n",
            "I0915 14:08:07.986491 139848901543808 learning.py:507] global step 2095: loss = 1.5407 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2096: loss = 2.2550 (0.641 sec/step)\n",
            "I0915 14:08:08.629735 139848901543808 learning.py:507] global step 2096: loss = 2.2550 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2097: loss = 2.2078 (0.637 sec/step)\n",
            "I0915 14:08:09.268648 139848901543808 learning.py:507] global step 2097: loss = 2.2078 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2098: loss = 1.5223 (0.625 sec/step)\n",
            "I0915 14:08:09.896441 139848901543808 learning.py:507] global step 2098: loss = 1.5223 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2099: loss = 1.7577 (0.636 sec/step)\n",
            "I0915 14:08:10.534466 139848901543808 learning.py:507] global step 2099: loss = 1.7577 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2100: loss = 2.0023 (0.665 sec/step)\n",
            "I0915 14:08:11.201773 139848901543808 learning.py:507] global step 2100: loss = 2.0023 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2101: loss = 2.6251 (0.638 sec/step)\n",
            "I0915 14:08:11.841540 139848901543808 learning.py:507] global step 2101: loss = 2.6251 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2102: loss = 1.5541 (0.642 sec/step)\n",
            "I0915 14:08:12.485338 139848901543808 learning.py:507] global step 2102: loss = 1.5541 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2103: loss = 1.9874 (0.642 sec/step)\n",
            "I0915 14:08:13.129553 139848901543808 learning.py:507] global step 2103: loss = 1.9874 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2104: loss = 2.1138 (0.632 sec/step)\n",
            "I0915 14:08:13.763891 139848901543808 learning.py:507] global step 2104: loss = 2.1138 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2105: loss = 2.6427 (0.654 sec/step)\n",
            "I0915 14:08:14.420248 139848901543808 learning.py:507] global step 2105: loss = 2.6427 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2106: loss = 2.2269 (0.655 sec/step)\n",
            "I0915 14:08:15.076856 139848901543808 learning.py:507] global step 2106: loss = 2.2269 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2107: loss = 1.6535 (0.647 sec/step)\n",
            "I0915 14:08:15.726220 139848901543808 learning.py:507] global step 2107: loss = 1.6535 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2108: loss = 1.8931 (0.663 sec/step)\n",
            "I0915 14:08:16.390832 139848901543808 learning.py:507] global step 2108: loss = 1.8931 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2109: loss = 1.9304 (0.660 sec/step)\n",
            "I0915 14:08:17.052693 139848901543808 learning.py:507] global step 2109: loss = 1.9304 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2110: loss = 2.7123 (0.634 sec/step)\n",
            "I0915 14:08:17.689193 139848901543808 learning.py:507] global step 2110: loss = 2.7123 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2111: loss = 2.1997 (0.649 sec/step)\n",
            "I0915 14:08:18.339793 139848901543808 learning.py:507] global step 2111: loss = 2.1997 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2112: loss = 3.4563 (0.641 sec/step)\n",
            "I0915 14:08:18.983151 139848901543808 learning.py:507] global step 2112: loss = 3.4563 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2113: loss = 1.2039 (0.633 sec/step)\n",
            "I0915 14:08:19.618374 139848901543808 learning.py:507] global step 2113: loss = 1.2039 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2114: loss = 0.9089 (0.651 sec/step)\n",
            "I0915 14:08:20.271278 139848901543808 learning.py:507] global step 2114: loss = 0.9089 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2115: loss = 1.5789 (0.628 sec/step)\n",
            "I0915 14:08:20.901060 139848901543808 learning.py:507] global step 2115: loss = 1.5789 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2116: loss = 1.8809 (0.632 sec/step)\n",
            "I0915 14:08:21.534802 139848901543808 learning.py:507] global step 2116: loss = 1.8809 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2117: loss = 2.5336 (0.647 sec/step)\n",
            "I0915 14:08:22.183630 139848901543808 learning.py:507] global step 2117: loss = 2.5336 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2118: loss = 1.5303 (0.631 sec/step)\n",
            "I0915 14:08:22.816222 139848901543808 learning.py:507] global step 2118: loss = 1.5303 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2119: loss = 2.5767 (0.633 sec/step)\n",
            "I0915 14:08:23.451419 139848901543808 learning.py:507] global step 2119: loss = 2.5767 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2120: loss = 1.2165 (0.617 sec/step)\n",
            "I0915 14:08:24.070475 139848901543808 learning.py:507] global step 2120: loss = 1.2165 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2121: loss = 1.8923 (0.645 sec/step)\n",
            "I0915 14:08:24.717161 139848901543808 learning.py:507] global step 2121: loss = 1.8923 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2122: loss = 2.3386 (0.624 sec/step)\n",
            "I0915 14:08:25.343171 139848901543808 learning.py:507] global step 2122: loss = 2.3386 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2123: loss = 1.9713 (0.621 sec/step)\n",
            "I0915 14:08:25.966612 139848901543808 learning.py:507] global step 2123: loss = 1.9713 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2124: loss = 1.4404 (0.643 sec/step)\n",
            "I0915 14:08:26.611567 139848901543808 learning.py:507] global step 2124: loss = 1.4404 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2125: loss = 1.8322 (0.618 sec/step)\n",
            "I0915 14:08:27.231870 139848901543808 learning.py:507] global step 2125: loss = 1.8322 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2126: loss = 1.7053 (0.626 sec/step)\n",
            "I0915 14:08:27.860388 139848901543808 learning.py:507] global step 2126: loss = 1.7053 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2127: loss = 1.7681 (0.650 sec/step)\n",
            "I0915 14:08:28.512136 139848901543808 learning.py:507] global step 2127: loss = 1.7681 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2128: loss = 1.5274 (0.631 sec/step)\n",
            "I0915 14:08:29.145029 139848901543808 learning.py:507] global step 2128: loss = 1.5274 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2129: loss = 1.5397 (0.632 sec/step)\n",
            "I0915 14:08:29.779174 139848901543808 learning.py:507] global step 2129: loss = 1.5397 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2130: loss = 2.1179 (0.654 sec/step)\n",
            "I0915 14:08:30.435059 139848901543808 learning.py:507] global step 2130: loss = 2.1179 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2131: loss = 1.3209 (0.632 sec/step)\n",
            "I0915 14:08:31.069232 139848901543808 learning.py:507] global step 2131: loss = 1.3209 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2132: loss = 1.6579 (0.621 sec/step)\n",
            "I0915 14:08:31.691653 139848901543808 learning.py:507] global step 2132: loss = 1.6579 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2133: loss = 2.0837 (0.625 sec/step)\n",
            "I0915 14:08:32.318955 139848901543808 learning.py:507] global step 2133: loss = 2.0837 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2134: loss = 1.8995 (0.637 sec/step)\n",
            "I0915 14:08:32.957475 139848901543808 learning.py:507] global step 2134: loss = 1.8995 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2135: loss = 1.6524 (0.645 sec/step)\n",
            "I0915 14:08:33.604468 139848901543808 learning.py:507] global step 2135: loss = 1.6524 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2136: loss = 1.2399 (0.647 sec/step)\n",
            "I0915 14:08:34.253875 139848901543808 learning.py:507] global step 2136: loss = 1.2399 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2137: loss = 2.5874 (0.652 sec/step)\n",
            "I0915 14:08:34.909329 139848901543808 learning.py:507] global step 2137: loss = 2.5874 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2138: loss = 2.6445 (0.624 sec/step)\n",
            "I0915 14:08:35.535223 139848901543808 learning.py:507] global step 2138: loss = 2.6445 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2139: loss = 3.1341 (0.619 sec/step)\n",
            "I0915 14:08:36.156499 139848901543808 learning.py:507] global step 2139: loss = 3.1341 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2140: loss = 3.0171 (0.645 sec/step)\n",
            "I0915 14:08:36.803427 139848901543808 learning.py:507] global step 2140: loss = 3.0171 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2141: loss = 1.4494 (0.610 sec/step)\n",
            "I0915 14:08:37.415710 139848901543808 learning.py:507] global step 2141: loss = 1.4494 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2142: loss = 2.0404 (0.631 sec/step)\n",
            "I0915 14:08:38.048764 139848901543808 learning.py:507] global step 2142: loss = 2.0404 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2143: loss = 1.8680 (0.641 sec/step)\n",
            "I0915 14:08:38.692709 139848901543808 learning.py:507] global step 2143: loss = 1.8680 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2144: loss = 1.8781 (0.639 sec/step)\n",
            "I0915 14:08:39.333452 139848901543808 learning.py:507] global step 2144: loss = 1.8781 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2145: loss = 1.9409 (0.642 sec/step)\n",
            "I0915 14:08:39.977430 139848901543808 learning.py:507] global step 2145: loss = 1.9409 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2146: loss = 2.1877 (0.626 sec/step)\n",
            "I0915 14:08:40.605271 139848901543808 learning.py:507] global step 2146: loss = 2.1877 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2147: loss = 1.6036 (0.630 sec/step)\n",
            "I0915 14:08:41.237295 139848901543808 learning.py:507] global step 2147: loss = 1.6036 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2148: loss = 1.9286 (0.652 sec/step)\n",
            "I0915 14:08:41.892026 139848901543808 learning.py:507] global step 2148: loss = 1.9286 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2149: loss = 2.4025 (0.654 sec/step)\n",
            "I0915 14:08:42.548402 139848901543808 learning.py:507] global step 2149: loss = 2.4025 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2150: loss = 1.7620 (0.624 sec/step)\n",
            "I0915 14:08:43.174108 139848901543808 learning.py:507] global step 2150: loss = 1.7620 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2151: loss = 1.8633 (0.625 sec/step)\n",
            "I0915 14:08:43.801178 139848901543808 learning.py:507] global step 2151: loss = 1.8633 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2152: loss = 2.0381 (0.641 sec/step)\n",
            "I0915 14:08:44.443836 139848901543808 learning.py:507] global step 2152: loss = 2.0381 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2153: loss = 1.8522 (0.658 sec/step)\n",
            "I0915 14:08:45.103589 139848901543808 learning.py:507] global step 2153: loss = 1.8522 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2154: loss = 1.7012 (0.633 sec/step)\n",
            "I0915 14:08:45.738325 139848901543808 learning.py:507] global step 2154: loss = 1.7012 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2155: loss = 2.3797 (0.633 sec/step)\n",
            "I0915 14:08:46.373916 139848901543808 learning.py:507] global step 2155: loss = 2.3797 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2156: loss = 2.1522 (0.616 sec/step)\n",
            "I0915 14:08:46.992258 139848901543808 learning.py:507] global step 2156: loss = 2.1522 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2157: loss = 1.9451 (0.613 sec/step)\n",
            "I0915 14:08:47.607588 139848901543808 learning.py:507] global step 2157: loss = 1.9451 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2158: loss = 1.9676 (0.657 sec/step)\n",
            "I0915 14:08:48.266500 139848901543808 learning.py:507] global step 2158: loss = 1.9676 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2159: loss = 1.6025 (0.627 sec/step)\n",
            "I0915 14:08:48.895373 139848901543808 learning.py:507] global step 2159: loss = 1.6025 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2160: loss = 1.7444 (0.617 sec/step)\n",
            "I0915 14:08:49.514633 139848901543808 learning.py:507] global step 2160: loss = 1.7444 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2161: loss = 1.6274 (0.645 sec/step)\n",
            "I0915 14:08:50.161475 139848901543808 learning.py:507] global step 2161: loss = 1.6274 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2162: loss = 1.3064 (0.649 sec/step)\n",
            "I0915 14:08:50.812191 139848901543808 learning.py:507] global step 2162: loss = 1.3064 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2163: loss = 2.0802 (0.653 sec/step)\n",
            "I0915 14:08:51.467418 139848901543808 learning.py:507] global step 2163: loss = 2.0802 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2164: loss = 2.0025 (0.667 sec/step)\n",
            "I0915 14:08:52.137044 139848901543808 learning.py:507] global step 2164: loss = 2.0025 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 2165: loss = 1.9230 (0.627 sec/step)\n",
            "I0915 14:08:52.766520 139848901543808 learning.py:507] global step 2165: loss = 1.9230 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2166: loss = 2.5158 (0.646 sec/step)\n",
            "I0915 14:08:53.414515 139848901543808 learning.py:507] global step 2166: loss = 2.5158 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2167: loss = 1.3271 (0.625 sec/step)\n",
            "I0915 14:08:54.041277 139848901543808 learning.py:507] global step 2167: loss = 1.3271 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2168: loss = 2.4676 (0.622 sec/step)\n",
            "I0915 14:08:54.665566 139848901543808 learning.py:507] global step 2168: loss = 2.4676 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2169: loss = 2.4180 (0.643 sec/step)\n",
            "I0915 14:08:55.310951 139848901543808 learning.py:507] global step 2169: loss = 2.4180 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2170: loss = 1.5259 (0.635 sec/step)\n",
            "I0915 14:08:55.948341 139848901543808 learning.py:507] global step 2170: loss = 1.5259 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2171: loss = 1.6283 (0.640 sec/step)\n",
            "I0915 14:08:56.590668 139848901543808 learning.py:507] global step 2171: loss = 1.6283 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2172: loss = 2.6838 (0.663 sec/step)\n",
            "I0915 14:08:57.255172 139848901543808 learning.py:507] global step 2172: loss = 2.6838 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2173: loss = 2.7069 (0.635 sec/step)\n",
            "I0915 14:08:57.892578 139848901543808 learning.py:507] global step 2173: loss = 2.7069 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2174: loss = 2.9697 (0.644 sec/step)\n",
            "I0915 14:08:58.538194 139848901543808 learning.py:507] global step 2174: loss = 2.9697 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2175: loss = 1.7461 (0.630 sec/step)\n",
            "I0915 14:08:59.170381 139848901543808 learning.py:507] global step 2175: loss = 1.7461 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2176: loss = 3.3811 (0.654 sec/step)\n",
            "I0915 14:08:59.826500 139848901543808 learning.py:507] global step 2176: loss = 3.3811 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2177: loss = 1.8697 (0.648 sec/step)\n",
            "I0915 14:09:00.477029 139848901543808 learning.py:507] global step 2177: loss = 1.8697 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2178: loss = 2.5901 (0.634 sec/step)\n",
            "I0915 14:09:01.112809 139848901543808 learning.py:507] global step 2178: loss = 2.5901 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2179: loss = 1.6518 (0.629 sec/step)\n",
            "I0915 14:09:01.743522 139848901543808 learning.py:507] global step 2179: loss = 1.6518 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2180: loss = 1.8682 (0.639 sec/step)\n",
            "I0915 14:09:02.384383 139848901543808 learning.py:507] global step 2180: loss = 1.8682 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2181: loss = 2.0864 (0.647 sec/step)\n",
            "I0915 14:09:03.033406 139848901543808 learning.py:507] global step 2181: loss = 2.0864 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2182: loss = 1.7995 (0.630 sec/step)\n",
            "I0915 14:09:03.665395 139848901543808 learning.py:507] global step 2182: loss = 1.7995 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2183: loss = 2.0459 (0.621 sec/step)\n",
            "I0915 14:09:04.288479 139848901543808 learning.py:507] global step 2183: loss = 2.0459 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2184: loss = 1.7464 (0.612 sec/step)\n",
            "I0915 14:09:04.902840 139848901543808 learning.py:507] global step 2184: loss = 1.7464 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2185: loss = 2.5411 (0.637 sec/step)\n",
            "I0915 14:09:05.541605 139848901543808 learning.py:507] global step 2185: loss = 2.5411 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2186: loss = 1.8891 (0.631 sec/step)\n",
            "I0915 14:09:06.174098 139848901543808 learning.py:507] global step 2186: loss = 1.8891 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2187: loss = 2.4926 (0.633 sec/step)\n",
            "I0915 14:09:06.809067 139848901543808 learning.py:507] global step 2187: loss = 2.4926 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2188: loss = 4.1835 (0.641 sec/step)\n",
            "I0915 14:09:07.452220 139848901543808 learning.py:507] global step 2188: loss = 4.1835 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2189: loss = 2.3137 (0.633 sec/step)\n",
            "I0915 14:09:08.087963 139848901543808 learning.py:507] global step 2189: loss = 2.3137 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2190: loss = 2.3011 (0.632 sec/step)\n",
            "I0915 14:09:08.722148 139848901543808 learning.py:507] global step 2190: loss = 2.3011 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2191: loss = 1.8037 (0.622 sec/step)\n",
            "I0915 14:09:09.345534 139848901543808 learning.py:507] global step 2191: loss = 1.8037 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2192: loss = 3.3925 (0.615 sec/step)\n",
            "I0915 14:09:09.962498 139848901543808 learning.py:507] global step 2192: loss = 3.3925 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2193: loss = 1.4640 (0.613 sec/step)\n",
            "I0915 14:09:10.577339 139848901543808 learning.py:507] global step 2193: loss = 1.4640 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2194: loss = 2.0103 (0.629 sec/step)\n",
            "I0915 14:09:11.208245 139848901543808 learning.py:507] global step 2194: loss = 2.0103 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2195: loss = 2.4253 (0.638 sec/step)\n",
            "I0915 14:09:11.848534 139848901543808 learning.py:507] global step 2195: loss = 2.4253 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2196: loss = 2.3068 (0.633 sec/step)\n",
            "I0915 14:09:12.483310 139848901543808 learning.py:507] global step 2196: loss = 2.3068 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2197: loss = 2.0016 (0.644 sec/step)\n",
            "I0915 14:09:13.129154 139848901543808 learning.py:507] global step 2197: loss = 2.0016 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2198: loss = 2.4371 (0.630 sec/step)\n",
            "I0915 14:09:13.761472 139848901543808 learning.py:507] global step 2198: loss = 2.4371 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2199: loss = 3.0686 (0.627 sec/step)\n",
            "I0915 14:09:14.392647 139848901543808 learning.py:507] global step 2199: loss = 3.0686 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2200: loss = 2.5236 (0.635 sec/step)\n",
            "I0915 14:09:15.029670 139848901543808 learning.py:507] global step 2200: loss = 2.5236 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2201: loss = 2.1024 (0.637 sec/step)\n",
            "I0915 14:09:15.669121 139848901543808 learning.py:507] global step 2201: loss = 2.1024 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2202: loss = 2.4784 (0.627 sec/step)\n",
            "I0915 14:09:16.297514 139848901543808 learning.py:507] global step 2202: loss = 2.4784 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2203: loss = 1.8383 (0.631 sec/step)\n",
            "I0915 14:09:16.930202 139848901543808 learning.py:507] global step 2203: loss = 1.8383 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2204: loss = 2.4479 (0.623 sec/step)\n",
            "I0915 14:09:17.554639 139848901543808 learning.py:507] global step 2204: loss = 2.4479 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2205: loss = 2.1180 (0.627 sec/step)\n",
            "I0915 14:09:18.183559 139848901543808 learning.py:507] global step 2205: loss = 2.1180 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2206: loss = 2.0122 (0.644 sec/step)\n",
            "I0915 14:09:18.829842 139848901543808 learning.py:507] global step 2206: loss = 2.0122 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2207: loss = 2.6727 (0.640 sec/step)\n",
            "I0915 14:09:19.472117 139848901543808 learning.py:507] global step 2207: loss = 2.6727 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2208: loss = 2.9368 (0.637 sec/step)\n",
            "I0915 14:09:20.111673 139848901543808 learning.py:507] global step 2208: loss = 2.9368 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2209: loss = 3.1219 (0.632 sec/step)\n",
            "I0915 14:09:20.745748 139848901543808 learning.py:507] global step 2209: loss = 3.1219 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2210: loss = 1.5415 (0.631 sec/step)\n",
            "I0915 14:09:21.379243 139848901543808 learning.py:507] global step 2210: loss = 1.5415 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2211: loss = 1.9308 (0.627 sec/step)\n",
            "I0915 14:09:22.008339 139848901543808 learning.py:507] global step 2211: loss = 1.9308 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2212: loss = 2.7808 (0.592 sec/step)\n",
            "I0915 14:09:22.602275 139848901543808 learning.py:507] global step 2212: loss = 2.7808 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 2213: loss = 1.9622 (0.735 sec/step)\n",
            "I0915 14:09:23.398449 139848901543808 learning.py:507] global step 2213: loss = 1.9622 (0.735 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2213.\n",
            "I0915 14:09:24.156658 139845834331904 supervisor.py:1050] Recording summary at step 2213.\n",
            "INFO:tensorflow:global step 2214: loss = 1.4968 (0.963 sec/step)\n",
            "I0915 14:09:24.398418 139848901543808 learning.py:507] global step 2214: loss = 1.4968 (0.963 sec/step)\n",
            "INFO:tensorflow:global step 2215: loss = 1.9455 (0.647 sec/step)\n",
            "I0915 14:09:25.047810 139848901543808 learning.py:507] global step 2215: loss = 1.9455 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2216: loss = 1.6852 (0.644 sec/step)\n",
            "I0915 14:09:25.693886 139848901543808 learning.py:507] global step 2216: loss = 1.6852 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2217: loss = 2.5582 (0.636 sec/step)\n",
            "I0915 14:09:26.331990 139848901543808 learning.py:507] global step 2217: loss = 2.5582 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2218: loss = 1.7540 (0.626 sec/step)\n",
            "I0915 14:09:26.960139 139848901543808 learning.py:507] global step 2218: loss = 1.7540 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2219: loss = 2.2996 (0.628 sec/step)\n",
            "I0915 14:09:27.590381 139848901543808 learning.py:507] global step 2219: loss = 2.2996 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2220: loss = 2.5437 (0.641 sec/step)\n",
            "I0915 14:09:28.233165 139848901543808 learning.py:507] global step 2220: loss = 2.5437 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2221: loss = 1.9492 (0.630 sec/step)\n",
            "I0915 14:09:28.864843 139848901543808 learning.py:507] global step 2221: loss = 1.9492 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2222: loss = 2.0097 (0.646 sec/step)\n",
            "I0915 14:09:29.512707 139848901543808 learning.py:507] global step 2222: loss = 2.0097 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2223: loss = 2.0370 (0.618 sec/step)\n",
            "I0915 14:09:30.132492 139848901543808 learning.py:507] global step 2223: loss = 2.0370 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2224: loss = 1.9601 (0.640 sec/step)\n",
            "I0915 14:09:30.774031 139848901543808 learning.py:507] global step 2224: loss = 1.9601 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2225: loss = 2.6354 (0.637 sec/step)\n",
            "I0915 14:09:31.412725 139848901543808 learning.py:507] global step 2225: loss = 2.6354 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2226: loss = 2.4494 (0.632 sec/step)\n",
            "I0915 14:09:32.046497 139848901543808 learning.py:507] global step 2226: loss = 2.4494 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2227: loss = 1.7979 (0.625 sec/step)\n",
            "I0915 14:09:32.673455 139848901543808 learning.py:507] global step 2227: loss = 1.7979 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2228: loss = 1.9021 (0.639 sec/step)\n",
            "I0915 14:09:33.313939 139848901543808 learning.py:507] global step 2228: loss = 1.9021 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2229: loss = 3.5570 (0.620 sec/step)\n",
            "I0915 14:09:33.936415 139848901543808 learning.py:507] global step 2229: loss = 3.5570 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2230: loss = 2.9128 (0.659 sec/step)\n",
            "I0915 14:09:34.597165 139848901543808 learning.py:507] global step 2230: loss = 2.9128 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2231: loss = 1.5573 (0.640 sec/step)\n",
            "I0915 14:09:35.239191 139848901543808 learning.py:507] global step 2231: loss = 1.5573 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2232: loss = 1.7409 (0.632 sec/step)\n",
            "I0915 14:09:35.872879 139848901543808 learning.py:507] global step 2232: loss = 1.7409 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2233: loss = 1.7727 (0.633 sec/step)\n",
            "I0915 14:09:36.508144 139848901543808 learning.py:507] global step 2233: loss = 1.7727 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2234: loss = 2.1216 (0.641 sec/step)\n",
            "I0915 14:09:37.151868 139848901543808 learning.py:507] global step 2234: loss = 2.1216 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2235: loss = 2.1790 (0.611 sec/step)\n",
            "I0915 14:09:37.764468 139848901543808 learning.py:507] global step 2235: loss = 2.1790 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2236: loss = 2.7903 (0.649 sec/step)\n",
            "I0915 14:09:38.415851 139848901543808 learning.py:507] global step 2236: loss = 2.7903 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2237: loss = 2.0374 (0.629 sec/step)\n",
            "I0915 14:09:39.046613 139848901543808 learning.py:507] global step 2237: loss = 2.0374 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2238: loss = 1.6531 (0.636 sec/step)\n",
            "I0915 14:09:39.684609 139848901543808 learning.py:507] global step 2238: loss = 1.6531 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2239: loss = 2.9301 (0.644 sec/step)\n",
            "I0915 14:09:40.330961 139848901543808 learning.py:507] global step 2239: loss = 2.9301 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2240: loss = 1.7470 (0.639 sec/step)\n",
            "I0915 14:09:40.972278 139848901543808 learning.py:507] global step 2240: loss = 1.7470 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2241: loss = 1.8506 (0.629 sec/step)\n",
            "I0915 14:09:41.603283 139848901543808 learning.py:507] global step 2241: loss = 1.8506 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2242: loss = 1.9618 (0.655 sec/step)\n",
            "I0915 14:09:42.259897 139848901543808 learning.py:507] global step 2242: loss = 1.9618 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2243: loss = 1.7160 (0.615 sec/step)\n",
            "I0915 14:09:42.876797 139848901543808 learning.py:507] global step 2243: loss = 1.7160 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2244: loss = 2.6681 (0.638 sec/step)\n",
            "I0915 14:09:43.516575 139848901543808 learning.py:507] global step 2244: loss = 2.6681 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2245: loss = 2.0282 (0.638 sec/step)\n",
            "I0915 14:09:44.157430 139848901543808 learning.py:507] global step 2245: loss = 2.0282 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2246: loss = 1.6664 (0.639 sec/step)\n",
            "I0915 14:09:44.798444 139848901543808 learning.py:507] global step 2246: loss = 1.6664 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2247: loss = 1.4011 (0.639 sec/step)\n",
            "I0915 14:09:45.439275 139848901543808 learning.py:507] global step 2247: loss = 1.4011 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2248: loss = 2.0482 (0.627 sec/step)\n",
            "I0915 14:09:46.067962 139848901543808 learning.py:507] global step 2248: loss = 2.0482 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2249: loss = 2.4314 (0.629 sec/step)\n",
            "I0915 14:09:46.698574 139848901543808 learning.py:507] global step 2249: loss = 2.4314 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2250: loss = 1.6881 (0.623 sec/step)\n",
            "I0915 14:09:47.325325 139848901543808 learning.py:507] global step 2250: loss = 1.6881 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2251: loss = 2.3793 (0.621 sec/step)\n",
            "I0915 14:09:47.949852 139848901543808 learning.py:507] global step 2251: loss = 2.3793 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2252: loss = 2.4737 (0.625 sec/step)\n",
            "I0915 14:09:48.576443 139848901543808 learning.py:507] global step 2252: loss = 2.4737 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2253: loss = 2.4471 (0.637 sec/step)\n",
            "I0915 14:09:49.215499 139848901543808 learning.py:507] global step 2253: loss = 2.4471 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2254: loss = 1.2818 (0.624 sec/step)\n",
            "I0915 14:09:49.841641 139848901543808 learning.py:507] global step 2254: loss = 1.2818 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2255: loss = 3.0380 (0.658 sec/step)\n",
            "I0915 14:09:50.502582 139848901543808 learning.py:507] global step 2255: loss = 3.0380 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2256: loss = 2.1553 (0.637 sec/step)\n",
            "I0915 14:09:51.143934 139848901543808 learning.py:507] global step 2256: loss = 2.1553 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2257: loss = 1.9133 (0.633 sec/step)\n",
            "I0915 14:09:51.778773 139848901543808 learning.py:507] global step 2257: loss = 1.9133 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2258: loss = 1.7662 (0.633 sec/step)\n",
            "I0915 14:09:52.413919 139848901543808 learning.py:507] global step 2258: loss = 1.7662 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2259: loss = 2.3597 (0.637 sec/step)\n",
            "I0915 14:09:53.053219 139848901543808 learning.py:507] global step 2259: loss = 2.3597 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2260: loss = 2.1035 (0.625 sec/step)\n",
            "I0915 14:09:53.680340 139848901543808 learning.py:507] global step 2260: loss = 2.1035 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2261: loss = 2.1107 (0.620 sec/step)\n",
            "I0915 14:09:54.302971 139848901543808 learning.py:507] global step 2261: loss = 2.1107 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2262: loss = 2.7665 (0.643 sec/step)\n",
            "I0915 14:09:54.948556 139848901543808 learning.py:507] global step 2262: loss = 2.7665 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2263: loss = 2.2407 (0.640 sec/step)\n",
            "I0915 14:09:55.590792 139848901543808 learning.py:507] global step 2263: loss = 2.2407 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2264: loss = 2.8630 (0.646 sec/step)\n",
            "I0915 14:09:56.239028 139848901543808 learning.py:507] global step 2264: loss = 2.8630 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2265: loss = 2.2308 (0.647 sec/step)\n",
            "I0915 14:09:56.888247 139848901543808 learning.py:507] global step 2265: loss = 2.2308 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2266: loss = 2.2811 (0.624 sec/step)\n",
            "I0915 14:09:57.513822 139848901543808 learning.py:507] global step 2266: loss = 2.2811 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2267: loss = 1.6893 (0.630 sec/step)\n",
            "I0915 14:09:58.146048 139848901543808 learning.py:507] global step 2267: loss = 1.6893 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2268: loss = 2.6853 (0.616 sec/step)\n",
            "I0915 14:09:58.764498 139848901543808 learning.py:507] global step 2268: loss = 2.6853 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2269: loss = 2.0436 (0.628 sec/step)\n",
            "I0915 14:09:59.394466 139848901543808 learning.py:507] global step 2269: loss = 2.0436 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2270: loss = 2.0931 (0.656 sec/step)\n",
            "I0915 14:10:00.052438 139848901543808 learning.py:507] global step 2270: loss = 2.0931 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2271: loss = 1.3054 (0.650 sec/step)\n",
            "I0915 14:10:00.704263 139848901543808 learning.py:507] global step 2271: loss = 1.3054 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2272: loss = 2.1086 (0.622 sec/step)\n",
            "I0915 14:10:01.329177 139848901543808 learning.py:507] global step 2272: loss = 2.1086 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2273: loss = 2.4880 (0.625 sec/step)\n",
            "I0915 14:10:01.955837 139848901543808 learning.py:507] global step 2273: loss = 2.4880 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2274: loss = 2.7992 (0.630 sec/step)\n",
            "I0915 14:10:02.587695 139848901543808 learning.py:507] global step 2274: loss = 2.7992 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2275: loss = 2.2384 (0.610 sec/step)\n",
            "I0915 14:10:03.199781 139848901543808 learning.py:507] global step 2275: loss = 2.2384 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2276: loss = 2.1119 (0.628 sec/step)\n",
            "I0915 14:10:03.829684 139848901543808 learning.py:507] global step 2276: loss = 2.1119 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2277: loss = 1.6224 (0.657 sec/step)\n",
            "I0915 14:10:04.488273 139848901543808 learning.py:507] global step 2277: loss = 1.6224 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2278: loss = 2.0777 (0.627 sec/step)\n",
            "I0915 14:10:05.116920 139848901543808 learning.py:507] global step 2278: loss = 2.0777 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2279: loss = 1.6146 (0.626 sec/step)\n",
            "I0915 14:10:05.744787 139848901543808 learning.py:507] global step 2279: loss = 1.6146 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2280: loss = 2.1811 (0.652 sec/step)\n",
            "I0915 14:10:06.398122 139848901543808 learning.py:507] global step 2280: loss = 2.1811 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2281: loss = 2.0578 (0.612 sec/step)\n",
            "I0915 14:10:07.011851 139848901543808 learning.py:507] global step 2281: loss = 2.0578 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2282: loss = 2.3681 (0.642 sec/step)\n",
            "I0915 14:10:07.656333 139848901543808 learning.py:507] global step 2282: loss = 2.3681 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2283: loss = 1.6065 (0.627 sec/step)\n",
            "I0915 14:10:08.285021 139848901543808 learning.py:507] global step 2283: loss = 1.6065 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2284: loss = 3.8287 (0.625 sec/step)\n",
            "I0915 14:10:08.911645 139848901543808 learning.py:507] global step 2284: loss = 3.8287 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2285: loss = 2.5084 (0.634 sec/step)\n",
            "I0915 14:10:09.547618 139848901543808 learning.py:507] global step 2285: loss = 2.5084 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2286: loss = 1.7318 (0.633 sec/step)\n",
            "I0915 14:10:10.182636 139848901543808 learning.py:507] global step 2286: loss = 1.7318 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2287: loss = 1.3877 (0.626 sec/step)\n",
            "I0915 14:10:10.810178 139848901543808 learning.py:507] global step 2287: loss = 1.3877 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2288: loss = 2.5222 (0.638 sec/step)\n",
            "I0915 14:10:11.450779 139848901543808 learning.py:507] global step 2288: loss = 2.5222 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2289: loss = 1.9082 (0.633 sec/step)\n",
            "I0915 14:10:12.086406 139848901543808 learning.py:507] global step 2289: loss = 1.9082 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2290: loss = 2.5729 (0.613 sec/step)\n",
            "I0915 14:10:12.700886 139848901543808 learning.py:507] global step 2290: loss = 2.5729 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2291: loss = 2.0201 (0.635 sec/step)\n",
            "I0915 14:10:13.338014 139848901543808 learning.py:507] global step 2291: loss = 2.0201 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2292: loss = 1.6619 (0.650 sec/step)\n",
            "I0915 14:10:13.990338 139848901543808 learning.py:507] global step 2292: loss = 1.6619 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2293: loss = 1.7241 (0.658 sec/step)\n",
            "I0915 14:10:14.650625 139848901543808 learning.py:507] global step 2293: loss = 1.7241 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2294: loss = 1.7314 (0.649 sec/step)\n",
            "I0915 14:10:15.301640 139848901543808 learning.py:507] global step 2294: loss = 1.7314 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2295: loss = 2.1385 (0.630 sec/step)\n",
            "I0915 14:10:15.933993 139848901543808 learning.py:507] global step 2295: loss = 2.1385 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2296: loss = 1.3888 (0.643 sec/step)\n",
            "I0915 14:10:16.578308 139848901543808 learning.py:507] global step 2296: loss = 1.3888 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2297: loss = 2.0232 (0.624 sec/step)\n",
            "I0915 14:10:17.203663 139848901543808 learning.py:507] global step 2297: loss = 2.0232 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2298: loss = 2.3865 (0.622 sec/step)\n",
            "I0915 14:10:17.827431 139848901543808 learning.py:507] global step 2298: loss = 2.3865 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2299: loss = 1.8497 (0.621 sec/step)\n",
            "I0915 14:10:18.449873 139848901543808 learning.py:507] global step 2299: loss = 1.8497 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2300: loss = 2.2887 (0.611 sec/step)\n",
            "I0915 14:10:19.062294 139848901543808 learning.py:507] global step 2300: loss = 2.2887 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2301: loss = 2.4349 (0.645 sec/step)\n",
            "I0915 14:10:19.708934 139848901543808 learning.py:507] global step 2301: loss = 2.4349 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2302: loss = 2.9076 (0.646 sec/step)\n",
            "I0915 14:10:20.356437 139848901543808 learning.py:507] global step 2302: loss = 2.9076 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2303: loss = 2.3722 (0.613 sec/step)\n",
            "I0915 14:10:20.971270 139848901543808 learning.py:507] global step 2303: loss = 2.3722 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2304: loss = 2.3505 (0.628 sec/step)\n",
            "I0915 14:10:21.601715 139848901543808 learning.py:507] global step 2304: loss = 2.3505 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2305: loss = 3.2037 (0.641 sec/step)\n",
            "I0915 14:10:22.245473 139848901543808 learning.py:507] global step 2305: loss = 3.2037 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2306: loss = 2.0072 (0.666 sec/step)\n",
            "I0915 14:10:22.913735 139848901543808 learning.py:507] global step 2306: loss = 2.0072 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 2307: loss = 2.2965 (0.620 sec/step)\n",
            "I0915 14:10:23.536953 139848901543808 learning.py:507] global step 2307: loss = 2.2965 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2308: loss = 3.1198 (0.620 sec/step)\n",
            "I0915 14:10:24.158885 139848901543808 learning.py:507] global step 2308: loss = 3.1198 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2309: loss = 2.0503 (0.657 sec/step)\n",
            "I0915 14:10:24.818106 139848901543808 learning.py:507] global step 2309: loss = 2.0503 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2310: loss = 2.0655 (0.624 sec/step)\n",
            "I0915 14:10:25.444009 139848901543808 learning.py:507] global step 2310: loss = 2.0655 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2311: loss = 1.7678 (0.650 sec/step)\n",
            "I0915 14:10:26.096820 139848901543808 learning.py:507] global step 2311: loss = 1.7678 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2312: loss = 2.1030 (0.627 sec/step)\n",
            "I0915 14:10:26.726039 139848901543808 learning.py:507] global step 2312: loss = 2.1030 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2313: loss = 2.6028 (0.619 sec/step)\n",
            "I0915 14:10:27.346810 139848901543808 learning.py:507] global step 2313: loss = 2.6028 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2314: loss = 1.8591 (0.645 sec/step)\n",
            "I0915 14:10:27.994341 139848901543808 learning.py:507] global step 2314: loss = 1.8591 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2315: loss = 1.8836 (0.630 sec/step)\n",
            "I0915 14:10:28.626273 139848901543808 learning.py:507] global step 2315: loss = 1.8836 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2316: loss = 1.5594 (0.633 sec/step)\n",
            "I0915 14:10:29.261265 139848901543808 learning.py:507] global step 2316: loss = 1.5594 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2317: loss = 1.0399 (0.654 sec/step)\n",
            "I0915 14:10:29.917381 139848901543808 learning.py:507] global step 2317: loss = 1.0399 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2318: loss = 1.7498 (0.638 sec/step)\n",
            "I0915 14:10:30.557854 139848901543808 learning.py:507] global step 2318: loss = 1.7498 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2319: loss = 2.6522 (0.612 sec/step)\n",
            "I0915 14:10:31.171366 139848901543808 learning.py:507] global step 2319: loss = 2.6522 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2320: loss = 1.9741 (0.650 sec/step)\n",
            "I0915 14:10:31.823701 139848901543808 learning.py:507] global step 2320: loss = 1.9741 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2321: loss = 1.7425 (0.631 sec/step)\n",
            "I0915 14:10:32.456878 139848901543808 learning.py:507] global step 2321: loss = 1.7425 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2322: loss = 0.8255 (0.628 sec/step)\n",
            "I0915 14:10:33.087041 139848901543808 learning.py:507] global step 2322: loss = 0.8255 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2323: loss = 1.8496 (0.617 sec/step)\n",
            "I0915 14:10:33.705368 139848901543808 learning.py:507] global step 2323: loss = 1.8496 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2324: loss = 1.9494 (0.616 sec/step)\n",
            "I0915 14:10:34.323238 139848901543808 learning.py:507] global step 2324: loss = 1.9494 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2325: loss = 2.0668 (0.635 sec/step)\n",
            "I0915 14:10:34.960900 139848901543808 learning.py:507] global step 2325: loss = 2.0668 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2326: loss = 2.0423 (0.647 sec/step)\n",
            "I0915 14:10:35.610197 139848901543808 learning.py:507] global step 2326: loss = 2.0423 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2327: loss = 1.5933 (0.650 sec/step)\n",
            "I0915 14:10:36.261669 139848901543808 learning.py:507] global step 2327: loss = 1.5933 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2328: loss = 1.6057 (0.607 sec/step)\n",
            "I0915 14:10:36.870319 139848901543808 learning.py:507] global step 2328: loss = 1.6057 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2329: loss = 2.3066 (0.645 sec/step)\n",
            "I0915 14:10:37.517590 139848901543808 learning.py:507] global step 2329: loss = 2.3066 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2330: loss = 2.2590 (0.671 sec/step)\n",
            "I0915 14:10:38.190298 139848901543808 learning.py:507] global step 2330: loss = 2.2590 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 2331: loss = 2.1350 (0.625 sec/step)\n",
            "I0915 14:10:38.816736 139848901543808 learning.py:507] global step 2331: loss = 2.1350 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2332: loss = 1.8432 (0.639 sec/step)\n",
            "I0915 14:10:39.457635 139848901543808 learning.py:507] global step 2332: loss = 1.8432 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2333: loss = 1.7761 (0.640 sec/step)\n",
            "I0915 14:10:40.099272 139848901543808 learning.py:507] global step 2333: loss = 1.7761 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2334: loss = 2.2726 (0.638 sec/step)\n",
            "I0915 14:10:40.739666 139848901543808 learning.py:507] global step 2334: loss = 2.2726 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2335: loss = 2.4758 (0.631 sec/step)\n",
            "I0915 14:10:41.373092 139848901543808 learning.py:507] global step 2335: loss = 2.4758 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2336: loss = 2.3473 (0.647 sec/step)\n",
            "I0915 14:10:42.021704 139848901543808 learning.py:507] global step 2336: loss = 2.3473 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2337: loss = 2.7499 (0.625 sec/step)\n",
            "I0915 14:10:42.648577 139848901543808 learning.py:507] global step 2337: loss = 2.7499 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2338: loss = 1.1717 (0.643 sec/step)\n",
            "I0915 14:10:43.294261 139848901543808 learning.py:507] global step 2338: loss = 1.1717 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2339: loss = 2.0681 (0.630 sec/step)\n",
            "I0915 14:10:43.925696 139848901543808 learning.py:507] global step 2339: loss = 2.0681 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2340: loss = 1.8539 (0.609 sec/step)\n",
            "I0915 14:10:44.536503 139848901543808 learning.py:507] global step 2340: loss = 1.8539 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2341: loss = 1.6332 (0.636 sec/step)\n",
            "I0915 14:10:45.174470 139848901543808 learning.py:507] global step 2341: loss = 1.6332 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2342: loss = 1.6993 (0.659 sec/step)\n",
            "I0915 14:10:45.835608 139848901543808 learning.py:507] global step 2342: loss = 1.6993 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2343: loss = 2.1675 (0.641 sec/step)\n",
            "I0915 14:10:46.478322 139848901543808 learning.py:507] global step 2343: loss = 2.1675 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2344: loss = 2.3894 (0.601 sec/step)\n",
            "I0915 14:10:47.081510 139848901543808 learning.py:507] global step 2344: loss = 2.3894 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 2345: loss = 1.5056 (0.640 sec/step)\n",
            "I0915 14:10:47.723066 139848901543808 learning.py:507] global step 2345: loss = 1.5056 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2346: loss = 2.9029 (0.652 sec/step)\n",
            "I0915 14:10:48.377421 139848901543808 learning.py:507] global step 2346: loss = 2.9029 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2347: loss = 1.6374 (0.645 sec/step)\n",
            "I0915 14:10:49.024224 139848901543808 learning.py:507] global step 2347: loss = 1.6374 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2348: loss = 2.0699 (0.606 sec/step)\n",
            "I0915 14:10:49.631754 139848901543808 learning.py:507] global step 2348: loss = 2.0699 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 2349: loss = 1.3948 (0.638 sec/step)\n",
            "I0915 14:10:50.272267 139848901543808 learning.py:507] global step 2349: loss = 1.3948 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2350: loss = 2.2053 (0.658 sec/step)\n",
            "I0915 14:10:50.932376 139848901543808 learning.py:507] global step 2350: loss = 2.2053 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2351: loss = 2.0081 (0.663 sec/step)\n",
            "I0915 14:10:51.597498 139848901543808 learning.py:507] global step 2351: loss = 2.0081 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2352: loss = 2.0059 (0.638 sec/step)\n",
            "I0915 14:10:52.237331 139848901543808 learning.py:507] global step 2352: loss = 2.0059 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2353: loss = 2.3682 (0.627 sec/step)\n",
            "I0915 14:10:52.866045 139848901543808 learning.py:507] global step 2353: loss = 2.3682 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2354: loss = 1.9809 (0.649 sec/step)\n",
            "I0915 14:10:53.517230 139848901543808 learning.py:507] global step 2354: loss = 1.9809 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2355: loss = 2.8430 (0.635 sec/step)\n",
            "I0915 14:10:54.154211 139848901543808 learning.py:507] global step 2355: loss = 2.8430 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2356: loss = 2.4089 (0.632 sec/step)\n",
            "I0915 14:10:54.788183 139848901543808 learning.py:507] global step 2356: loss = 2.4089 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2357: loss = 2.6080 (0.659 sec/step)\n",
            "I0915 14:10:55.448944 139848901543808 learning.py:507] global step 2357: loss = 2.6080 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2358: loss = 2.2305 (0.666 sec/step)\n",
            "I0915 14:10:56.117261 139848901543808 learning.py:507] global step 2358: loss = 2.2305 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 2359: loss = 3.4377 (0.656 sec/step)\n",
            "I0915 14:10:56.775451 139848901543808 learning.py:507] global step 2359: loss = 3.4377 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2360: loss = 1.4006 (0.637 sec/step)\n",
            "I0915 14:10:57.414900 139848901543808 learning.py:507] global step 2360: loss = 1.4006 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2361: loss = 2.5555 (0.615 sec/step)\n",
            "I0915 14:10:58.031877 139848901543808 learning.py:507] global step 2361: loss = 2.5555 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2362: loss = 2.5103 (0.636 sec/step)\n",
            "I0915 14:10:58.669937 139848901543808 learning.py:507] global step 2362: loss = 2.5103 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2363: loss = 1.5111 (0.641 sec/step)\n",
            "I0915 14:10:59.313032 139848901543808 learning.py:507] global step 2363: loss = 1.5111 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2364: loss = 1.9830 (0.642 sec/step)\n",
            "I0915 14:10:59.957241 139848901543808 learning.py:507] global step 2364: loss = 1.9830 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2365: loss = 2.6798 (0.647 sec/step)\n",
            "I0915 14:11:00.605924 139848901543808 learning.py:507] global step 2365: loss = 2.6798 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2366: loss = 1.9485 (0.629 sec/step)\n",
            "I0915 14:11:01.236642 139848901543808 learning.py:507] global step 2366: loss = 1.9485 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2367: loss = 2.5299 (0.644 sec/step)\n",
            "I0915 14:11:01.882856 139848901543808 learning.py:507] global step 2367: loss = 2.5299 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2368: loss = 3.0819 (0.638 sec/step)\n",
            "I0915 14:11:02.523487 139848901543808 learning.py:507] global step 2368: loss = 3.0819 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2369: loss = 1.8151 (0.634 sec/step)\n",
            "I0915 14:11:03.159524 139848901543808 learning.py:507] global step 2369: loss = 1.8151 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2370: loss = 2.0314 (0.636 sec/step)\n",
            "I0915 14:11:03.797691 139848901543808 learning.py:507] global step 2370: loss = 2.0314 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2371: loss = 2.3933 (0.617 sec/step)\n",
            "I0915 14:11:04.416334 139848901543808 learning.py:507] global step 2371: loss = 2.3933 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2372: loss = 1.9614 (0.620 sec/step)\n",
            "I0915 14:11:05.038229 139848901543808 learning.py:507] global step 2372: loss = 1.9614 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2373: loss = 2.1525 (0.625 sec/step)\n",
            "I0915 14:11:05.664635 139848901543808 learning.py:507] global step 2373: loss = 2.1525 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2374: loss = 1.3861 (0.648 sec/step)\n",
            "I0915 14:11:06.314371 139848901543808 learning.py:507] global step 2374: loss = 1.3861 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2375: loss = 2.3644 (0.624 sec/step)\n",
            "I0915 14:11:06.940772 139848901543808 learning.py:507] global step 2375: loss = 2.3644 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2376: loss = 2.8524 (0.602 sec/step)\n",
            "I0915 14:11:07.545264 139848901543808 learning.py:507] global step 2376: loss = 2.8524 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2377: loss = 1.8556 (0.654 sec/step)\n",
            "I0915 14:11:08.201131 139848901543808 learning.py:507] global step 2377: loss = 1.8556 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2378: loss = 1.8235 (0.656 sec/step)\n",
            "I0915 14:11:08.859494 139848901543808 learning.py:507] global step 2378: loss = 1.8235 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2379: loss = 2.2949 (0.647 sec/step)\n",
            "I0915 14:11:09.508927 139848901543808 learning.py:507] global step 2379: loss = 2.2949 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2380: loss = 2.0031 (0.614 sec/step)\n",
            "I0915 14:11:10.125073 139848901543808 learning.py:507] global step 2380: loss = 2.0031 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2381: loss = 2.4274 (0.605 sec/step)\n",
            "I0915 14:11:10.731633 139848901543808 learning.py:507] global step 2381: loss = 2.4274 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2382: loss = 1.4742 (0.638 sec/step)\n",
            "I0915 14:11:11.372025 139848901543808 learning.py:507] global step 2382: loss = 1.4742 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2383: loss = 2.6838 (0.662 sec/step)\n",
            "I0915 14:11:12.036216 139848901543808 learning.py:507] global step 2383: loss = 2.6838 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2384: loss = 2.2946 (0.655 sec/step)\n",
            "I0915 14:11:12.692942 139848901543808 learning.py:507] global step 2384: loss = 2.2946 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2385: loss = 2.4857 (0.645 sec/step)\n",
            "I0915 14:11:13.339681 139848901543808 learning.py:507] global step 2385: loss = 2.4857 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2386: loss = 1.7053 (0.618 sec/step)\n",
            "I0915 14:11:13.959877 139848901543808 learning.py:507] global step 2386: loss = 1.7053 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2387: loss = 2.4824 (0.617 sec/step)\n",
            "I0915 14:11:14.578689 139848901543808 learning.py:507] global step 2387: loss = 2.4824 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2388: loss = 1.5932 (0.625 sec/step)\n",
            "I0915 14:11:15.205332 139848901543808 learning.py:507] global step 2388: loss = 1.5932 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2389: loss = 2.3186 (0.627 sec/step)\n",
            "I0915 14:11:15.834070 139848901543808 learning.py:507] global step 2389: loss = 2.3186 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2390: loss = 1.1056 (0.660 sec/step)\n",
            "I0915 14:11:16.496542 139848901543808 learning.py:507] global step 2390: loss = 1.1056 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2391: loss = 2.4477 (0.620 sec/step)\n",
            "I0915 14:11:17.118248 139848901543808 learning.py:507] global step 2391: loss = 2.4477 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2392: loss = 2.2265 (0.637 sec/step)\n",
            "I0915 14:11:17.757172 139848901543808 learning.py:507] global step 2392: loss = 2.2265 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2393: loss = 1.9718 (0.661 sec/step)\n",
            "I0915 14:11:18.419953 139848901543808 learning.py:507] global step 2393: loss = 1.9718 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2394: loss = 2.6299 (0.644 sec/step)\n",
            "I0915 14:11:19.066547 139848901543808 learning.py:507] global step 2394: loss = 2.6299 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2395: loss = 2.1699 (0.649 sec/step)\n",
            "I0915 14:11:19.718161 139848901543808 learning.py:507] global step 2395: loss = 2.1699 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2396: loss = 1.6685 (0.626 sec/step)\n",
            "I0915 14:11:20.345953 139848901543808 learning.py:507] global step 2396: loss = 1.6685 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2397: loss = 1.7499 (0.625 sec/step)\n",
            "I0915 14:11:20.973581 139848901543808 learning.py:507] global step 2397: loss = 1.7499 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2398: loss = 2.8763 (0.608 sec/step)\n",
            "I0915 14:11:21.584181 139848901543808 learning.py:507] global step 2398: loss = 2.8763 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2399: loss = 2.2947 (0.636 sec/step)\n",
            "I0915 14:11:22.222684 139848901543808 learning.py:507] global step 2399: loss = 2.2947 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2400: loss = 1.1464 (0.610 sec/step)\n",
            "I0915 14:11:22.834894 139848901543808 learning.py:507] global step 2400: loss = 1.1464 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2401: loss = 1.6885 (0.807 sec/step)\n",
            "I0915 14:11:23.838646 139848901543808 learning.py:507] global step 2401: loss = 1.6885 (0.807 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2401.\n",
            "I0915 14:11:24.061457 139845834331904 supervisor.py:1050] Recording summary at step 2401.\n",
            "INFO:tensorflow:global step 2402: loss = 2.4467 (0.706 sec/step)\n",
            "I0915 14:11:24.547316 139848901543808 learning.py:507] global step 2402: loss = 2.4467 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 2403: loss = 1.9136 (0.612 sec/step)\n",
            "I0915 14:11:25.161678 139848901543808 learning.py:507] global step 2403: loss = 1.9136 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2404: loss = 2.1057 (0.623 sec/step)\n",
            "I0915 14:11:25.786882 139848901543808 learning.py:507] global step 2404: loss = 2.1057 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2405: loss = 1.7296 (0.639 sec/step)\n",
            "I0915 14:11:26.427598 139848901543808 learning.py:507] global step 2405: loss = 1.7296 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2406: loss = 1.9905 (0.646 sec/step)\n",
            "I0915 14:11:27.075629 139848901543808 learning.py:507] global step 2406: loss = 1.9905 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2407: loss = 1.4713 (0.636 sec/step)\n",
            "I0915 14:11:27.713964 139848901543808 learning.py:507] global step 2407: loss = 1.4713 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2408: loss = 1.4988 (0.662 sec/step)\n",
            "I0915 14:11:28.377622 139848901543808 learning.py:507] global step 2408: loss = 1.4988 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2409: loss = 3.4980 (0.613 sec/step)\n",
            "I0915 14:11:28.992403 139848901543808 learning.py:507] global step 2409: loss = 3.4980 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2410: loss = 2.7452 (0.630 sec/step)\n",
            "I0915 14:11:29.623922 139848901543808 learning.py:507] global step 2410: loss = 2.7452 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2411: loss = 1.8749 (0.622 sec/step)\n",
            "I0915 14:11:30.247706 139848901543808 learning.py:507] global step 2411: loss = 1.8749 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2412: loss = 2.8662 (0.623 sec/step)\n",
            "I0915 14:11:30.871937 139848901543808 learning.py:507] global step 2412: loss = 2.8662 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2413: loss = 1.3491 (0.635 sec/step)\n",
            "I0915 14:11:31.508702 139848901543808 learning.py:507] global step 2413: loss = 1.3491 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2414: loss = 1.4924 (0.624 sec/step)\n",
            "I0915 14:11:32.134290 139848901543808 learning.py:507] global step 2414: loss = 1.4924 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2415: loss = 2.5630 (0.640 sec/step)\n",
            "I0915 14:11:32.776045 139848901543808 learning.py:507] global step 2415: loss = 2.5630 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2416: loss = 2.2702 (0.641 sec/step)\n",
            "I0915 14:11:33.419996 139848901543808 learning.py:507] global step 2416: loss = 2.2702 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2417: loss = 1.5645 (0.630 sec/step)\n",
            "I0915 14:11:34.052132 139848901543808 learning.py:507] global step 2417: loss = 1.5645 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2418: loss = 2.2671 (0.645 sec/step)\n",
            "I0915 14:11:34.699110 139848901543808 learning.py:507] global step 2418: loss = 2.2671 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2419: loss = 1.8867 (0.623 sec/step)\n",
            "I0915 14:11:35.323894 139848901543808 learning.py:507] global step 2419: loss = 1.8867 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2420: loss = 2.4006 (0.641 sec/step)\n",
            "I0915 14:11:35.966772 139848901543808 learning.py:507] global step 2420: loss = 2.4006 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2421: loss = 1.1267 (0.621 sec/step)\n",
            "I0915 14:11:36.589716 139848901543808 learning.py:507] global step 2421: loss = 1.1267 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2422: loss = 2.0179 (0.654 sec/step)\n",
            "I0915 14:11:37.245570 139848901543808 learning.py:507] global step 2422: loss = 2.0179 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2423: loss = 1.8067 (0.658 sec/step)\n",
            "I0915 14:11:37.905128 139848901543808 learning.py:507] global step 2423: loss = 1.8067 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2424: loss = 1.9624 (0.640 sec/step)\n",
            "I0915 14:11:38.547312 139848901543808 learning.py:507] global step 2424: loss = 1.9624 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2425: loss = 1.9750 (0.642 sec/step)\n",
            "I0915 14:11:39.191492 139848901543808 learning.py:507] global step 2425: loss = 1.9750 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2426: loss = 2.0763 (0.657 sec/step)\n",
            "I0915 14:11:39.850554 139848901543808 learning.py:507] global step 2426: loss = 2.0763 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2427: loss = 3.0920 (0.638 sec/step)\n",
            "I0915 14:11:40.491508 139848901543808 learning.py:507] global step 2427: loss = 3.0920 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2428: loss = 1.6563 (0.641 sec/step)\n",
            "I0915 14:11:41.134267 139848901543808 learning.py:507] global step 2428: loss = 1.6563 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2429: loss = 1.5112 (0.678 sec/step)\n",
            "I0915 14:11:41.814417 139848901543808 learning.py:507] global step 2429: loss = 1.5112 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2430: loss = 1.2351 (0.623 sec/step)\n",
            "I0915 14:11:42.439250 139848901543808 learning.py:507] global step 2430: loss = 1.2351 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2431: loss = 1.7457 (0.645 sec/step)\n",
            "I0915 14:11:43.086652 139848901543808 learning.py:507] global step 2431: loss = 1.7457 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2432: loss = 1.4418 (0.633 sec/step)\n",
            "I0915 14:11:43.721302 139848901543808 learning.py:507] global step 2432: loss = 1.4418 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2433: loss = 2.5788 (0.655 sec/step)\n",
            "I0915 14:11:44.378554 139848901543808 learning.py:507] global step 2433: loss = 2.5788 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2434: loss = 1.7185 (0.624 sec/step)\n",
            "I0915 14:11:45.004252 139848901543808 learning.py:507] global step 2434: loss = 1.7185 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2435: loss = 2.0659 (0.639 sec/step)\n",
            "I0915 14:11:45.644993 139848901543808 learning.py:507] global step 2435: loss = 2.0659 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2436: loss = 1.8866 (0.661 sec/step)\n",
            "I0915 14:11:46.308280 139848901543808 learning.py:507] global step 2436: loss = 1.8866 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2437: loss = 2.1097 (0.628 sec/step)\n",
            "I0915 14:11:46.938209 139848901543808 learning.py:507] global step 2437: loss = 2.1097 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2438: loss = 2.5514 (0.650 sec/step)\n",
            "I0915 14:11:47.590417 139848901543808 learning.py:507] global step 2438: loss = 2.5514 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2439: loss = 1.8674 (0.628 sec/step)\n",
            "I0915 14:11:48.220715 139848901543808 learning.py:507] global step 2439: loss = 1.8674 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2440: loss = 1.6354 (0.631 sec/step)\n",
            "I0915 14:11:48.853529 139848901543808 learning.py:507] global step 2440: loss = 1.6354 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2441: loss = 2.3630 (0.631 sec/step)\n",
            "I0915 14:11:49.486423 139848901543808 learning.py:507] global step 2441: loss = 2.3630 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2442: loss = 1.7807 (0.628 sec/step)\n",
            "I0915 14:11:50.116237 139848901543808 learning.py:507] global step 2442: loss = 1.7807 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2443: loss = 2.2115 (0.613 sec/step)\n",
            "I0915 14:11:50.731783 139848901543808 learning.py:507] global step 2443: loss = 2.2115 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2444: loss = 2.0701 (0.635 sec/step)\n",
            "I0915 14:11:51.368558 139848901543808 learning.py:507] global step 2444: loss = 2.0701 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2445: loss = 2.0442 (0.640 sec/step)\n",
            "I0915 14:11:52.010797 139848901543808 learning.py:507] global step 2445: loss = 2.0442 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2446: loss = 3.5101 (0.622 sec/step)\n",
            "I0915 14:11:52.635168 139848901543808 learning.py:507] global step 2446: loss = 3.5101 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2447: loss = 1.5900 (0.638 sec/step)\n",
            "I0915 14:11:53.275701 139848901543808 learning.py:507] global step 2447: loss = 1.5900 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2448: loss = 1.6925 (0.654 sec/step)\n",
            "I0915 14:11:53.933023 139848901543808 learning.py:507] global step 2448: loss = 1.6925 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2449: loss = 3.0644 (0.630 sec/step)\n",
            "I0915 14:11:54.565219 139848901543808 learning.py:507] global step 2449: loss = 3.0644 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2450: loss = 2.2049 (0.612 sec/step)\n",
            "I0915 14:11:55.179337 139848901543808 learning.py:507] global step 2450: loss = 2.2049 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2451: loss = 1.9232 (0.642 sec/step)\n",
            "I0915 14:11:55.823164 139848901543808 learning.py:507] global step 2451: loss = 1.9232 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2452: loss = 2.1272 (0.632 sec/step)\n",
            "I0915 14:11:56.457432 139848901543808 learning.py:507] global step 2452: loss = 2.1272 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2453: loss = 2.3389 (0.622 sec/step)\n",
            "I0915 14:11:57.081069 139848901543808 learning.py:507] global step 2453: loss = 2.3389 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2454: loss = 1.8788 (0.669 sec/step)\n",
            "I0915 14:11:57.752380 139848901543808 learning.py:507] global step 2454: loss = 1.8788 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 2455: loss = 2.0411 (0.633 sec/step)\n",
            "I0915 14:11:58.387259 139848901543808 learning.py:507] global step 2455: loss = 2.0411 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2456: loss = 3.8286 (0.632 sec/step)\n",
            "I0915 14:11:59.021470 139848901543808 learning.py:507] global step 2456: loss = 3.8286 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2457: loss = 1.6476 (0.625 sec/step)\n",
            "I0915 14:11:59.648393 139848901543808 learning.py:507] global step 2457: loss = 1.6476 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2458: loss = 3.1335 (0.610 sec/step)\n",
            "I0915 14:12:00.260434 139848901543808 learning.py:507] global step 2458: loss = 3.1335 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2459: loss = 1.8872 (0.624 sec/step)\n",
            "I0915 14:12:00.886278 139848901543808 learning.py:507] global step 2459: loss = 1.8872 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2460: loss = 1.8448 (0.632 sec/step)\n",
            "I0915 14:12:01.520484 139848901543808 learning.py:507] global step 2460: loss = 1.8448 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2461: loss = 1.9076 (0.628 sec/step)\n",
            "I0915 14:12:02.150390 139848901543808 learning.py:507] global step 2461: loss = 1.9076 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2462: loss = 2.0507 (0.626 sec/step)\n",
            "I0915 14:12:02.778202 139848901543808 learning.py:507] global step 2462: loss = 2.0507 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2463: loss = 2.8012 (0.641 sec/step)\n",
            "I0915 14:12:03.420958 139848901543808 learning.py:507] global step 2463: loss = 2.8012 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2464: loss = 1.8233 (0.625 sec/step)\n",
            "I0915 14:12:04.048231 139848901543808 learning.py:507] global step 2464: loss = 1.8233 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2465: loss = 1.7453 (0.648 sec/step)\n",
            "I0915 14:12:04.697774 139848901543808 learning.py:507] global step 2465: loss = 1.7453 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2466: loss = 1.8611 (0.608 sec/step)\n",
            "I0915 14:12:05.307486 139848901543808 learning.py:507] global step 2466: loss = 1.8611 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2467: loss = 2.8143 (0.637 sec/step)\n",
            "I0915 14:12:05.946533 139848901543808 learning.py:507] global step 2467: loss = 2.8143 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2468: loss = 1.2932 (0.638 sec/step)\n",
            "I0915 14:12:06.585974 139848901543808 learning.py:507] global step 2468: loss = 1.2932 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2469: loss = 2.0563 (0.633 sec/step)\n",
            "I0915 14:12:07.221268 139848901543808 learning.py:507] global step 2469: loss = 2.0563 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2470: loss = 1.6011 (0.632 sec/step)\n",
            "I0915 14:12:07.855434 139848901543808 learning.py:507] global step 2470: loss = 1.6011 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2471: loss = 2.4704 (0.661 sec/step)\n",
            "I0915 14:12:08.519131 139848901543808 learning.py:507] global step 2471: loss = 2.4704 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2472: loss = 1.9914 (0.639 sec/step)\n",
            "I0915 14:12:09.159703 139848901543808 learning.py:507] global step 2472: loss = 1.9914 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2473: loss = 1.2038 (0.643 sec/step)\n",
            "I0915 14:12:09.805235 139848901543808 learning.py:507] global step 2473: loss = 1.2038 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2474: loss = 2.0273 (0.626 sec/step)\n",
            "I0915 14:12:10.433160 139848901543808 learning.py:507] global step 2474: loss = 2.0273 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2475: loss = 1.8453 (0.634 sec/step)\n",
            "I0915 14:12:11.068958 139848901543808 learning.py:507] global step 2475: loss = 1.8453 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2476: loss = 3.0283 (0.675 sec/step)\n",
            "I0915 14:12:11.746086 139848901543808 learning.py:507] global step 2476: loss = 3.0283 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 2477: loss = 2.7557 (0.638 sec/step)\n",
            "I0915 14:12:12.386571 139848901543808 learning.py:507] global step 2477: loss = 2.7557 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2478: loss = 1.9578 (0.658 sec/step)\n",
            "I0915 14:12:13.046594 139848901543808 learning.py:507] global step 2478: loss = 1.9578 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2479: loss = 1.1325 (0.636 sec/step)\n",
            "I0915 14:12:13.684790 139848901543808 learning.py:507] global step 2479: loss = 1.1325 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2480: loss = 3.0254 (0.635 sec/step)\n",
            "I0915 14:12:14.321599 139848901543808 learning.py:507] global step 2480: loss = 3.0254 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2481: loss = 2.4019 (0.617 sec/step)\n",
            "I0915 14:12:14.940593 139848901543808 learning.py:507] global step 2481: loss = 2.4019 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2482: loss = 2.2740 (0.596 sec/step)\n",
            "I0915 14:12:15.538629 139848901543808 learning.py:507] global step 2482: loss = 2.2740 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2483: loss = 2.2837 (0.617 sec/step)\n",
            "I0915 14:12:16.157428 139848901543808 learning.py:507] global step 2483: loss = 2.2837 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2484: loss = 2.0775 (0.644 sec/step)\n",
            "I0915 14:12:16.802912 139848901543808 learning.py:507] global step 2484: loss = 2.0775 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2485: loss = 1.9354 (0.631 sec/step)\n",
            "I0915 14:12:17.435574 139848901543808 learning.py:507] global step 2485: loss = 1.9354 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2486: loss = 2.8321 (0.641 sec/step)\n",
            "I0915 14:12:18.078651 139848901543808 learning.py:507] global step 2486: loss = 2.8321 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2487: loss = 3.1347 (0.661 sec/step)\n",
            "I0915 14:12:18.742185 139848901543808 learning.py:507] global step 2487: loss = 3.1347 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2488: loss = 2.1274 (0.621 sec/step)\n",
            "I0915 14:12:19.365337 139848901543808 learning.py:507] global step 2488: loss = 2.1274 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2489: loss = 2.3846 (0.636 sec/step)\n",
            "I0915 14:12:20.003435 139848901543808 learning.py:507] global step 2489: loss = 2.3846 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2490: loss = 1.8284 (0.660 sec/step)\n",
            "I0915 14:12:20.665467 139848901543808 learning.py:507] global step 2490: loss = 1.8284 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2491: loss = 2.1757 (0.630 sec/step)\n",
            "I0915 14:12:21.297399 139848901543808 learning.py:507] global step 2491: loss = 2.1757 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2492: loss = 3.3999 (0.636 sec/step)\n",
            "I0915 14:12:21.935204 139848901543808 learning.py:507] global step 2492: loss = 3.3999 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2493: loss = 1.5049 (0.645 sec/step)\n",
            "I0915 14:12:22.582687 139848901543808 learning.py:507] global step 2493: loss = 1.5049 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2494: loss = 2.4972 (0.662 sec/step)\n",
            "I0915 14:12:23.246768 139848901543808 learning.py:507] global step 2494: loss = 2.4972 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2495: loss = 1.7440 (0.646 sec/step)\n",
            "I0915 14:12:23.894973 139848901543808 learning.py:507] global step 2495: loss = 1.7440 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2496: loss = 1.7735 (0.622 sec/step)\n",
            "I0915 14:12:24.519470 139848901543808 learning.py:507] global step 2496: loss = 1.7735 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2497: loss = 2.3038 (0.629 sec/step)\n",
            "I0915 14:12:25.150446 139848901543808 learning.py:507] global step 2497: loss = 2.3038 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2498: loss = 1.9042 (0.622 sec/step)\n",
            "I0915 14:12:25.774085 139848901543808 learning.py:507] global step 2498: loss = 1.9042 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2499: loss = 1.9150 (0.629 sec/step)\n",
            "I0915 14:12:26.405442 139848901543808 learning.py:507] global step 2499: loss = 1.9150 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2500: loss = 2.0592 (0.620 sec/step)\n",
            "I0915 14:12:27.027442 139848901543808 learning.py:507] global step 2500: loss = 2.0592 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2501: loss = 2.1156 (0.642 sec/step)\n",
            "I0915 14:12:27.671293 139848901543808 learning.py:507] global step 2501: loss = 2.1156 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2502: loss = 2.5795 (0.652 sec/step)\n",
            "I0915 14:12:28.325326 139848901543808 learning.py:507] global step 2502: loss = 2.5795 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2503: loss = 1.9380 (0.658 sec/step)\n",
            "I0915 14:12:28.985256 139848901543808 learning.py:507] global step 2503: loss = 1.9380 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2504: loss = 1.6379 (0.621 sec/step)\n",
            "I0915 14:12:29.608633 139848901543808 learning.py:507] global step 2504: loss = 1.6379 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2505: loss = 1.9171 (0.644 sec/step)\n",
            "I0915 14:12:30.255106 139848901543808 learning.py:507] global step 2505: loss = 1.9171 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2506: loss = 1.5035 (0.624 sec/step)\n",
            "I0915 14:12:30.881370 139848901543808 learning.py:507] global step 2506: loss = 1.5035 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2507: loss = 3.3710 (0.638 sec/step)\n",
            "I0915 14:12:31.521211 139848901543808 learning.py:507] global step 2507: loss = 3.3710 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2508: loss = 2.3333 (0.620 sec/step)\n",
            "I0915 14:12:32.143635 139848901543808 learning.py:507] global step 2508: loss = 2.3333 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2509: loss = 1.9327 (0.609 sec/step)\n",
            "I0915 14:12:32.755338 139848901543808 learning.py:507] global step 2509: loss = 1.9327 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2510: loss = 2.5095 (0.639 sec/step)\n",
            "I0915 14:12:33.397060 139848901543808 learning.py:507] global step 2510: loss = 2.5095 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2511: loss = 2.0058 (0.623 sec/step)\n",
            "I0915 14:12:34.022061 139848901543808 learning.py:507] global step 2511: loss = 2.0058 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2512: loss = 3.5285 (0.645 sec/step)\n",
            "I0915 14:12:34.669985 139848901543808 learning.py:507] global step 2512: loss = 3.5285 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2513: loss = 2.1067 (0.654 sec/step)\n",
            "I0915 14:12:35.325967 139848901543808 learning.py:507] global step 2513: loss = 2.1067 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2514: loss = 2.9547 (0.619 sec/step)\n",
            "I0915 14:12:35.946529 139848901543808 learning.py:507] global step 2514: loss = 2.9547 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2515: loss = 1.5662 (0.647 sec/step)\n",
            "I0915 14:12:36.595549 139848901543808 learning.py:507] global step 2515: loss = 1.5662 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2516: loss = 2.1103 (0.627 sec/step)\n",
            "I0915 14:12:37.224511 139848901543808 learning.py:507] global step 2516: loss = 2.1103 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2517: loss = 2.7062 (0.623 sec/step)\n",
            "I0915 14:12:37.849636 139848901543808 learning.py:507] global step 2517: loss = 2.7062 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2518: loss = 1.9374 (0.632 sec/step)\n",
            "I0915 14:12:38.483710 139848901543808 learning.py:507] global step 2518: loss = 1.9374 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2519: loss = 2.4414 (0.627 sec/step)\n",
            "I0915 14:12:39.113400 139848901543808 learning.py:507] global step 2519: loss = 2.4414 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2520: loss = 1.6926 (0.643 sec/step)\n",
            "I0915 14:12:39.758393 139848901543808 learning.py:507] global step 2520: loss = 1.6926 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2521: loss = 2.3780 (0.647 sec/step)\n",
            "I0915 14:12:40.407761 139848901543808 learning.py:507] global step 2521: loss = 2.3780 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2522: loss = 1.3517 (0.616 sec/step)\n",
            "I0915 14:12:41.025449 139848901543808 learning.py:507] global step 2522: loss = 1.3517 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2523: loss = 2.1587 (0.646 sec/step)\n",
            "I0915 14:12:41.673400 139848901543808 learning.py:507] global step 2523: loss = 2.1587 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2524: loss = 1.9310 (0.655 sec/step)\n",
            "I0915 14:12:42.329965 139848901543808 learning.py:507] global step 2524: loss = 1.9310 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 2525: loss = 3.1663 (0.618 sec/step)\n",
            "I0915 14:12:42.949765 139848901543808 learning.py:507] global step 2525: loss = 3.1663 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2526: loss = 1.5553 (0.632 sec/step)\n",
            "I0915 14:12:43.584225 139848901543808 learning.py:507] global step 2526: loss = 1.5553 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2527: loss = 1.6603 (0.628 sec/step)\n",
            "I0915 14:12:44.215065 139848901543808 learning.py:507] global step 2527: loss = 1.6603 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2528: loss = 2.2951 (0.629 sec/step)\n",
            "I0915 14:12:44.846243 139848901543808 learning.py:507] global step 2528: loss = 2.2951 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2529: loss = 1.7086 (0.636 sec/step)\n",
            "I0915 14:12:45.484094 139848901543808 learning.py:507] global step 2529: loss = 1.7086 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2530: loss = 2.6054 (0.628 sec/step)\n",
            "I0915 14:12:46.114063 139848901543808 learning.py:507] global step 2530: loss = 2.6054 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2531: loss = 2.0135 (0.648 sec/step)\n",
            "I0915 14:12:46.763760 139848901543808 learning.py:507] global step 2531: loss = 2.0135 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2532: loss = 2.3008 (0.614 sec/step)\n",
            "I0915 14:12:47.379830 139848901543808 learning.py:507] global step 2532: loss = 2.3008 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2533: loss = 2.1242 (0.617 sec/step)\n",
            "I0915 14:12:47.999055 139848901543808 learning.py:507] global step 2533: loss = 2.1242 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2534: loss = 2.2197 (0.609 sec/step)\n",
            "I0915 14:12:48.609914 139848901543808 learning.py:507] global step 2534: loss = 2.2197 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2535: loss = 2.3311 (0.634 sec/step)\n",
            "I0915 14:12:49.246401 139848901543808 learning.py:507] global step 2535: loss = 2.3311 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2536: loss = 2.5228 (0.661 sec/step)\n",
            "I0915 14:12:49.909243 139848901543808 learning.py:507] global step 2536: loss = 2.5228 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2537: loss = 2.2511 (0.626 sec/step)\n",
            "I0915 14:12:50.537689 139848901543808 learning.py:507] global step 2537: loss = 2.2511 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2538: loss = 3.2042 (0.626 sec/step)\n",
            "I0915 14:12:51.166028 139848901543808 learning.py:507] global step 2538: loss = 3.2042 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2539: loss = 2.0452 (0.645 sec/step)\n",
            "I0915 14:12:51.812984 139848901543808 learning.py:507] global step 2539: loss = 2.0452 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2540: loss = 2.3263 (0.650 sec/step)\n",
            "I0915 14:12:52.465429 139848901543808 learning.py:507] global step 2540: loss = 2.3263 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2541: loss = 1.5288 (0.637 sec/step)\n",
            "I0915 14:12:53.105126 139848901543808 learning.py:507] global step 2541: loss = 1.5288 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2542: loss = 2.7381 (0.631 sec/step)\n",
            "I0915 14:12:53.738554 139848901543808 learning.py:507] global step 2542: loss = 2.7381 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2543: loss = 2.8124 (0.633 sec/step)\n",
            "I0915 14:12:54.373831 139848901543808 learning.py:507] global step 2543: loss = 2.8124 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2544: loss = 1.4539 (0.652 sec/step)\n",
            "I0915 14:12:55.027694 139848901543808 learning.py:507] global step 2544: loss = 1.4539 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2545: loss = 2.7234 (0.628 sec/step)\n",
            "I0915 14:12:55.657954 139848901543808 learning.py:507] global step 2545: loss = 2.7234 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2546: loss = 3.1953 (0.648 sec/step)\n",
            "I0915 14:12:56.308939 139848901543808 learning.py:507] global step 2546: loss = 3.1953 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2547: loss = 2.9419 (0.614 sec/step)\n",
            "I0915 14:12:56.924898 139848901543808 learning.py:507] global step 2547: loss = 2.9419 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2548: loss = 2.6834 (0.610 sec/step)\n",
            "I0915 14:12:57.536559 139848901543808 learning.py:507] global step 2548: loss = 2.6834 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2549: loss = 2.3808 (0.642 sec/step)\n",
            "I0915 14:12:58.180604 139848901543808 learning.py:507] global step 2549: loss = 2.3808 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2550: loss = 1.5123 (0.624 sec/step)\n",
            "I0915 14:12:58.807104 139848901543808 learning.py:507] global step 2550: loss = 1.5123 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2551: loss = 1.5095 (0.618 sec/step)\n",
            "I0915 14:12:59.427023 139848901543808 learning.py:507] global step 2551: loss = 1.5095 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2552: loss = 2.5081 (0.667 sec/step)\n",
            "I0915 14:13:00.095568 139848901543808 learning.py:507] global step 2552: loss = 2.5081 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 2553: loss = 1.4865 (0.605 sec/step)\n",
            "I0915 14:13:00.702472 139848901543808 learning.py:507] global step 2553: loss = 1.4865 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2554: loss = 1.9208 (0.644 sec/step)\n",
            "I0915 14:13:01.349024 139848901543808 learning.py:507] global step 2554: loss = 1.9208 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2555: loss = 2.3884 (0.645 sec/step)\n",
            "I0915 14:13:01.996152 139848901543808 learning.py:507] global step 2555: loss = 2.3884 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2556: loss = 2.3602 (0.633 sec/step)\n",
            "I0915 14:13:02.630810 139848901543808 learning.py:507] global step 2556: loss = 2.3602 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2557: loss = 2.3383 (0.640 sec/step)\n",
            "I0915 14:13:03.273239 139848901543808 learning.py:507] global step 2557: loss = 2.3383 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2558: loss = 2.0042 (0.614 sec/step)\n",
            "I0915 14:13:03.888839 139848901543808 learning.py:507] global step 2558: loss = 2.0042 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2559: loss = 2.2143 (0.636 sec/step)\n",
            "I0915 14:13:04.527175 139848901543808 learning.py:507] global step 2559: loss = 2.2143 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2560: loss = 2.1272 (0.640 sec/step)\n",
            "I0915 14:13:05.169562 139848901543808 learning.py:507] global step 2560: loss = 2.1272 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2561: loss = 1.7883 (0.667 sec/step)\n",
            "I0915 14:13:05.838393 139848901543808 learning.py:507] global step 2561: loss = 1.7883 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 2562: loss = 1.2343 (0.632 sec/step)\n",
            "I0915 14:13:06.472698 139848901543808 learning.py:507] global step 2562: loss = 1.2343 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2563: loss = 1.9685 (0.626 sec/step)\n",
            "I0915 14:13:07.100922 139848901543808 learning.py:507] global step 2563: loss = 1.9685 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2564: loss = 1.8741 (0.613 sec/step)\n",
            "I0915 14:13:07.716547 139848901543808 learning.py:507] global step 2564: loss = 1.8741 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2565: loss = 2.4469 (0.641 sec/step)\n",
            "I0915 14:13:08.359539 139848901543808 learning.py:507] global step 2565: loss = 2.4469 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2566: loss = 2.9560 (0.608 sec/step)\n",
            "I0915 14:13:08.969501 139848901543808 learning.py:507] global step 2566: loss = 2.9560 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2567: loss = 2.8217 (0.621 sec/step)\n",
            "I0915 14:13:09.592774 139848901543808 learning.py:507] global step 2567: loss = 2.8217 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2568: loss = 2.2957 (0.664 sec/step)\n",
            "I0915 14:13:10.258412 139848901543808 learning.py:507] global step 2568: loss = 2.2957 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 2569: loss = 3.6865 (0.636 sec/step)\n",
            "I0915 14:13:10.896778 139848901543808 learning.py:507] global step 2569: loss = 3.6865 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2570: loss = 1.9369 (0.679 sec/step)\n",
            "I0915 14:13:11.578305 139848901543808 learning.py:507] global step 2570: loss = 1.9369 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 2571: loss = 2.4223 (0.635 sec/step)\n",
            "I0915 14:13:12.215630 139848901543808 learning.py:507] global step 2571: loss = 2.4223 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2572: loss = 1.9974 (0.636 sec/step)\n",
            "I0915 14:13:12.853939 139848901543808 learning.py:507] global step 2572: loss = 1.9974 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2573: loss = 2.3681 (0.626 sec/step)\n",
            "I0915 14:13:13.481638 139848901543808 learning.py:507] global step 2573: loss = 2.3681 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2574: loss = 2.5112 (0.624 sec/step)\n",
            "I0915 14:13:14.107903 139848901543808 learning.py:507] global step 2574: loss = 2.5112 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2575: loss = 2.1322 (0.616 sec/step)\n",
            "I0915 14:13:14.725529 139848901543808 learning.py:507] global step 2575: loss = 2.1322 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2576: loss = 1.2752 (0.636 sec/step)\n",
            "I0915 14:13:15.363314 139848901543808 learning.py:507] global step 2576: loss = 1.2752 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2577: loss = 1.3870 (0.634 sec/step)\n",
            "I0915 14:13:15.999253 139848901543808 learning.py:507] global step 2577: loss = 1.3870 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2578: loss = 2.3506 (0.631 sec/step)\n",
            "I0915 14:13:16.632618 139848901543808 learning.py:507] global step 2578: loss = 2.3506 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2579: loss = 2.0795 (0.628 sec/step)\n",
            "I0915 14:13:17.263801 139848901543808 learning.py:507] global step 2579: loss = 2.0795 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2580: loss = 2.0621 (0.653 sec/step)\n",
            "I0915 14:13:17.918999 139848901543808 learning.py:507] global step 2580: loss = 2.0621 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2581: loss = 2.2719 (0.653 sec/step)\n",
            "I0915 14:13:18.574098 139848901543808 learning.py:507] global step 2581: loss = 2.2719 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2582: loss = 2.0688 (0.625 sec/step)\n",
            "I0915 14:13:19.200765 139848901543808 learning.py:507] global step 2582: loss = 2.0688 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2583: loss = 1.5803 (0.635 sec/step)\n",
            "I0915 14:13:19.837160 139848901543808 learning.py:507] global step 2583: loss = 1.5803 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2584: loss = 2.8837 (0.623 sec/step)\n",
            "I0915 14:13:20.461757 139848901543808 learning.py:507] global step 2584: loss = 2.8837 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2585: loss = 3.0668 (0.633 sec/step)\n",
            "I0915 14:13:21.096916 139848901543808 learning.py:507] global step 2585: loss = 3.0668 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2586: loss = 1.9290 (0.635 sec/step)\n",
            "I0915 14:13:21.733423 139848901543808 learning.py:507] global step 2586: loss = 1.9290 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2587: loss = 1.9585 (0.628 sec/step)\n",
            "I0915 14:13:22.363337 139848901543808 learning.py:507] global step 2587: loss = 1.9585 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2588: loss = 2.1650 (0.678 sec/step)\n",
            "I0915 14:13:23.045459 139848901543808 learning.py:507] global step 2588: loss = 2.1650 (0.678 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2588.\n",
            "I0915 14:13:23.769521 139845834331904 supervisor.py:1050] Recording summary at step 2588.\n",
            "INFO:tensorflow:global step 2589: loss = 3.2747 (1.048 sec/step)\n",
            "I0915 14:13:24.096076 139848901543808 learning.py:507] global step 2589: loss = 3.2747 (1.048 sec/step)\n",
            "INFO:tensorflow:global step 2590: loss = 1.5037 (0.614 sec/step)\n",
            "I0915 14:13:24.712285 139848901543808 learning.py:507] global step 2590: loss = 1.5037 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2591: loss = 2.7135 (0.631 sec/step)\n",
            "I0915 14:13:25.345188 139848901543808 learning.py:507] global step 2591: loss = 2.7135 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2592: loss = 1.7055 (0.647 sec/step)\n",
            "I0915 14:13:25.994007 139848901543808 learning.py:507] global step 2592: loss = 1.7055 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2593: loss = 2.6147 (0.628 sec/step)\n",
            "I0915 14:13:26.623555 139848901543808 learning.py:507] global step 2593: loss = 2.6147 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2594: loss = 2.4348 (0.652 sec/step)\n",
            "I0915 14:13:27.278085 139848901543808 learning.py:507] global step 2594: loss = 2.4348 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2595: loss = 2.2122 (0.649 sec/step)\n",
            "I0915 14:13:27.929409 139848901543808 learning.py:507] global step 2595: loss = 2.2122 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2596: loss = 2.7450 (0.626 sec/step)\n",
            "I0915 14:13:28.556932 139848901543808 learning.py:507] global step 2596: loss = 2.7450 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2597: loss = 3.0403 (0.638 sec/step)\n",
            "I0915 14:13:29.197100 139848901543808 learning.py:507] global step 2597: loss = 3.0403 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2598: loss = 2.4123 (0.626 sec/step)\n",
            "I0915 14:13:29.824696 139848901543808 learning.py:507] global step 2598: loss = 2.4123 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2599: loss = 3.1345 (0.604 sec/step)\n",
            "I0915 14:13:30.430238 139848901543808 learning.py:507] global step 2599: loss = 3.1345 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2600: loss = 1.2942 (0.654 sec/step)\n",
            "I0915 14:13:31.086470 139848901543808 learning.py:507] global step 2600: loss = 1.2942 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2601: loss = 2.0888 (0.648 sec/step)\n",
            "I0915 14:13:31.736542 139848901543808 learning.py:507] global step 2601: loss = 2.0888 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2602: loss = 2.5564 (0.611 sec/step)\n",
            "I0915 14:13:32.349429 139848901543808 learning.py:507] global step 2602: loss = 2.5564 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2603: loss = 1.9118 (0.642 sec/step)\n",
            "I0915 14:13:32.993239 139848901543808 learning.py:507] global step 2603: loss = 1.9118 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2604: loss = 1.7511 (0.640 sec/step)\n",
            "I0915 14:13:33.635696 139848901543808 learning.py:507] global step 2604: loss = 1.7511 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2605: loss = 1.9493 (0.637 sec/step)\n",
            "I0915 14:13:34.274216 139848901543808 learning.py:507] global step 2605: loss = 1.9493 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2606: loss = 1.7782 (0.622 sec/step)\n",
            "I0915 14:13:34.898308 139848901543808 learning.py:507] global step 2606: loss = 1.7782 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2607: loss = 2.2652 (0.622 sec/step)\n",
            "I0915 14:13:35.522305 139848901543808 learning.py:507] global step 2607: loss = 2.2652 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2608: loss = 3.9679 (0.640 sec/step)\n",
            "I0915 14:13:36.164309 139848901543808 learning.py:507] global step 2608: loss = 3.9679 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2609: loss = 1.8916 (0.627 sec/step)\n",
            "I0915 14:13:36.794799 139848901543808 learning.py:507] global step 2609: loss = 1.8916 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2610: loss = 1.6995 (0.639 sec/step)\n",
            "I0915 14:13:37.438113 139848901543808 learning.py:507] global step 2610: loss = 1.6995 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2611: loss = 2.4213 (0.635 sec/step)\n",
            "I0915 14:13:38.075318 139848901543808 learning.py:507] global step 2611: loss = 2.4213 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2612: loss = 1.7069 (0.620 sec/step)\n",
            "I0915 14:13:38.697387 139848901543808 learning.py:507] global step 2612: loss = 1.7069 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2613: loss = 2.3095 (0.625 sec/step)\n",
            "I0915 14:13:39.324498 139848901543808 learning.py:507] global step 2613: loss = 2.3095 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2614: loss = 1.6954 (0.623 sec/step)\n",
            "I0915 14:13:39.949069 139848901543808 learning.py:507] global step 2614: loss = 1.6954 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2615: loss = 1.0704 (0.622 sec/step)\n",
            "I0915 14:13:40.573413 139848901543808 learning.py:507] global step 2615: loss = 1.0704 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2616: loss = 2.1504 (0.657 sec/step)\n",
            "I0915 14:13:41.232545 139848901543808 learning.py:507] global step 2616: loss = 2.1504 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2617: loss = 0.9946 (0.616 sec/step)\n",
            "I0915 14:13:41.850388 139848901543808 learning.py:507] global step 2617: loss = 0.9946 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2618: loss = 1.5491 (0.643 sec/step)\n",
            "I0915 14:13:42.494895 139848901543808 learning.py:507] global step 2618: loss = 1.5491 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2619: loss = 2.1608 (0.646 sec/step)\n",
            "I0915 14:13:43.143093 139848901543808 learning.py:507] global step 2619: loss = 2.1608 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2620: loss = 1.5761 (0.629 sec/step)\n",
            "I0915 14:13:43.774082 139848901543808 learning.py:507] global step 2620: loss = 1.5761 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2621: loss = 2.0120 (0.661 sec/step)\n",
            "I0915 14:13:44.437256 139848901543808 learning.py:507] global step 2621: loss = 2.0120 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2622: loss = 1.7948 (0.612 sec/step)\n",
            "I0915 14:13:45.051445 139848901543808 learning.py:507] global step 2622: loss = 1.7948 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2623: loss = 2.4648 (0.630 sec/step)\n",
            "I0915 14:13:45.683857 139848901543808 learning.py:507] global step 2623: loss = 2.4648 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2624: loss = 3.1520 (0.645 sec/step)\n",
            "I0915 14:13:46.330622 139848901543808 learning.py:507] global step 2624: loss = 3.1520 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2625: loss = 2.0005 (0.637 sec/step)\n",
            "I0915 14:13:46.969703 139848901543808 learning.py:507] global step 2625: loss = 2.0005 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2626: loss = 1.8788 (0.638 sec/step)\n",
            "I0915 14:13:47.609418 139848901543808 learning.py:507] global step 2626: loss = 1.8788 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2627: loss = 2.6692 (0.614 sec/step)\n",
            "I0915 14:13:48.224911 139848901543808 learning.py:507] global step 2627: loss = 2.6692 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2628: loss = 2.5023 (0.613 sec/step)\n",
            "I0915 14:13:48.840568 139848901543808 learning.py:507] global step 2628: loss = 2.5023 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2629: loss = 2.1956 (0.629 sec/step)\n",
            "I0915 14:13:49.471205 139848901543808 learning.py:507] global step 2629: loss = 2.1956 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2630: loss = 1.6373 (0.634 sec/step)\n",
            "I0915 14:13:50.107574 139848901543808 learning.py:507] global step 2630: loss = 1.6373 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2631: loss = 1.8132 (0.636 sec/step)\n",
            "I0915 14:13:50.746051 139848901543808 learning.py:507] global step 2631: loss = 1.8132 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2632: loss = 3.9084 (0.638 sec/step)\n",
            "I0915 14:13:51.386133 139848901543808 learning.py:507] global step 2632: loss = 3.9084 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2633: loss = 2.6498 (0.640 sec/step)\n",
            "I0915 14:13:52.027873 139848901543808 learning.py:507] global step 2633: loss = 2.6498 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2634: loss = 2.3564 (0.642 sec/step)\n",
            "I0915 14:13:52.671549 139848901543808 learning.py:507] global step 2634: loss = 2.3564 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2635: loss = 1.9616 (0.624 sec/step)\n",
            "I0915 14:13:53.297076 139848901543808 learning.py:507] global step 2635: loss = 1.9616 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2636: loss = 2.4427 (0.647 sec/step)\n",
            "I0915 14:13:53.946191 139848901543808 learning.py:507] global step 2636: loss = 2.4427 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2637: loss = 2.0822 (0.652 sec/step)\n",
            "I0915 14:13:54.600416 139848901543808 learning.py:507] global step 2637: loss = 2.0822 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2638: loss = 1.9100 (0.618 sec/step)\n",
            "I0915 14:13:55.220494 139848901543808 learning.py:507] global step 2638: loss = 1.9100 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2639: loss = 1.7978 (0.625 sec/step)\n",
            "I0915 14:13:55.847639 139848901543808 learning.py:507] global step 2639: loss = 1.7978 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2640: loss = 2.8235 (0.634 sec/step)\n",
            "I0915 14:13:56.483605 139848901543808 learning.py:507] global step 2640: loss = 2.8235 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2641: loss = 1.7649 (0.623 sec/step)\n",
            "I0915 14:13:57.108831 139848901543808 learning.py:507] global step 2641: loss = 1.7649 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2642: loss = 1.8910 (0.663 sec/step)\n",
            "I0915 14:13:57.773744 139848901543808 learning.py:507] global step 2642: loss = 1.8910 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2643: loss = 1.4645 (0.614 sec/step)\n",
            "I0915 14:13:58.390843 139848901543808 learning.py:507] global step 2643: loss = 1.4645 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2644: loss = 2.2346 (0.656 sec/step)\n",
            "I0915 14:13:59.049033 139848901543808 learning.py:507] global step 2644: loss = 2.2346 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2645: loss = 2.2685 (0.644 sec/step)\n",
            "I0915 14:13:59.694933 139848901543808 learning.py:507] global step 2645: loss = 2.2685 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2646: loss = 1.6908 (0.631 sec/step)\n",
            "I0915 14:14:00.327373 139848901543808 learning.py:507] global step 2646: loss = 1.6908 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2647: loss = 2.1550 (0.622 sec/step)\n",
            "I0915 14:14:00.951128 139848901543808 learning.py:507] global step 2647: loss = 2.1550 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2648: loss = 1.3071 (0.625 sec/step)\n",
            "I0915 14:14:01.578142 139848901543808 learning.py:507] global step 2648: loss = 1.3071 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2649: loss = 2.4242 (0.661 sec/step)\n",
            "I0915 14:14:02.241689 139848901543808 learning.py:507] global step 2649: loss = 2.4242 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2650: loss = 1.6182 (0.631 sec/step)\n",
            "I0915 14:14:02.874242 139848901543808 learning.py:507] global step 2650: loss = 1.6182 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2651: loss = 2.0530 (0.623 sec/step)\n",
            "I0915 14:14:03.499065 139848901543808 learning.py:507] global step 2651: loss = 2.0530 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2652: loss = 2.0040 (0.616 sec/step)\n",
            "I0915 14:14:04.116616 139848901543808 learning.py:507] global step 2652: loss = 2.0040 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2653: loss = 2.6010 (0.644 sec/step)\n",
            "I0915 14:14:04.762509 139848901543808 learning.py:507] global step 2653: loss = 2.6010 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2654: loss = 2.2167 (0.629 sec/step)\n",
            "I0915 14:14:05.393683 139848901543808 learning.py:507] global step 2654: loss = 2.2167 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2655: loss = 2.1196 (0.634 sec/step)\n",
            "I0915 14:14:06.029915 139848901543808 learning.py:507] global step 2655: loss = 2.1196 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2656: loss = 2.5473 (0.631 sec/step)\n",
            "I0915 14:14:06.662772 139848901543808 learning.py:507] global step 2656: loss = 2.5473 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2657: loss = 2.5875 (0.616 sec/step)\n",
            "I0915 14:14:07.280793 139848901543808 learning.py:507] global step 2657: loss = 2.5875 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2658: loss = 3.6783 (0.632 sec/step)\n",
            "I0915 14:14:07.915090 139848901543808 learning.py:507] global step 2658: loss = 3.6783 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2659: loss = 2.3921 (0.628 sec/step)\n",
            "I0915 14:14:08.545318 139848901543808 learning.py:507] global step 2659: loss = 2.3921 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2660: loss = 2.1025 (0.638 sec/step)\n",
            "I0915 14:14:09.185582 139848901543808 learning.py:507] global step 2660: loss = 2.1025 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2661: loss = 2.2186 (0.640 sec/step)\n",
            "I0915 14:14:09.827921 139848901543808 learning.py:507] global step 2661: loss = 2.2186 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2662: loss = 2.2801 (0.616 sec/step)\n",
            "I0915 14:14:10.445871 139848901543808 learning.py:507] global step 2662: loss = 2.2801 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2663: loss = 2.5658 (0.650 sec/step)\n",
            "I0915 14:14:11.097618 139848901543808 learning.py:507] global step 2663: loss = 2.5658 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2664: loss = 1.3705 (0.634 sec/step)\n",
            "I0915 14:14:11.734051 139848901543808 learning.py:507] global step 2664: loss = 1.3705 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2665: loss = 1.7823 (0.673 sec/step)\n",
            "I0915 14:14:12.408692 139848901543808 learning.py:507] global step 2665: loss = 1.7823 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 2666: loss = 1.1620 (0.639 sec/step)\n",
            "I0915 14:14:13.050061 139848901543808 learning.py:507] global step 2666: loss = 1.1620 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2667: loss = 2.0625 (0.641 sec/step)\n",
            "I0915 14:14:13.692840 139848901543808 learning.py:507] global step 2667: loss = 2.0625 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2668: loss = 2.6234 (0.633 sec/step)\n",
            "I0915 14:14:14.327858 139848901543808 learning.py:507] global step 2668: loss = 2.6234 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2669: loss = 2.3594 (0.622 sec/step)\n",
            "I0915 14:14:14.952412 139848901543808 learning.py:507] global step 2669: loss = 2.3594 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2670: loss = 2.5130 (0.629 sec/step)\n",
            "I0915 14:14:15.583577 139848901543808 learning.py:507] global step 2670: loss = 2.5130 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2671: loss = 1.2243 (0.626 sec/step)\n",
            "I0915 14:14:16.211951 139848901543808 learning.py:507] global step 2671: loss = 1.2243 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2672: loss = 2.4845 (0.617 sec/step)\n",
            "I0915 14:14:16.830909 139848901543808 learning.py:507] global step 2672: loss = 2.4845 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2673: loss = 1.7963 (0.627 sec/step)\n",
            "I0915 14:14:17.459668 139848901543808 learning.py:507] global step 2673: loss = 1.7963 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2674: loss = 3.2056 (0.631 sec/step)\n",
            "I0915 14:14:18.092282 139848901543808 learning.py:507] global step 2674: loss = 3.2056 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2675: loss = 2.1935 (0.654 sec/step)\n",
            "I0915 14:14:18.748119 139848901543808 learning.py:507] global step 2675: loss = 2.1935 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2676: loss = 1.7915 (0.623 sec/step)\n",
            "I0915 14:14:19.373102 139848901543808 learning.py:507] global step 2676: loss = 1.7915 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2677: loss = 2.2508 (0.656 sec/step)\n",
            "I0915 14:14:20.030736 139848901543808 learning.py:507] global step 2677: loss = 2.2508 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2678: loss = 1.0516 (0.624 sec/step)\n",
            "I0915 14:14:20.656522 139848901543808 learning.py:507] global step 2678: loss = 1.0516 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2679: loss = 1.8451 (0.624 sec/step)\n",
            "I0915 14:14:21.282783 139848901543808 learning.py:507] global step 2679: loss = 1.8451 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2680: loss = 1.8643 (0.632 sec/step)\n",
            "I0915 14:14:21.916469 139848901543808 learning.py:507] global step 2680: loss = 1.8643 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2681: loss = 2.9000 (0.659 sec/step)\n",
            "I0915 14:14:22.577276 139848901543808 learning.py:507] global step 2681: loss = 2.9000 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2682: loss = 1.8688 (0.629 sec/step)\n",
            "I0915 14:14:23.208476 139848901543808 learning.py:507] global step 2682: loss = 1.8688 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2683: loss = 2.5862 (0.607 sec/step)\n",
            "I0915 14:14:23.817408 139848901543808 learning.py:507] global step 2683: loss = 2.5862 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2684: loss = 2.4906 (0.660 sec/step)\n",
            "I0915 14:14:24.480054 139848901543808 learning.py:507] global step 2684: loss = 2.4906 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2685: loss = 1.9699 (0.664 sec/step)\n",
            "I0915 14:14:25.147384 139848901543808 learning.py:507] global step 2685: loss = 1.9699 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 2686: loss = 1.6361 (0.641 sec/step)\n",
            "I0915 14:14:25.795148 139848901543808 learning.py:507] global step 2686: loss = 1.6361 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2687: loss = 1.3806 (0.639 sec/step)\n",
            "I0915 14:14:26.436155 139848901543808 learning.py:507] global step 2687: loss = 1.3806 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2688: loss = 2.1568 (0.638 sec/step)\n",
            "I0915 14:14:27.077107 139848901543808 learning.py:507] global step 2688: loss = 2.1568 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2689: loss = 1.7742 (0.640 sec/step)\n",
            "I0915 14:14:27.719500 139848901543808 learning.py:507] global step 2689: loss = 1.7742 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2690: loss = 1.6176 (0.620 sec/step)\n",
            "I0915 14:14:28.341416 139848901543808 learning.py:507] global step 2690: loss = 1.6176 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2691: loss = 1.8526 (0.656 sec/step)\n",
            "I0915 14:14:28.999199 139848901543808 learning.py:507] global step 2691: loss = 1.8526 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2692: loss = 1.6486 (0.632 sec/step)\n",
            "I0915 14:14:29.632945 139848901543808 learning.py:507] global step 2692: loss = 1.6486 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2693: loss = 2.4674 (0.647 sec/step)\n",
            "I0915 14:14:30.282279 139848901543808 learning.py:507] global step 2693: loss = 2.4674 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2694: loss = 1.7424 (0.628 sec/step)\n",
            "I0915 14:14:30.911820 139848901543808 learning.py:507] global step 2694: loss = 1.7424 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2695: loss = 2.2968 (0.598 sec/step)\n",
            "I0915 14:14:31.511939 139848901543808 learning.py:507] global step 2695: loss = 2.2968 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2696: loss = 2.5035 (0.634 sec/step)\n",
            "I0915 14:14:32.147615 139848901543808 learning.py:507] global step 2696: loss = 2.5035 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2697: loss = 2.5534 (0.636 sec/step)\n",
            "I0915 14:14:32.785903 139848901543808 learning.py:507] global step 2697: loss = 2.5534 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2698: loss = 2.2833 (0.641 sec/step)\n",
            "I0915 14:14:33.429279 139848901543808 learning.py:507] global step 2698: loss = 2.2833 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2699: loss = 2.1733 (0.628 sec/step)\n",
            "I0915 14:14:34.059187 139848901543808 learning.py:507] global step 2699: loss = 2.1733 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2700: loss = 1.9322 (0.625 sec/step)\n",
            "I0915 14:14:34.685853 139848901543808 learning.py:507] global step 2700: loss = 1.9322 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2701: loss = 2.0719 (0.648 sec/step)\n",
            "I0915 14:14:35.335442 139848901543808 learning.py:507] global step 2701: loss = 2.0719 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2702: loss = 1.9941 (0.638 sec/step)\n",
            "I0915 14:14:35.975379 139848901543808 learning.py:507] global step 2702: loss = 1.9941 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2703: loss = 4.0497 (0.631 sec/step)\n",
            "I0915 14:14:36.608124 139848901543808 learning.py:507] global step 2703: loss = 4.0497 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2704: loss = 1.3481 (0.628 sec/step)\n",
            "I0915 14:14:37.238008 139848901543808 learning.py:507] global step 2704: loss = 1.3481 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2705: loss = 2.4011 (0.652 sec/step)\n",
            "I0915 14:14:37.891631 139848901543808 learning.py:507] global step 2705: loss = 2.4011 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2706: loss = 2.2737 (0.625 sec/step)\n",
            "I0915 14:14:38.518368 139848901543808 learning.py:507] global step 2706: loss = 2.2737 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2707: loss = 1.5793 (0.636 sec/step)\n",
            "I0915 14:14:39.155777 139848901543808 learning.py:507] global step 2707: loss = 1.5793 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2708: loss = 2.0446 (0.639 sec/step)\n",
            "I0915 14:14:39.796648 139848901543808 learning.py:507] global step 2708: loss = 2.0446 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2709: loss = 1.4116 (0.643 sec/step)\n",
            "I0915 14:14:40.442189 139848901543808 learning.py:507] global step 2709: loss = 1.4116 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2710: loss = 1.5214 (0.660 sec/step)\n",
            "I0915 14:14:41.104250 139848901543808 learning.py:507] global step 2710: loss = 1.5214 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2711: loss = 1.6221 (0.614 sec/step)\n",
            "I0915 14:14:41.719790 139848901543808 learning.py:507] global step 2711: loss = 1.6221 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2712: loss = 1.2979 (0.603 sec/step)\n",
            "I0915 14:14:42.324738 139848901543808 learning.py:507] global step 2712: loss = 1.2979 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 2713: loss = 2.0478 (0.641 sec/step)\n",
            "I0915 14:14:42.968069 139848901543808 learning.py:507] global step 2713: loss = 2.0478 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2714: loss = 1.3853 (0.675 sec/step)\n",
            "I0915 14:14:43.645551 139848901543808 learning.py:507] global step 2714: loss = 1.3853 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 2715: loss = 2.2722 (0.653 sec/step)\n",
            "I0915 14:14:44.300673 139848901543808 learning.py:507] global step 2715: loss = 2.2722 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2716: loss = 2.5464 (0.649 sec/step)\n",
            "I0915 14:14:44.951515 139848901543808 learning.py:507] global step 2716: loss = 2.5464 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2717: loss = 1.1072 (0.626 sec/step)\n",
            "I0915 14:14:45.579406 139848901543808 learning.py:507] global step 2717: loss = 1.1072 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2718: loss = 1.8087 (0.643 sec/step)\n",
            "I0915 14:14:46.224986 139848901543808 learning.py:507] global step 2718: loss = 1.8087 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2719: loss = 3.0744 (0.653 sec/step)\n",
            "I0915 14:14:46.879702 139848901543808 learning.py:507] global step 2719: loss = 3.0744 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2720: loss = 1.7124 (0.615 sec/step)\n",
            "I0915 14:14:47.496824 139848901543808 learning.py:507] global step 2720: loss = 1.7124 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2721: loss = 1.6702 (0.625 sec/step)\n",
            "I0915 14:14:48.123140 139848901543808 learning.py:507] global step 2721: loss = 1.6702 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2722: loss = 1.9855 (0.644 sec/step)\n",
            "I0915 14:14:48.768793 139848901543808 learning.py:507] global step 2722: loss = 1.9855 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2723: loss = 2.0236 (0.661 sec/step)\n",
            "I0915 14:14:49.431866 139848901543808 learning.py:507] global step 2723: loss = 2.0236 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2724: loss = 1.5070 (0.651 sec/step)\n",
            "I0915 14:14:50.084480 139848901543808 learning.py:507] global step 2724: loss = 1.5070 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2725: loss = 1.6090 (0.632 sec/step)\n",
            "I0915 14:14:50.718166 139848901543808 learning.py:507] global step 2725: loss = 1.6090 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2726: loss = 2.0616 (0.660 sec/step)\n",
            "I0915 14:14:51.380198 139848901543808 learning.py:507] global step 2726: loss = 2.0616 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2727: loss = 1.5131 (0.609 sec/step)\n",
            "I0915 14:14:51.991173 139848901543808 learning.py:507] global step 2727: loss = 1.5131 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2728: loss = 2.0172 (0.678 sec/step)\n",
            "I0915 14:14:52.670681 139848901543808 learning.py:507] global step 2728: loss = 2.0172 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2729: loss = 2.2991 (0.641 sec/step)\n",
            "I0915 14:14:53.313829 139848901543808 learning.py:507] global step 2729: loss = 2.2991 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2730: loss = 2.5963 (0.648 sec/step)\n",
            "I0915 14:14:53.963870 139848901543808 learning.py:507] global step 2730: loss = 2.5963 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2731: loss = 2.3287 (0.622 sec/step)\n",
            "I0915 14:14:54.587763 139848901543808 learning.py:507] global step 2731: loss = 2.3287 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2732: loss = 2.5199 (0.621 sec/step)\n",
            "I0915 14:14:55.212286 139848901543808 learning.py:507] global step 2732: loss = 2.5199 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2733: loss = 2.5466 (0.611 sec/step)\n",
            "I0915 14:14:55.825424 139848901543808 learning.py:507] global step 2733: loss = 2.5466 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2734: loss = 2.3490 (0.643 sec/step)\n",
            "I0915 14:14:56.470762 139848901543808 learning.py:507] global step 2734: loss = 2.3490 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2735: loss = 1.4751 (0.608 sec/step)\n",
            "I0915 14:14:57.080393 139848901543808 learning.py:507] global step 2735: loss = 1.4751 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2736: loss = 2.8479 (0.612 sec/step)\n",
            "I0915 14:14:57.694792 139848901543808 learning.py:507] global step 2736: loss = 2.8479 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2737: loss = 1.9731 (0.637 sec/step)\n",
            "I0915 14:14:58.333991 139848901543808 learning.py:507] global step 2737: loss = 1.9731 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2738: loss = 2.1583 (0.626 sec/step)\n",
            "I0915 14:14:58.962072 139848901543808 learning.py:507] global step 2738: loss = 2.1583 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2739: loss = 1.3648 (0.641 sec/step)\n",
            "I0915 14:14:59.604989 139848901543808 learning.py:507] global step 2739: loss = 1.3648 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2740: loss = 2.0548 (0.627 sec/step)\n",
            "I0915 14:15:00.233734 139848901543808 learning.py:507] global step 2740: loss = 2.0548 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2741: loss = 1.5916 (0.623 sec/step)\n",
            "I0915 14:15:00.859223 139848901543808 learning.py:507] global step 2741: loss = 1.5916 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2742: loss = 1.7760 (0.621 sec/step)\n",
            "I0915 14:15:01.482254 139848901543808 learning.py:507] global step 2742: loss = 1.7760 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2743: loss = 2.3596 (0.622 sec/step)\n",
            "I0915 14:15:02.106597 139848901543808 learning.py:507] global step 2743: loss = 2.3596 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2744: loss = 3.0130 (0.624 sec/step)\n",
            "I0915 14:15:02.732234 139848901543808 learning.py:507] global step 2744: loss = 3.0130 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2745: loss = 2.4792 (0.629 sec/step)\n",
            "I0915 14:15:03.363148 139848901543808 learning.py:507] global step 2745: loss = 2.4792 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2746: loss = 1.8056 (0.628 sec/step)\n",
            "I0915 14:15:03.992868 139848901543808 learning.py:507] global step 2746: loss = 1.8056 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 2747: loss = 1.6423 (0.623 sec/step)\n",
            "I0915 14:15:04.618096 139848901543808 learning.py:507] global step 2747: loss = 1.6423 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2748: loss = 1.6627 (0.647 sec/step)\n",
            "I0915 14:15:05.266660 139848901543808 learning.py:507] global step 2748: loss = 1.6627 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2749: loss = 1.7411 (0.619 sec/step)\n",
            "I0915 14:15:05.887464 139848901543808 learning.py:507] global step 2749: loss = 1.7411 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2750: loss = 2.3790 (0.664 sec/step)\n",
            "I0915 14:15:06.552769 139848901543808 learning.py:507] global step 2750: loss = 2.3790 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 2751: loss = 2.2117 (0.648 sec/step)\n",
            "I0915 14:15:07.202712 139848901543808 learning.py:507] global step 2751: loss = 2.2117 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2752: loss = 2.4531 (0.648 sec/step)\n",
            "I0915 14:15:07.853222 139848901543808 learning.py:507] global step 2752: loss = 2.4531 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2753: loss = 1.9349 (0.634 sec/step)\n",
            "I0915 14:15:08.489471 139848901543808 learning.py:507] global step 2753: loss = 1.9349 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2754: loss = 1.7188 (0.666 sec/step)\n",
            "I0915 14:15:09.157193 139848901543808 learning.py:507] global step 2754: loss = 1.7188 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 2755: loss = 2.0818 (0.632 sec/step)\n",
            "I0915 14:15:09.791803 139848901543808 learning.py:507] global step 2755: loss = 2.0818 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2756: loss = 1.8692 (0.631 sec/step)\n",
            "I0915 14:15:10.424847 139848901543808 learning.py:507] global step 2756: loss = 1.8692 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2757: loss = 1.7368 (0.641 sec/step)\n",
            "I0915 14:15:11.067639 139848901543808 learning.py:507] global step 2757: loss = 1.7368 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2758: loss = 2.8463 (0.654 sec/step)\n",
            "I0915 14:15:11.723659 139848901543808 learning.py:507] global step 2758: loss = 2.8463 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2759: loss = 2.1029 (0.680 sec/step)\n",
            "I0915 14:15:12.405529 139848901543808 learning.py:507] global step 2759: loss = 2.1029 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2760: loss = 1.7452 (0.641 sec/step)\n",
            "I0915 14:15:13.048440 139848901543808 learning.py:507] global step 2760: loss = 1.7452 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2761: loss = 1.7739 (0.616 sec/step)\n",
            "I0915 14:15:13.666240 139848901543808 learning.py:507] global step 2761: loss = 1.7739 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2762: loss = 1.8866 (0.640 sec/step)\n",
            "I0915 14:15:14.308184 139848901543808 learning.py:507] global step 2762: loss = 1.8866 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2763: loss = 1.9009 (0.613 sec/step)\n",
            "I0915 14:15:14.923585 139848901543808 learning.py:507] global step 2763: loss = 1.9009 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2764: loss = 2.3557 (0.646 sec/step)\n",
            "I0915 14:15:15.572379 139848901543808 learning.py:507] global step 2764: loss = 2.3557 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2765: loss = 1.9480 (0.640 sec/step)\n",
            "I0915 14:15:16.215259 139848901543808 learning.py:507] global step 2765: loss = 1.9480 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2766: loss = 1.7570 (0.637 sec/step)\n",
            "I0915 14:15:16.854525 139848901543808 learning.py:507] global step 2766: loss = 1.7570 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2767: loss = 1.5719 (0.630 sec/step)\n",
            "I0915 14:15:17.486600 139848901543808 learning.py:507] global step 2767: loss = 1.5719 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2768: loss = 1.1306 (0.642 sec/step)\n",
            "I0915 14:15:18.130301 139848901543808 learning.py:507] global step 2768: loss = 1.1306 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2769: loss = 2.5246 (0.633 sec/step)\n",
            "I0915 14:15:18.765466 139848901543808 learning.py:507] global step 2769: loss = 2.5246 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2770: loss = 1.8804 (0.627 sec/step)\n",
            "I0915 14:15:19.394387 139848901543808 learning.py:507] global step 2770: loss = 1.8804 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2771: loss = 1.3100 (0.621 sec/step)\n",
            "I0915 14:15:20.017426 139848901543808 learning.py:507] global step 2771: loss = 1.3100 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2772: loss = 1.5827 (0.643 sec/step)\n",
            "I0915 14:15:20.662449 139848901543808 learning.py:507] global step 2772: loss = 1.5827 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2773: loss = 1.8130 (0.656 sec/step)\n",
            "I0915 14:15:21.320711 139848901543808 learning.py:507] global step 2773: loss = 1.8130 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2774: loss = 1.4931 (0.648 sec/step)\n",
            "I0915 14:15:21.970532 139848901543808 learning.py:507] global step 2774: loss = 1.4931 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2775: loss = 1.2861 (0.641 sec/step)\n",
            "I0915 14:15:22.613215 139848901543808 learning.py:507] global step 2775: loss = 1.2861 (0.641 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "I0915 14:15:22.837718 139845851117312 supervisor.py:1117] Saving checkpoint to path ./training_demo/training/model.ckpt\n",
            "INFO:tensorflow:global step 2776: loss = 2.4958 (0.926 sec/step)\n",
            "I0915 14:15:23.606393 139848901543808 learning.py:507] global step 2776: loss = 2.4958 (0.926 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2776.\n",
            "I0915 14:15:24.475733 139845834331904 supervisor.py:1050] Recording summary at step 2776.\n",
            "INFO:tensorflow:global step 2777: loss = 2.0174 (1.165 sec/step)\n",
            "I0915 14:15:24.781073 139848901543808 learning.py:507] global step 2777: loss = 2.0174 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 2778: loss = 2.6859 (1.035 sec/step)\n",
            "I0915 14:15:25.835278 139848901543808 learning.py:507] global step 2778: loss = 2.6859 (1.035 sec/step)\n",
            "INFO:tensorflow:global step 2779: loss = 1.8748 (1.078 sec/step)\n",
            "I0915 14:15:26.924644 139848901543808 learning.py:507] global step 2779: loss = 1.8748 (1.078 sec/step)\n",
            "INFO:tensorflow:global step 2780: loss = 1.9219 (0.896 sec/step)\n",
            "I0915 14:15:27.823441 139848901543808 learning.py:507] global step 2780: loss = 1.9219 (0.896 sec/step)\n",
            "INFO:tensorflow:global step 2781: loss = 1.7444 (0.627 sec/step)\n",
            "I0915 14:15:28.452704 139848901543808 learning.py:507] global step 2781: loss = 1.7444 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2782: loss = 1.5827 (0.621 sec/step)\n",
            "I0915 14:15:29.075931 139848901543808 learning.py:507] global step 2782: loss = 1.5827 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2783: loss = 1.7519 (0.636 sec/step)\n",
            "I0915 14:15:29.714016 139848901543808 learning.py:507] global step 2783: loss = 1.7519 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2784: loss = 2.0009 (0.626 sec/step)\n",
            "I0915 14:15:30.342467 139848901543808 learning.py:507] global step 2784: loss = 2.0009 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2785: loss = 1.9489 (0.638 sec/step)\n",
            "I0915 14:15:30.982782 139848901543808 learning.py:507] global step 2785: loss = 1.9489 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2786: loss = 1.3058 (0.653 sec/step)\n",
            "I0915 14:15:31.637648 139848901543808 learning.py:507] global step 2786: loss = 1.3058 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2787: loss = 2.4360 (0.617 sec/step)\n",
            "I0915 14:15:32.256774 139848901543808 learning.py:507] global step 2787: loss = 2.4360 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2788: loss = 2.1423 (0.610 sec/step)\n",
            "I0915 14:15:32.868903 139848901543808 learning.py:507] global step 2788: loss = 2.1423 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2789: loss = 2.0272 (0.656 sec/step)\n",
            "I0915 14:15:33.526223 139848901543808 learning.py:507] global step 2789: loss = 2.0272 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2790: loss = 1.9421 (0.609 sec/step)\n",
            "I0915 14:15:34.137665 139848901543808 learning.py:507] global step 2790: loss = 1.9421 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2791: loss = 1.8247 (0.643 sec/step)\n",
            "I0915 14:15:34.783564 139848901543808 learning.py:507] global step 2791: loss = 1.8247 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2792: loss = 2.9013 (0.656 sec/step)\n",
            "I0915 14:15:35.441885 139848901543808 learning.py:507] global step 2792: loss = 2.9013 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2793: loss = 0.8920 (0.619 sec/step)\n",
            "I0915 14:15:36.063323 139848901543808 learning.py:507] global step 2793: loss = 0.8920 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2794: loss = 2.7384 (0.644 sec/step)\n",
            "I0915 14:15:36.709006 139848901543808 learning.py:507] global step 2794: loss = 2.7384 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2795: loss = 1.6573 (0.640 sec/step)\n",
            "I0915 14:15:37.350551 139848901543808 learning.py:507] global step 2795: loss = 1.6573 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2796: loss = 1.9794 (0.630 sec/step)\n",
            "I0915 14:15:37.982319 139848901543808 learning.py:507] global step 2796: loss = 1.9794 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2797: loss = 1.7680 (0.651 sec/step)\n",
            "I0915 14:15:38.635981 139848901543808 learning.py:507] global step 2797: loss = 1.7680 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2798: loss = 1.6603 (0.630 sec/step)\n",
            "I0915 14:15:39.268332 139848901543808 learning.py:507] global step 2798: loss = 1.6603 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2799: loss = 1.7171 (0.620 sec/step)\n",
            "I0915 14:15:39.890260 139848901543808 learning.py:507] global step 2799: loss = 1.7171 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2800: loss = 1.5396 (0.620 sec/step)\n",
            "I0915 14:15:40.511784 139848901543808 learning.py:507] global step 2800: loss = 1.5396 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2801: loss = 1.5010 (0.649 sec/step)\n",
            "I0915 14:15:41.163605 139848901543808 learning.py:507] global step 2801: loss = 1.5010 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2802: loss = 2.8596 (0.635 sec/step)\n",
            "I0915 14:15:41.800115 139848901543808 learning.py:507] global step 2802: loss = 2.8596 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2803: loss = 2.1819 (0.645 sec/step)\n",
            "I0915 14:15:42.447626 139848901543808 learning.py:507] global step 2803: loss = 2.1819 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2804: loss = 1.1613 (0.648 sec/step)\n",
            "I0915 14:15:43.097683 139848901543808 learning.py:507] global step 2804: loss = 1.1613 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2805: loss = 1.7323 (0.615 sec/step)\n",
            "I0915 14:15:43.714851 139848901543808 learning.py:507] global step 2805: loss = 1.7323 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2806: loss = 3.0741 (0.598 sec/step)\n",
            "I0915 14:15:44.314632 139848901543808 learning.py:507] global step 2806: loss = 3.0741 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2807: loss = 1.7750 (0.622 sec/step)\n",
            "I0915 14:15:44.938962 139848901543808 learning.py:507] global step 2807: loss = 1.7750 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2808: loss = 2.2427 (0.659 sec/step)\n",
            "I0915 14:15:45.599781 139848901543808 learning.py:507] global step 2808: loss = 2.2427 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2809: loss = 1.4574 (0.617 sec/step)\n",
            "I0915 14:15:46.218567 139848901543808 learning.py:507] global step 2809: loss = 1.4574 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2810: loss = 2.1723 (0.632 sec/step)\n",
            "I0915 14:15:46.852203 139848901543808 learning.py:507] global step 2810: loss = 2.1723 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2811: loss = 1.4644 (0.649 sec/step)\n",
            "I0915 14:15:47.504333 139848901543808 learning.py:507] global step 2811: loss = 1.4644 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2812: loss = 1.9173 (0.643 sec/step)\n",
            "I0915 14:15:48.149847 139848901543808 learning.py:507] global step 2812: loss = 1.9173 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2813: loss = 1.4417 (0.635 sec/step)\n",
            "I0915 14:15:48.786950 139848901543808 learning.py:507] global step 2813: loss = 1.4417 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2814: loss = 1.0638 (0.642 sec/step)\n",
            "I0915 14:15:49.431096 139848901543808 learning.py:507] global step 2814: loss = 1.0638 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2815: loss = 1.9853 (0.637 sec/step)\n",
            "I0915 14:15:50.070464 139848901543808 learning.py:507] global step 2815: loss = 1.9853 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2816: loss = 2.9276 (0.631 sec/step)\n",
            "I0915 14:15:50.703221 139848901543808 learning.py:507] global step 2816: loss = 2.9276 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2817: loss = 1.8557 (0.631 sec/step)\n",
            "I0915 14:15:51.336790 139848901543808 learning.py:507] global step 2817: loss = 1.8557 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2818: loss = 2.2816 (0.645 sec/step)\n",
            "I0915 14:15:51.983949 139848901543808 learning.py:507] global step 2818: loss = 2.2816 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2819: loss = 1.7680 (0.633 sec/step)\n",
            "I0915 14:15:52.620203 139848901543808 learning.py:507] global step 2819: loss = 1.7680 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2820: loss = 2.3016 (0.629 sec/step)\n",
            "I0915 14:15:53.251276 139848901543808 learning.py:507] global step 2820: loss = 2.3016 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2821: loss = 3.2400 (0.641 sec/step)\n",
            "I0915 14:15:53.895666 139848901543808 learning.py:507] global step 2821: loss = 3.2400 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2822: loss = 2.4479 (0.618 sec/step)\n",
            "I0915 14:15:54.515571 139848901543808 learning.py:507] global step 2822: loss = 2.4479 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2823: loss = 1.3211 (0.623 sec/step)\n",
            "I0915 14:15:55.140155 139848901543808 learning.py:507] global step 2823: loss = 1.3211 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2824: loss = 2.8990 (0.629 sec/step)\n",
            "I0915 14:15:55.770990 139848901543808 learning.py:507] global step 2824: loss = 2.8990 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2825: loss = 2.2754 (0.631 sec/step)\n",
            "I0915 14:15:56.404240 139848901543808 learning.py:507] global step 2825: loss = 2.2754 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2826: loss = 1.9706 (0.645 sec/step)\n",
            "I0915 14:15:57.051115 139848901543808 learning.py:507] global step 2826: loss = 1.9706 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2827: loss = 1.6698 (0.633 sec/step)\n",
            "I0915 14:15:57.686236 139848901543808 learning.py:507] global step 2827: loss = 1.6698 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2828: loss = 1.3271 (0.660 sec/step)\n",
            "I0915 14:15:58.348241 139848901543808 learning.py:507] global step 2828: loss = 1.3271 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2829: loss = 1.9754 (0.630 sec/step)\n",
            "I0915 14:15:58.980251 139848901543808 learning.py:507] global step 2829: loss = 1.9754 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2830: loss = 2.0823 (0.619 sec/step)\n",
            "I0915 14:15:59.601985 139848901543808 learning.py:507] global step 2830: loss = 2.0823 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2831: loss = 1.7038 (0.620 sec/step)\n",
            "I0915 14:16:00.224017 139848901543808 learning.py:507] global step 2831: loss = 1.7038 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2832: loss = 1.7951 (0.625 sec/step)\n",
            "I0915 14:16:00.850883 139848901543808 learning.py:507] global step 2832: loss = 1.7951 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2833: loss = 1.6744 (0.638 sec/step)\n",
            "I0915 14:16:01.491446 139848901543808 learning.py:507] global step 2833: loss = 1.6744 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2834: loss = 1.7881 (0.623 sec/step)\n",
            "I0915 14:16:02.116586 139848901543808 learning.py:507] global step 2834: loss = 1.7881 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2835: loss = 1.8770 (0.637 sec/step)\n",
            "I0915 14:16:02.755756 139848901543808 learning.py:507] global step 2835: loss = 1.8770 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2836: loss = 1.4319 (0.643 sec/step)\n",
            "I0915 14:16:03.400949 139848901543808 learning.py:507] global step 2836: loss = 1.4319 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2837: loss = 2.5438 (0.610 sec/step)\n",
            "I0915 14:16:04.012867 139848901543808 learning.py:507] global step 2837: loss = 2.5438 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2838: loss = 2.2351 (0.632 sec/step)\n",
            "I0915 14:16:04.646720 139848901543808 learning.py:507] global step 2838: loss = 2.2351 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2839: loss = 2.4249 (0.635 sec/step)\n",
            "I0915 14:16:05.284071 139848901543808 learning.py:507] global step 2839: loss = 2.4249 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2840: loss = 2.4290 (0.660 sec/step)\n",
            "I0915 14:16:05.945846 139848901543808 learning.py:507] global step 2840: loss = 2.4290 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2841: loss = 1.7604 (0.622 sec/step)\n",
            "I0915 14:16:06.569224 139848901543808 learning.py:507] global step 2841: loss = 1.7604 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2842: loss = 2.0428 (0.643 sec/step)\n",
            "I0915 14:16:07.213891 139848901543808 learning.py:507] global step 2842: loss = 2.0428 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2843: loss = 1.9811 (0.615 sec/step)\n",
            "I0915 14:16:07.831282 139848901543808 learning.py:507] global step 2843: loss = 1.9811 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2844: loss = 2.5288 (0.647 sec/step)\n",
            "I0915 14:16:08.480369 139848901543808 learning.py:507] global step 2844: loss = 2.5288 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2845: loss = 2.2492 (0.646 sec/step)\n",
            "I0915 14:16:09.128086 139848901543808 learning.py:507] global step 2845: loss = 2.2492 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2846: loss = 1.2316 (0.661 sec/step)\n",
            "I0915 14:16:09.790459 139848901543808 learning.py:507] global step 2846: loss = 1.2316 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2847: loss = 2.4843 (0.633 sec/step)\n",
            "I0915 14:16:10.425049 139848901543808 learning.py:507] global step 2847: loss = 2.4843 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2848: loss = 1.3354 (0.642 sec/step)\n",
            "I0915 14:16:11.069253 139848901543808 learning.py:507] global step 2848: loss = 1.3354 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2849: loss = 1.7593 (0.621 sec/step)\n",
            "I0915 14:16:11.691734 139848901543808 learning.py:507] global step 2849: loss = 1.7593 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2850: loss = 1.2762 (0.637 sec/step)\n",
            "I0915 14:16:12.330247 139848901543808 learning.py:507] global step 2850: loss = 1.2762 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2851: loss = 2.9303 (0.635 sec/step)\n",
            "I0915 14:16:12.967341 139848901543808 learning.py:507] global step 2851: loss = 2.9303 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2852: loss = 2.2037 (0.644 sec/step)\n",
            "I0915 14:16:13.613534 139848901543808 learning.py:507] global step 2852: loss = 2.2037 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2853: loss = 1.5979 (0.653 sec/step)\n",
            "I0915 14:16:14.268301 139848901543808 learning.py:507] global step 2853: loss = 1.5979 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2854: loss = 1.3626 (0.623 sec/step)\n",
            "I0915 14:16:14.893107 139848901543808 learning.py:507] global step 2854: loss = 1.3626 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2855: loss = 1.4144 (0.640 sec/step)\n",
            "I0915 14:16:15.535130 139848901543808 learning.py:507] global step 2855: loss = 1.4144 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2856: loss = 1.5536 (0.627 sec/step)\n",
            "I0915 14:16:16.163811 139848901543808 learning.py:507] global step 2856: loss = 1.5536 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2857: loss = 1.6319 (0.650 sec/step)\n",
            "I0915 14:16:16.816552 139848901543808 learning.py:507] global step 2857: loss = 1.6319 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2858: loss = 2.3490 (0.652 sec/step)\n",
            "I0915 14:16:17.470463 139848901543808 learning.py:507] global step 2858: loss = 2.3490 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2859: loss = 2.3837 (0.641 sec/step)\n",
            "I0915 14:16:18.113804 139848901543808 learning.py:507] global step 2859: loss = 2.3837 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2860: loss = 2.2380 (0.641 sec/step)\n",
            "I0915 14:16:18.757167 139848901543808 learning.py:507] global step 2860: loss = 2.2380 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2861: loss = 1.7490 (0.644 sec/step)\n",
            "I0915 14:16:19.403639 139848901543808 learning.py:507] global step 2861: loss = 1.7490 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2862: loss = 2.4201 (0.631 sec/step)\n",
            "I0915 14:16:20.036588 139848901543808 learning.py:507] global step 2862: loss = 2.4201 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2863: loss = 1.8591 (0.623 sec/step)\n",
            "I0915 14:16:20.661938 139848901543808 learning.py:507] global step 2863: loss = 1.8591 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2864: loss = 2.2140 (0.633 sec/step)\n",
            "I0915 14:16:21.296864 139848901543808 learning.py:507] global step 2864: loss = 2.2140 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2865: loss = 2.0823 (0.637 sec/step)\n",
            "I0915 14:16:21.935478 139848901543808 learning.py:507] global step 2865: loss = 2.0823 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2866: loss = 2.2687 (0.636 sec/step)\n",
            "I0915 14:16:22.573694 139848901543808 learning.py:507] global step 2866: loss = 2.2687 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2867: loss = 1.9770 (0.625 sec/step)\n",
            "I0915 14:16:23.200424 139848901543808 learning.py:507] global step 2867: loss = 1.9770 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2868: loss = 2.7515 (0.638 sec/step)\n",
            "I0915 14:16:23.840174 139848901543808 learning.py:507] global step 2868: loss = 2.7515 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2869: loss = 2.1753 (0.646 sec/step)\n",
            "I0915 14:16:24.488200 139848901543808 learning.py:507] global step 2869: loss = 2.1753 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2870: loss = 2.0532 (0.625 sec/step)\n",
            "I0915 14:16:25.115015 139848901543808 learning.py:507] global step 2870: loss = 2.0532 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2871: loss = 1.4638 (0.621 sec/step)\n",
            "I0915 14:16:25.738596 139848901543808 learning.py:507] global step 2871: loss = 1.4638 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2872: loss = 1.6803 (0.659 sec/step)\n",
            "I0915 14:16:26.400174 139848901543808 learning.py:507] global step 2872: loss = 1.6803 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2873: loss = 1.7811 (0.656 sec/step)\n",
            "I0915 14:16:27.058579 139848901543808 learning.py:507] global step 2873: loss = 1.7811 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2874: loss = 2.0593 (0.627 sec/step)\n",
            "I0915 14:16:27.688307 139848901543808 learning.py:507] global step 2874: loss = 2.0593 (0.627 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"./models/research/object_detection/legacy/train.py\", line 184, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"./models/research/object_detection/legacy/train.py\", line 180, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/content/models/research/object_detection/legacy/trainer.py\", line 416, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkLj3DMFSKQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71c9921d-7347-44b3-b59b-9106b9698b02"
      },
      "source": [
        "!python ./models/research/object_detection/export_tflite_ssd_graph.py --input_type image_tensor --pipeline_config_path ./training_demo/training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix ./training_demo/training/model.ckpt-2775 --output_directory ./training_demo/frozen_tflite/frozen_inference_graph.pb -add_postprocessing_op True --max_detections 6"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/export_tflite_ssd_graph.py:143: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From ./models/research/object_detection/export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0915 14:17:25.655996 140087342557056 deprecation_wrapper.py:119] From ./models/research/object_detection/export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:186: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0915 14:17:25.660630 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:186: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:230: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0915 14:17:25.661059 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:230: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0915 14:17:25.664628 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0915 14:17:29.385568 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0915 14:17:29.402028 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:29.402184 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:29.527884 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:29.664666 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:29.788025 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:29.910643 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:17:30.033514 140087342557056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0915 14:17:30.167298 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-09-15 14:17:30.168811: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-15 14:17:30.190536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.191273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:17:30.191634: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:17:30.193258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:17:30.205145: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:17:30.205557: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:17:30.207316: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:17:30.227179: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:17:30.241372: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:17:30.241521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.242298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.243011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:17:30.256677: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-15 14:17:30.257013: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20c1c00 executing computations on platform Host. Devices:\n",
            "2019-09-15 14:17:30.257063: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-15 14:17:30.330483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.331538: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20c2840 executing computations on platform CUDA. Devices:\n",
            "2019-09-15 14:17:30.331580: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-15 14:17:30.331755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.332621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:17:30.332710: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:17:30.332740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:17:30.332765: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:17:30.332789: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:17:30.332818: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:17:30.332845: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:17:30.332875: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:17:30.332984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.333851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.334592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:17:30.334650: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:17:30.336186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:17:30.336217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:17:30.336230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:17:30.336412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.337180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:30.337858: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-15 14:17:30.337909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:260: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0915 14:17:30.916287 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:260: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:285: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0915 14:17:30.921661 140087342557056 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_tflite_ssd_graph_lib.py:285: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0915 14:17:31.564754 140087342557056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-09-15 14:17:32.218062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:32.218845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:17:32.218937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:17:32.218980: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:17:32.219009: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:17:32.219035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:17:32.219058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:17:32.219083: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:17:32.219109: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:17:32.219202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:32.219999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:32.220661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:17:32.220708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:17:32.220721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:17:32.220731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:17:32.220894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:32.221666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:17:32.222415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from ./training_demo/training/model.ckpt-2775\n",
            "I0915 14:17:32.223972 140087342557056 saver.py:1280] Restoring parameters from ./training_demo/training/model.ckpt-2775\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0915 14:17:32.875715 140087342557056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W0915 14:17:32.876076 140087342557056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 404 variables.\n",
            "I0915 14:17:33.215626 140087342557056 graph_util_impl.py:311] Froze 404 variables.\n",
            "INFO:tensorflow:Converted 404 variables to const ops.\n",
            "I0915 14:17:33.287141 140087342557056 graph_util_impl.py:364] Converted 404 variables to const ops.\n",
            "2019-09-15 14:17:33.393695: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVRXeQPw-Nnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "ee886c79-7ac9-42fb-fb9e-2cee76a20d55"
      },
      "source": [
        "!tflite_convert --output_file=./training_demo/frozen_tflite/tflite_graph.tflite --graph_def_file=./training_demo/frozen_tflite/frozen_inference_graph.pb/tflite_graph.pb --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --input_shape=1,300,300,3 --allow_custom_ops"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-15 14:19:19.605201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-15 14:19:19.623033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.623809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:19:19.624065: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:19:19.625853: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:19:19.627437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:19:19.627793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:19:19.629562: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:19:19.630841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:19:19.635376: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:19:19.635479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.636326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.637033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:19:19.643446: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-15 14:19:19.643771: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b61775bb80 executing computations on platform Host. Devices:\n",
            "2019-09-15 14:19:19.643821: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-15 14:19:19.700572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.701437: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b61c04c380 executing computations on platform CUDA. Devices:\n",
            "2019-09-15 14:19:19.701469: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-15 14:19:19.701687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.702392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:19:19.702461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:19:19.702486: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:19:19.702507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:19:19.702527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:19:19.702549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:19:19.702570: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:19:19.702590: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:19:19.702661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.703377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.704110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:19:19.704163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:19:19.705571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:19:19.705602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:19:19.705615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:19:19.705741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.706597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:19:19.707247: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-15 14:19:19.707307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U1WTUTr_3eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "159c8dd7-af89-4202-bb9a-5a181257248c"
      },
      "source": [
        "!zip -r ./training_demo/training.zip ./training_demo/frozen_tflite/\n",
        "\n",
        "file1 = drive.CreateFile({'title': 'frozen_inference_graph.pb.zip'})\n",
        "file1.SetContentFile('training_demo/training.zip')\n",
        "file1.Upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: training_demo/frozen_tflite/ (stored 0%)\n",
            "  adding: training_demo/frozen_tflite/tflite_graph.tflite (deflated 7%)\n",
            "  adding: training_demo/frozen_tflite/frozen_inference_graph.pb/ (stored 0%)\n",
            "  adding: training_demo/frozen_tflite/frozen_inference_graph.pb/tflite_graph.pb (deflated 9%)\n",
            "  adding: training_demo/frozen_tflite/frozen_inference_graph.pb/tflite_graph.pbtxt (deflated 56%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3j0XSPaAIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5ba3bdd-c5b2-4489-fbc1-b70252506e9b"
      },
      "source": [
        "!python /content/models/research/object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path /content/training_demo/training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix /content/training_demo/training/model.ckpt-2775 --output_directory /content/training_demo/training/output_inference_graph_v1.pb"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0915 14:51:17.129539 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0915 14:51:17.136210 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0915 14:51:17.136642 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0915 14:51:17.179132 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0915 14:51:17.214150 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0915 14:51:20.813526 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0915 14:51:20.829205 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:20.829411 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:20.950769 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:21.086913 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:21.224319 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:21.354321 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0915 14:51:21.480187 140446541793152 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/post_processing.py:567: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0915 14:51:22.097649 140446541793152 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:567: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:260: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0915 14:51:22.725044 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:260: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0915 14:51:22.725514 140446541793152 deprecation.py:323] From /content/models/research/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0915 14:51:22.729658 140446541793152 deprecation.py:323] From /content/models/research/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W0915 14:51:22.731213 140446541793152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "177 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/3.08m params)\n",
            "  BoxPredictor_0 (--/29.41k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/12.12k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (21, 21/21 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x21, 12.10k/12.10k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/107.59k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/53.80k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (42, 42/42 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x42, 53.76k/53.76k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/43.07k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/21.55k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (42, 42/42 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x42, 21.50k/21.50k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/21.57k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/10.79k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (42, 42/42 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x42, 10.75k/10.75k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/21.57k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/10.79k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (42, 42/42 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x42, 10.75k/10.75k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/10.82k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/5.42k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (42, 42/42 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x42, 5.38k/5.38k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "177 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/17.64k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/add_8 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/add_11 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/add (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_1 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_3 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_4 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/add_7 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_6 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_10 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_9 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0915 14:51:24.413444 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-09-15 14:51:25.909027: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-15 14:51:25.928243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:25.929010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:51:25.929379: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:51:25.931492: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:51:25.932981: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:51:25.933408: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:51:25.935229: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:51:25.936622: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:51:25.941618: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:51:25.941763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:25.942576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:25.943255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:51:25.950685: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-15 14:51:25.950987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f50840 executing computations on platform Host. Devices:\n",
            "2019-09-15 14:51:25.951035: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-15 14:51:26.010557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.011398: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f4fc00 executing computations on platform CUDA. Devices:\n",
            "2019-09-15 14:51:26.011429: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-15 14:51:26.011631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.012301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:51:26.012407: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:51:26.012451: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:51:26.012494: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:51:26.012534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:51:26.012572: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:51:26.012610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:51:26.012648: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:51:26.012786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.013548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.014234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:51:26.014300: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:51:26.015888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:51:26.015921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:51:26.015949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:51:26.016119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.016923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:26.017650: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-15 14:51:26.017711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0915 14:51:26.018625 140446541793152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/training_demo/training/model.ckpt-2775\n",
            "I0915 14:51:26.020298 140446541793152 saver.py:1280] Restoring parameters from /content/training_demo/training/model.ckpt-2775\n",
            "2019-09-15 14:51:29.094918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:29.095723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:51:29.095824: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:51:29.095874: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:51:29.095925: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:51:29.095972: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:51:29.096013: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:51:29.096057: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:51:29.096101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:51:29.096227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:29.097138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:29.097812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:51:29.097868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:51:29.097892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:51:29.097907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:51:29.098050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:29.098814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:29.099559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from /content/training_demo/training/model.ckpt-2775\n",
            "I0915 14:51:29.101201 140446541793152 saver.py:1280] Restoring parameters from /content/training_demo/training/model.ckpt-2775\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0915 14:51:29.906086 140446541793152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W0915 14:51:29.906491 140446541793152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 404 variables.\n",
            "I0915 14:51:30.322945 140446541793152 graph_util_impl.py:311] Froze 404 variables.\n",
            "INFO:tensorflow:Converted 404 variables to const ops.\n",
            "I0915 14:51:30.434045 140446541793152 graph_util_impl.py:364] Converted 404 variables to const ops.\n",
            "2019-09-15 14:51:30.593981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:30.594848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-15 14:51:30.594953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-15 14:51:30.594997: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-15 14:51:30.595044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-15 14:51:30.595083: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-15 14:51:30.595147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-15 14:51:30.595199: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-15 14:51:30.595237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-15 14:51:30.595387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:30.596285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:30.597006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-15 14:51:30.597105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-15 14:51:30.597126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-15 14:51:30.597139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-15 14:51:30.597273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:30.598127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-15 14:51:30.598911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0915 14:51:31.406788 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0915 14:51:31.407530 140446541793152 deprecation.py:323] From /content/models/research/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0915 14:51:31.408302 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0915 14:51:31.408584 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0915 14:51:31.408940 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:307: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W0915 14:51:31.409146 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:307: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I0915 14:51:31.409698 140446541793152 builder_impl.py:636] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0915 14:51:31.409846 140446541793152 builder_impl.py:456] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /content/training_demo/training/output_inference_graph_v1.pb/saved_model/saved_model.pb\n",
            "I0915 14:51:31.773036 140446541793152 builder_impl.py:421] SavedModel written to: /content/training_demo/training/output_inference_graph_v1.pb/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0915 14:51:31.811564 140446541793152 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to /content/training_demo/training/output_inference_graph_v1.pb/pipeline.config\n",
            "I0915 14:51:31.811801 140446541793152 config_util.py:190] Writing pipeline config file to /content/training_demo/training/output_inference_graph_v1.pb/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXp0R1HvU5NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fa19335-29ba-4305-d28b-4f01ab62a9e6"
      },
      "source": [
        "%cd models/research/\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yizund9UlBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "7722952a-353c-42c3-89d5-cb55c9540539"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = \"/content/training_demo/training/output_inference_graph_v1.pb/frozen_inference_graph.pb\"\n",
        "\n",
        "image_path = \"/content/training_demo/images/test/5.jpg\"\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = \"/content/training_demo/annotations/label_map.pbtxt\"\n",
        "\n",
        "# Number of classes to detect\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "# Load a (frozen) Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "# Loading label map\n",
        "# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "# Helper code\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Detection\n",
        "with detection_graph.as_default():\n",
        "    with tf.Session(graph=detection_graph) as sess:\n",
        "\n",
        "      image_np = load_image_into_numpy_array(Image.open(image_path))\n",
        "    \n",
        "      image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "      # Extract image tensor\n",
        "      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "      # Extract detection boxes\n",
        "      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "      # Extract detection scores\n",
        "      scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "      # Extract detection classes\n",
        "      classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "      # Extract number of detectionsd\n",
        "      num_detections = detection_graph.get_tensor_by_name(\n",
        "          'num_detections:0')\n",
        "      # Actual detection.\n",
        "      (boxes, scores, classes, num_detections) = sess.run(\n",
        "          [boxes, scores, classes, num_detections],\n",
        "          feed_dict={image_tensor: image_np_expanded})\n",
        "      # Visualization of the results of a detection.\n",
        "      vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np,\n",
        "          np.squeeze(boxes),\n",
        "          np.squeeze(classes).astype(np.int32),\n",
        "          np.squeeze(scores),\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          line_thickness=4,\n",
        "          min_score_thresh = 0.4\n",
        "          )\n",
        "     \n",
        "      IMAGE_SIZE = (24, 16)\n",
        "      plt.figure(figsize=IMAGE_SIZE)\n",
        "      plt.imshow(image_np)  "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABLMAAAOICAYAAAAtmLOCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5dLw/z5FH+kyvFa9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1728x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}